---
output: github_document
bibliography: vignettes/references.bib
editor_options: 
  markdown: 
    wrap: sentence
---

<!-- badges: start -->
[![R-CMD-check](https://github.com/jepusto/metaselection/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/jepusto/metaselection/actions/workflows/R-CMD-check.yaml)
[![Codecov Status](https://codecov.io/gh/jepusto/metaselection/graph/badge.svg?token=8T7IUFT1QV)](https://codecov.io/gh/jepusto/metaselection)
<!-- [![CRAN Version](http://www.r-pkg.org/badges/version/metaselection)](https://CRAN.R-project.org/package=metaselection) -->
<!-- [![](http://cranlogs.r-pkg.org/badges/grand-total/metaselection)](https://CRAN.R-project.org/package=metaselection) -->
<!-- [![](http://cranlogs.r-pkg.org/badges/last-month/metaselection)](https://CRAN.R-project.org/package=metaselection) -->
<!-- badges: end -->

# metaselection

Selective reporting occurs when statistically significant, affirmative results are more likely to be reported (and therefore more likely to be available for meta-analysis) compared to null, non-affirmative results.
Selective reporting is a major concern for research synthesis because it distorts the evidence base available for meta-analysis.
Failure to account for selective reporting can inflate effect size estimates from meta-analysis and bias estimates of heterogeneity, making it difficult to draw accurate conclusions from a synthesis.

There are many tools available already to investigate and correct for selective outcome reporting.
Widely used methods include: graphical diagnostics like funnel plots; tests and adjustments for funnel plot asymmetry like trim-and-fill, Egger's regression, PET/PEESE, selection models, and p-value diagnostics.
However, very few methods available for investigating selective reporting can accommodate dependent effect sizes.
Such limitation poses a problem for meta-analyses in education, psychology and other social sciences, where dependent effects are a very common feature of meta-analytic data.

Dependent effect sizes occur when primary studies report multiple measures of the outcomes or repeated measures of the outcome.
Failing to account for dependency can result in misleading conclusions too narrow confidence intervals and hypothesis tests that have inflated type one error rates.

X (2024) developed and examined methods for investigating and accounting for selective reporting in meta-analysis that account for dependent effect sizes.
The results showed that selection models combined with robust variance estimation led to lower bias in the estimate of the overall effect size.
Combining the selection models with cluster bootstrapping led to close to nominal coverage rates.

Our metaselection package provides a set of functions that implements these methods.
The main function, `selection_model()`, fits step function and beta density selection models. To handle dependence in the effect size estimates, the function provides options to use cluster-robust (sandwich) variance estimation or cluster bootstrapping to assess uncertainty in the model parameter estimates.

## Installation

You can install the development version of the package from GitHub with:

```{r, eval = FALSE}
remotes::install_github("jepusto/metaselection", build_vignettes = TRUE)
```

It may take a few minutes to install the package with the vignette. Setting `build_vignettes = FALSE` will lead to faster installation, although it will preclude viewing the package vignette.

## Example

The following example uses data from a meta-analysis by Lehmann meta-analysis which examined the effects of color red on attractiveness judgments. 
The dataset is included in the `metadat` package [@metadat] as `dat.lehmann`.
In the code below, we fit a step function selection model to the Lehmann dataset using the `selection_model()` function, with confidence intervals computed using cluster bootstrapping.
For further details, please see the vignette.

```{r, warning = FALSE, message = FALSE}
library(metaselection)

data("dat.lehmann2018", package = "metadat")
dat.lehmann2018$study <- dat.lehmann2018$Full_Citation
dat.lehmann2018$sei <- sqrt(dat.lehmann2018$vi)

set.seed(20240910)

mod_3PSM_boot <- selection_model(
  data = dat.lehmann2018, 
  yi = yi,
  sei = sei,
  cluster = study,
  selection_type = "step",
  steps = .025,
  CI_type = "percentile",
  bootstrap = "multinomial",
  R = 19 # Set to a much bigger number of bootstraps for real applications
)

summary(mod_3PSM_boot)
```

The package is designed to work with the `progressr` package. To turn on progress bars for all bootstrap calculations, use
```{r, eval = FALSE}
progressr::handlers(global = TRUE)
```
See `vignette("progressr-intro")` for further details.

The package is also designed to work with the `future` package for parallel computing. To enable parallel computation of bootstrap calculations, simply set an appropriate parallelization plan such as
```{r, eval = FALSE}
library(future)
plan(multisession)
```
See the `metaselection` package vignette for a more detailed demonstration.

## Related Work

We want to recognize other packages that provide functions for fitting selection models and closely related techniques.

Several existing packages provide implementations of selection models assuming that effect size estimates are independent.
The `metafor` package [@Viechtbauer2010conducting] includes the `selmodel()` function, which allows users to fit many different types of selection models.
The `weightr` package [@weightr] includes functions to estimate a class of p-value selection models described in @vevea1995general
However, the functions available in these packages can only be applied to meta-analytic data assuming that the effect sizes are independent.
In addition, the `PublicationBias` package [@PublicationBias] implements sensitivity analyses for selective reporting bias that incorporate cluster-robust variance estimation methods for handling dependent effect sizes.
However, the sensitivity analyses implemented in the package are based on a pre-specified degree of selective reporting, rather than allowing the degree of selection to be estimated from the data. The sensitivity analyses are also based on a specific and simple form of selection model, and do not allow consideration of more complex forms of selection functions.

## Acknowledgements

## References

---
title: "Simulation Results"
output: pdf_document
bibliography: references.bib
---

```{r sim-results-setup}
source("process-simulation-results.R")

mu_graph_res_main$method = fct(mu_graph_res_main$method, levels = c("Beta","CHE-ISCW","PET/PEESE"))
mu_graph_res_ci_main$method = fct(mu_graph_res_ci_main$method, levels = c("Beta","CHE-ISCW","PET/PEESE"))

beta_bias_main <-
  mu_graph_res_main %>%
  filter(model_estimator == "beta_CML") %>%
  summarize(
    max_abs_bias = max(abs(bias)),
    min_abs_bias = min(abs(bias)),
    max_bias = max(bias),
    min_bias = min(bias)
  )

che_bias_main <-
  mu_graph_res_main %>%
  filter(model_estimator == "Comparison_CHE-ISCW") %>%
  summarize(
    max_abs_bias = max(abs(bias)),
    min_abs_bias = min(abs(bias)),
    max_bias = max(bias),
    min_bias = min(bias)
  )

pp_bias_main <-
  mu_graph_res_main %>%
  filter(model_estimator == "Comparison_PET/PEESE") %>%
  summarize(
    max_abs_bias = max(abs(bias)),
    min_abs_bias = min(abs(bias)),
    max_bias = max(bias),
    min_bias = min(bias)
  )

mu_graph_res_miss$method = fct(mu_graph_res_miss$method, levels = c("Beta","3PSM","4PSM"))
mu_graph_res_ci_miss$method = fct(mu_graph_res_ci_miss$method, levels = c("Beta","3PSM","4PSM"))

```

We organize our simulation results into two parts. First, we compare the performance of the beta-function selection model to the CHE-ISCW and PET/PEESE approaches. Specifically, we compare the bias and accuracy of the average effect size, the coverage rates of 95% confidence intervals of the average effect size, and the estimation of the marginal variance of the effect size distribution. Second, we compare the performance of the beta-function selection model to the 3PSM and 4PSM step-function models to assess the robustness of these models to misspecification of the selection function.

## Beta-Function Selection Model Compared to CHE-ISCW and PET/PEESE
<!-- Need a better header name -->

### Average Effect Size

The CHE-ISCW and PET/PEESE approaches had perfect convergence rates, producing results for every replication in every condition. The beta-function selection model exhibited very high convergence rates, with an average rate of `r round(100 * mean(mu_graph_res_main$convergence[which(mu_graph_res_main$model_estimator=="beta_CML")]), 1)`%; the lowest convergence rates occurred under conditions with the lowest degree of heterogeneity $\tau = 0.05$. Supplementary Figure \@ref(fig:convergence-rates-main) depicts the range of convergence rates of the beta-function selection model.

#### Bias

```{r mu-bias-main}
#| fig.width: 9
#| fig.height: 6.5
#| fig.env: "sidewaysfigure"
#| fig.cap: "Bias of the average effect size by method, selection probability, average SMD, and between-study heterogeneity"

ggplot(mu_graph_res_main) + 
  aes(x = selection_strength, y = bias, color = method, fill = method) +
  geom_hline(yintercept = 0) +
  geom_boxplot(alpha = .5, coef = Inf) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_discrete(labels = function(x) stringr::str_wrap(x, width = 3))+
  facet_grid(
    tau ~ mean_smd, 
    labeller = label_bquote(
      rows = tau == .(tau),
      cols = mu == .(mean_smd)
    ),
    scales = "free_y"
  ) +
  labs(
    x = "Selection probability", 
    y = "Bias", 
    color = "Method",
    fill = "Method"
  ) + 
  theme_bw() +
  theme(legend.position = "top")

```

Figure @ref(fig:mu-bias-main) shows the bias for each method of estimating the average effect size (vertical axis) as a function of selective reporting strength (horizontal axis), average effect size (grid column), and between-study heterogeneity ($\tau$, grid row). Each box plot summarizes variation in bias across the remaining simulation factors: the heterogeneity ratio, the correlation between effect size estimates, and the number of observed studies. Note that the vertical axis scale differs by grid row, reflecting how some methods’ bias is more sensitive to the level of heterogeneity.

The beta-function selection model has negligible to small bias across all conditions, ranging from `r round(beta_bias_main$min_bias,2)` to `r round(beta_bias_main$max_bias,2)`. Its bias was essentially zero for all conditions when the average effect size is non-zero or when selective reporting is weak or absent. Bias was largest when selective reporting is very strong, average effect size is zero, and heterogeneity is large.

In contrast, bias for the comparison methods ranged from `r round(che_bias_main$min_bias,2)` to `r round(che_bias_main$max_bias,2)` for CHE-ISCW and `r round(pp_bias_main$min_bias,2)` to `r round(pp_bias_main$max_bias,2)` for PET/PEESE. Both methods are generally biased when selective reporting is not absent.
For CHE-ISCW, which does not directly adjust for selective reporting, bias was closest to zero when average effect size is large $(\mu = 0.8)$ and heterogeneity is low $(\tau \leq 0.15)$.
For PET/PEESE, which uses a regression adjustment to account for possible selective reporting, bias was closest to zero when average effect size is moderate $(\mu = 0.4)$ and heterogeneity is low $(\tau = 0.15)$.
For both comparison methods, bias grows stronger when selection is stronger, when average effect size is smaller, and when heterogeneity is larger; however, bias is generally less pronounced for PET/PEESE than CHE-ISCW.

#### Scaled RMSE

```{r mu-rmse-main}
#| fig.width: 9
#| fig.height: 6.5
#| fig.env: "sidewaysfigure"
#| fig.cap: "Scaled root mean-squared error of the average effect size by method, selection probability, average SMD, and between-study heterogeneity"

ggplot(mu_graph_res_main) + 
  aes(x = selection_strength, y = scrmse, color = method, fill = method) +
  geom_hline(yintercept = 0) +
  geom_boxplot(alpha = .5, coef = Inf) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_discrete(labels = function(x) stringr::str_wrap(x, width = 3))+
  facet_grid(
    tau ~ mean_smd, 
    labeller = label_bquote(
      rows = tau == .(tau),
      cols = mu == .(mean_smd)
    ),
    scales = "free_y"
  ) +
  labs(
    x = "Selection probability", 
    y = "Scaled Root Mean-Squared Error", 
    color = "Method",
    fill = "Method"
  ) + 
  theme_bw() +
  theme(legend.position = "top")
```

Scaled RMSE captures both bias and variability, providing an overall measure of inaccuracy. Figure @ref(fig:mu-rmse-main), constructed in the same format as Figure @ref(fig:mu-bias-main), shows the scaled RMSE for each method of estimating the average effect size. Additional detail is provided in Figures @ref(fig:rmse-CHE-Beta-main) and @ref(fig:rmse-PET-Beta-main) in Appendix @ref(mu-simulation-results-main), which plot the ratio of RMSEs for each pair of methods to compare their relative accuracy.

Taken together, the figures indicate that no single method achieves the lowest RMSE across all conditions. Instead, each method reflects different bias–variance trade-offs.
The beta-function selection model generally outperforms the others---achieving lower RMSE---when selective reporting is strong or very strong, average effect size is small $(\mu \leq 0.2)$, and heterogeneity is moderate to large $(\tau \geq 0.3)$.
CHE-ISCW consistently has the lowest RMSE when selective reporting is weak or absent.
PET/PEESE performs best when selective reporting is strong to very strong, average effect is small $(\mu \leq 0.2)$, and heterogeneity is low $(\tau \leq 0.15)$.

These bias–variance trade-offs stem from the fact that CHE-ISCW, which does not explicitly adjust for selective reporting, is more prone to bias when selection is present. In contrast, the beta-function selection model tends to produce smaller biases under such conditions. However, when selective reporting is weak or absent, CHE-ISCW is more precise than the methods that adjust for selective reporting (i.e., the beta-function selection model and PET/PEESE). Under those conditions, the added variability introduced by estimating a selection model or PET/PEESE adjustment outweighs the minimal reduction in bias they provide.

#### Confidence Interval Coverage

```{r comparison-coverage-main}
#| fig.width: 9
#| fig.height: 6.5
#| fig.env: "sidewaysfigure"
#| fig.cap: "Coverage levels of confidence intervals for the average effect size based on cluster-robust variance approximations, by method, number of studies, average SMD, and between-study heterogeneity. Dashed lines correspond to the nominal confidence level of 0.95. Coverage rates of the CHE-ISCW and PET/PEESE intervals are not depicted when they fall below 0.5"

mu_graph_res_ci_main %>%
  filter(
    CI_type %in% c("large-sample")
  ) %>%
  ggplot(aes(x = J, y = coverage, color = method, fill = method)) +
  geom_boxplot(alpha = .5, coef = Inf) +
  geom_hline(yintercept = 0.95, linetype = "dashed") +
  coord_cartesian(ylim = c(0.5, 1.0)) + 
  scale_y_continuous(expand = expansion(c(0,0),c(0.02,0))) + 
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  facet_grid(
    tau ~ mean_smd, 
    labeller = label_bquote(
      rows = tau == .(tau),
      cols = mu == .(mean_smd)
    ),
    scales = "free_y"
  ) +
  labs(
    x = "Number of studies (J)", 
    y = "Coverage rate", 
    color = "Method",
    fill = "Method"
  ) + 
  theme_bw() +
  theme(legend.position = "top")

```

Figure \@ref(fig:comparison-coverage-main) shows the coverage rates of 95% confidence intervals based on large-sample cluster-robust variance approximations for the three methods.^[Note that the vertical axis of Figure \@ref(fig:comparison-coverage-main) is restricted to the range [0.5, 1.0], and coverage rates of the intervals based on CHE-ISCW and PET/PEESE are not depicted when they fall below 0.5. Supplementary Figure \@ref(fig:comparison-coverage-full-main) depicts the full range of coverage rates.]
Across most conditions, coverage rates fall below the nominal rate of 0.95 for all methods. However, the beta-function selection model generally achieves higher coverage than the comparison methods, particularly when heterogeneity is moderate to large ($\tau \geq 0.3$), or when heterogeneity is small ($\tau \leq 0.15$), average effect size is small ($\mu \leq 0.2$), and number of studies ($J$) is 60 or more.

In contrast, the confidence intervals produced by the comparison methods are often severely miscalibrated. When CHE-ISCW and PET/PEESE are biased due to selective reporting, their intervals tend to be centered away from the true parameter. As a result, as the number of studies increases, the standard errors of the average effect size estimate—--and thus the interval widths—--shrink, causing coverage rates to decline sharply, in some cases approaching zero.

```{r Beta-coverage-main}
#| fig.width: 9
#| fig.height: 5
#| fig.env: "sidewaysfigure"
#| fig.cap: "Coverage levels of confidence intervals for the average effect size estimated using the beta-function selection model, by bootstrap method, number of studies, average SMD, and between-study heterogeneity. Dashed lines correspond to the nominal confidence level of 0.95."

mu_graph_res_ci_main %>%
  filter(
    bootstrap_condition == "bootstrap",
    estimator %in% c("CML")
    #estimator %in% c("CML"),
    #CI_type %in% c("large-sample","percentile")
  ) %>%
  ggplot(aes(x = J, y = coverage, color = CI_boot_method, fill = CI_boot_method)) +
  geom_boxplot(alpha = .5, coef = Inf) +
  geom_hline(yintercept = 0.95, linetype = "dashed") +
  #scale_y_continuous(limits = c(0.55, 1.0), breaks = seq(0.55,1.0,0.05), expand = expansion(0,0)) +
  scale_y_continuous(limits = c(0.55, 1.0)) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  facet_grid(
    tau ~ mean_smd,
    labeller = label_bquote(
      rows = tau == .(tau),
      cols = mu == .(mean_smd)
    ),
    scales = "free_y"
  ) +
  labs(
    x = "Number of studies (J)",
    y = "Coverage rate",
    color = "Bootstrap Method",
    fill = "Bootstrap Method"
  ) +
  theme_bw() +
  theme(legend.position = "top")

```

Figure \@ref(fig:Beta-coverage-main) depicts the coverage rates of 95% confidence intervals for the average effect size estimated using the beta-function selection model, comparing intervals based on the large-sample cluster-robust variance method to four two-stage bootstrapping methods (percentile, basic, studentized, and bias-corrected-and-accelerated). Due to the computational demands of bootstrapping, we evaluated the bootstrap confidence intervals under a more limited range of data-generating conditions, including a maximum sample size of $J = 60$. Although no method achieves exact nominal coverage across all conditions, the percentile, studentized, and bias-corrected-and-accelerated bootstrap intervals consistently yield coverage rates comparable to—--or better than—--those based on the large-sample cluster-robust variance method. Among these, the percentile bootstrap intervals performed best, achieving coverage above 90% in nearly all conditions examined. <!-- Revisit write up once I can get the figure to show the upper right condition. -->


## Beta-Function Selection Model Compared to 3PSM and 4PSM Step-Function Selection Models
<!-- Need a better header name -->

### Average Effect Size

Convergence rates were higher for the step-function selection models than for the beta-function selection model. Both the 3PSM and 4PSM models had convergence rates of 99% and above across all `r formatC(4 * 4 * 2 * 2 * 5 * 4, big.mark = ",")` conditions, while the beta-function selection model had convergence rates below 99% for `r sum(mu_graph_res_miss$convergence[which(mu_graph_res_miss$method=="Beta")] < .99)` conditions.

#### Bias

```{r mu-bias-miss}
#| fig.width: 9
#| fig.height: 6.5
#| fig.env: "sidewaysfigure"
#| fig.cap: "Bias of the average effect size by method, selection probability, average SMD, and between-study heterogeneity"

ggplot(mu_graph_res_miss) + 
  aes(x = selection_strength, y = bias, color = method, fill = method) +
  geom_hline(yintercept = 0) +
  geom_boxplot(alpha = .5, coef = Inf) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_discrete(labels = function(x) stringr::str_wrap(x, width = 3))+
  facet_grid(
    tau ~ mean_smd, 
    labeller = label_bquote(
      rows = tau == .(tau),
      cols = mu == .(mean_smd)
    ),
    scales = "free_y"
  ) +
  labs(
    x = "Selection probability", 
    y = "Bias", 
    color = "Method",
    fill = "Method"
  ) + 
  theme_bw() +
  theme(legend.position = "top")

```

Figure @ref(fig:mu-bias-miss) displays the bias for the three selection models. Across most conditions, bias is consistently closer to zero when the average effect size is estimated using the beta-function selection model compared to the step-function selection models. Among the step-function selection models, 4PSM generally outperforms 3PSM, though both are more prone to bias when the selection process is misspecified. The main exception occurs when average effect size is moderate to large $(\mu \geq 0.4)$ and heterogeneity is low $(\tau \leq 0.15)$, in which case all three models exhibit essentially zero bias. This pattern is consistent with the fact that the data were generated under a beta-function selection process and suggests that the step-function selection models, particularly 3PSM, are not robust to misspecification of the selection mechanism—--especially when selective reporting is moderate to strong or when average effect is not large $(\mu < 0.8)$.

#### Scaled RMSE

```{r mu-rmse-main-miss}
#| fig.width: 9
#| fig.height: 6.5
#| fig.env: "sidewaysfigure"
#| fig.cap: "Scaled root mean-squared error of the average effect size by method, selection probability, average SMD, and between-study heterogeneity"

ggplot(mu_graph_res_miss) + 
  aes(x = selection_strength, y = scrmse, color = method, fill = method) +
  geom_hline(yintercept = 0) +
  geom_boxplot(alpha = .5, coef = Inf) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_discrete(labels = function(x) stringr::str_wrap(x, width = 3))+
  facet_grid(
    tau ~ mean_smd, 
    labeller = label_bquote(
      rows = tau == .(tau),
      cols = mu == .(mean_smd)
    ),
    scales = "free_y"
  ) +
  labs(
    x = "Selection probability", 
    y = "Scaled Root Mean-Squared Error", 
    color = "Method",
    fill = "Method"
  ) + 
  theme_bw() +
  theme(legend.position = "top")
```

Figure @ref(fig:mu-rmse-miss) presents the scaled RMSE for the three selection models and highlights a clear bias–variance trade-off. When average effect size is zero and selective reporting is moderate to very strong, the results mirror the bias results pattern: the beta-function selection model outperforms the step-function selection models, and the 4PSM performs better than the 3PSM. However, the relative performance shifts under other conditions. When average effect size is $\mu = 0.2$ and selective reporting is strong to very strong, 4PSM yields the lowest RMSE. In contrast, when average effect size is moderate or large ($\mu \geq 0.2$), or when $\mu = 0.2$ and selective reporting is absent to moderate, 3PSM outperforms both alternatives. RMSE is similar across all models when average effect size is moderate to large ($\mu \geq 0.4$) and heterogeneity is low ($\tau \leq 0.15$).

#### Confidence Interval Coverage

```{r comparison-coverage-miss}
#| fig.width: 9
#| fig.height: 6.5
#| fig.env: "sidewaysfigure"
#| fig.cap: "Coverage levels of confidence intervals for the average effect size based on cluster-robust variance approximations, by method, number of studies, average SMD, and between-study heterogeneity. Dashed lines correspond to the nominal confidence level of 0.95. Coverage rates of the 3PSM and 4PSM intervals are not depicted when they fall below 0.5"

mu_graph_res_ci_miss %>%
  filter(
    CI_type %in% c("large-sample")
  ) %>%
  ggplot(aes(x = J, y = coverage, color = method, fill = method)) +
  geom_boxplot(alpha = .5, coef = Inf) +
  geom_hline(yintercept = 0.95, linetype = "dashed") +
  coord_cartesian(ylim = c(0.5, 1.0)) + 
  scale_y_continuous(expand = expansion(c(0,0),c(0.02,0))) + 
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  facet_grid(
    tau ~ mean_smd, 
    labeller = label_bquote(
      rows = tau == .(tau),
      cols = mu == .(mean_smd)
    ),
    scales = "free_y"
  ) +
  labs(
    x = "Number of studies (J)", 
    y = "Coverage rate", 
    color = "Method",
    fill = "Method"
  ) + 
  theme_bw() +
  theme(legend.position = "top")

```

Figure \@ref(fig:comparison-coverage-miss) shows the coverage rates of 95% confidence intervals based on large-sample cluster-robust variance approximations for the three models^[Once again, the vertical axis of Figure \@ref(fig:comparison-coverage-miss) is restricted to the range [0.5, 1.0], and coverage rates of the intervals based on 3PSM and 4PSM are not depicted when they fall below 0.5. Supplementary Figure \@ref(fig:comparison-coverage-full-miss) depicts the full range of coverage rates.]
Coverage rates fall below the nominal 0.95 level for all three selection models across most conditions. However, the beta-function selection model generally achieves higher coverage than the step-function selection models, particularly when heterogeneity is moderate to large ($\tau \geq 0.3$), or when heterogeneity is low ($\tau \leq 0.15$), average effect size is small ($\mu \leq 0.2$), and number of studies ($J$) is 60 or more. As with CHE-ISCW and PET/PEESE, the confidence intervals produced by the step-function selection models are often miscalibrated, due in part to underestimated standard errors, which result in overly narrow intervals. Among the step-function selection models, coverage is generally higher for 4PSM than for 3PSM.





### Effect Size Variance




---
title: "Empirical Application"
output: pdf_document
bibliography: references.bib
---

```{r, echo = F, warning = F, message = F}
library(metaselection)
library(metadat)
library(metafor)
library(tidyverse)
library(here)
#devtools::load_all()

sci <- read_csv(here('tests/testdata','ScienceMeta.csv'))

sci <- sci |>
  rename(esid = es_id, studyid = study_id) |>
  mutate(
    d = yi,
<<<<<<< HEAD
    Var_d = vi,
=======
    Va = vi,
    Var_d = Va,
>>>>>>> step-function-paper
    sd_d = sqrt(Var_d),
    var_d = Var_d,
    p_onesided = 1 - pnorm(d / sd_d)
  )

num_studies <- length(unique(sci$studyid))
num_effects <- length(unique(sci$esid))
```

<<<<<<< HEAD
To demonstrate the interpretation of the beta-function selection model in practice, we reanalyzed the data from a meta-analysis of the effects of science interventions on K–12 student science achievement, conducted by BSCS Science Learning [@taylor2018science]. The meta-analytic sample included findings from `r num_studies` studies with `r num_effects` effects. Approximately half of the studies contributed multiple effects per study, due to either multiple samples or multiple outcomes, leading to dependent effect sizes. Effect sizes were measured as standardized mean differences representing the effects of the interventions on student achievement outcomes, with positive effects corresponding to improvements in student achievement. 
<!-- JEP: Add a contour-enhanced funnel plot of the effect sizes, perhaps colored by assessment type? -->

```{r science-learning-models}
#| echo: false
#| warning: false
#| message: false
#| cache: true

=======
To illustrate the new beta-function selection model in an empirical application, we reanalyzed the data from a meta-analysis conducted by BSCS Science Learning. BSCS Science Learning conducted a meta-analysis of science interventions effects on K–12 student science achievement [@taylor2018science]. The meta-analytic sample included findings from `r num_studies` studies with `r num_effects` effects. Approximately half of the studies contributed multiple effects per study, due to either multiple samples or multiple outcomes, leading to dependent effect sizes. Effect sizes were measured as standardized mean differences representing the effects of the interventions on student achievement outcomes. We conducted the analyses using R Version 4.4.3 [@rcoreteam].

```{r, echo = F, warning = F, message = F}
>>>>>>> step-function-paper
# Unadjusted model
sci_unadj_mean <- rma.uni(
  data = sci,
  yi = d,
  sei = sd_d,
  method = "ML"
<<<<<<< HEAD
) |> 
  robust(cluster = studyid, clubSandwich = TRUE)
=======
)

sci_unadj_mean_cil <- sci_unadj_mean$tau2 - 1.96*sci_unadj_mean$se.tau2
sci_unadj_mean_ciu <- sci_unadj_mean$tau2 + 1.96*sci_unadj_mean$se.tau2
>>>>>>> step-function-paper

# Beta model
sci_adj_beta_mean <- selection_model(
  dat = sci,
  yi = d,
  sei = sd_d,
  pi = p_onesided,
  cluster = studyid,
  selection_type = "beta",
  steps = c(.025,.975),
  make_sandwich = TRUE
)

<<<<<<< HEAD

=======
>>>>>>> step-function-paper
# Single-step model
sci_adj_step3_mean <- selection_model(
  dat = sci, 
  yi = d,
  sei = sd_d,
  cluster = studyid,
  selection_type = "step",
  steps = 0.025
)

# Two-step model
sci_adj_step4_mean <- selection_model(
  dat = sci, 
  yi = d,
  sei = sd_d,
  cluster = studyid,
  selection_type = "step",
  steps = c(0.025, .5)
)
<<<<<<< HEAD


# Unadjusted model - moderator

=======
```

We used the `rma.uni()` function from the `metafor` package to fit a standard random-effects model based on an independent effects working model with random effects for each study [@Viechtbauer2010conducting]. To adjust for selective reporting, we used the `selection_model()` function from the `metaselection` package to fit the new beta-function selection model along with our recently developed step-function selection model [@metaselection]. Both selection models are based on an independent effects working model with random effects for each study and account for dependent effects using RVE. For the step-function model, we estimated both a single-step model with a threshold at $\alpha_1 = 0.025$ and a two-step model with thresholds at $\alpha_1 = 0.025$ and $\alpha_2 = 0.5$.

The overall estimate of the average effect using the unadjusted model is `r round(as.numeric(sci_unadj_mean$beta), 3)`, 95% CI [`r round(as.numeric(sci_unadj_mean$ci.lb), 3)`, `r round(as.numeric(sci_unadj_mean$ci.ub), 3)`], which is significantly different from zero ($p$ = `r round(sci_unadj_mean$pval, 3)`). The estimates from the selection models are also all positive and significant, but they vary in magnitude and thus their adjustment for selective reporting. Compared to the unadjusted model, the average adjusted effect is 25% small when using the beta-function selection model ($ES$ = `r round(sci_adj_beta_mean$est$Est[sci_adj_beta_mean$est$param=="beta"], 3)`), 32% larger when using the single-step model ($ES$ = `r round(sci_adj_step3_mean$est$Est[sci_adj_step3_mean$est$param=="beta"], 3)`), and 12% larger when using the two-step selection model ($ES$ = `r round(sci_adj_step4_mean$est$Est[sci_adj_step4_mean$est$param=="beta"], 3)`).

```{r, echo = F, warning = F, message = F}
# Unadjusted model - moderator
>>>>>>> step-function-paper
sci_unadj_mod <- rma.uni(
  data = sci,
  yi = d,
  sei = sd_d,
  method = "ML",
  mods = ~ outcome_type_author
<<<<<<< HEAD
) |>
  robust(cluster = studyid, clubSandwich = TRUE)
=======
)

sci_unadj_mod_cil <- sci_unadj_mod$tau2 - 1.96*sci_unadj_mod$se.tau2
sci_unadj_mod_ciu <- sci_unadj_mod$tau2 + 1.96*sci_unadj_mod$se.tau2
>>>>>>> step-function-paper

# Beta model - moderator
sci_adj_beta_mod <- selection_model(
  dat = sci,
  yi = d,
  sei = sd_d,
  pi = p_onesided,
  cluster = studyid,
  selection_type = "beta",
  steps = c(.025,.975),
  make_sandwich = TRUE,
  mean_mods = ~ outcome_type_author
)

# Single-step model - moderator
sci_adj_step3_mod <- selection_model(
  dat = sci, 
  yi = d,
  sei = sd_d,
  cluster = studyid,
  selection_type = "step",
  steps = 0.025,
  mean_mods = ~ outcome_type_author
)

# Two-step model - moderator
sci_adj_step4_mod <- selection_model(
  dat = sci, 
  yi = d,
  sei = sd_d,
  cluster = studyid,
  selection_type = "step",
  steps = c(0.025, .5),
  mean_mods = ~ outcome_type_author
)
<<<<<<< HEAD

```

We conducted the analyses using R Version 4.4.3 [@rcoreteam].
We first used the `rma.uni()` function from the `metafor` package to fit a standard random-effects model based on an independent effects working model with random effects for each study [@Viechtbauer2010conducting], with standard errors clustered by study. 
The overall estimate of the average effect using the unadjusted model is `r formatC(sci_unadj_mean$beta, format = "f", digits = 2)`, cluster-robust 95% CI [`r formatC(sci_unadj_mean$ci.lb, format = "f", digits = 2)`, `r formatC(sci_unadj_mean$ci.ub, format = "f", digits = 2)`], which is significantly different from zero $(p `r format.pval(sci_unadj_mean$pval, digits = 3, eps = 1e-3)`)$. 
The unadjusted estimate of heterogeneity is $\tau = `r formatC(sqrt(sci_unadj_mean$tau2), format = "f", digits = 2)`$.

To adjust for selective reporting, we used the `selection_model()` function from the `metaselection` package to fit the beta-function selection model (with default truncation thresholds of $\alpha_1 = .025, \alpha_2 = .975$) along with two step-function selection models [@metaselection]. Both selection models are based on an independent effects working model with random effects for each study and account for dependent effects using robust variance estimation, clustering by study. For the step-function models, we estimated both a single-step model with a threshold at $\alpha_1 = 0.025$ and a two-step model with thresholds at $\alpha_1 = .025$ and $\alpha_2 = .5$.

```{r science-learning-res, tab.pos = "tb", tab.cap="science learning meta-analysis"}

all_res_mean <- 
  bind_rows(
    beta = sci_adj_beta_mean$est,
    `one-step` = sci_adj_step3_mean$est,
    `two-step` = sci_adj_step4_mean$est,
    .id = "Model"
  ) %>%
  rownames_to_column() 

all_res_mod <- 
  bind_rows(
    beta = sci_adj_beta_mod$est,
    `one-step` = sci_adj_step3_mod$est,
    `two-step` = sci_adj_step4_mod$est,
    .id = "Model"
  ) %>%
  rownames_to_column()

all_res <- 
  bind_rows(
    `Summary meta-analysis` = all_res_mean,
    `Moderator analysis` = all_res_mod,
    .id = "Specification"
  ) %>%
  mutate(
    Specification = factor(Specification, levels = c("Summary meta-analysis","Moderator analysis")),
    across(c(Est, CI_lo, CI_hi), ~ case_when(
      param == "gamma" ~ exp(.x / 2), 
      param %in% c("zeta1","zeta2") ~ exp(.x),
      TRUE ~ .x
    )),
    param = recode(param, 
                   beta = "$\\beta_0$", 
                   `beta_(Intercept)` = "$\\beta_0$",
                   beta_outcome_type_author = "$\\beta_1$",
                   gamma = "$\\tau$", 
                   zeta1 = "$\\lambda_1$", 
                   zeta2 = "$\\lambda_2$"),
  )

all_res_table <- 
  all_res %>%
  mutate(
    Est_SE = paste0(formatC(Est, format = "f", digits = 2), " (",formatC(SE, format = "f", digits = 2),")"),
    CI = paste0("[", formatC(CI_lo, format = "f", digits = 2),", ", formatC(CI_hi, format = "f", digits = 2), "]")
  ) %>%
  select(Specification, Model, param, Est_SE, CI) %>%
  pivot_wider(names_from = Model, values_from = c(Est_SE, CI), names_vary = "slowest")

library(kableExtra)
options(knitr.kable.NA = '')

Specification_group <- table(all_res_table$Specification)

all_res_table %>%
  select(-Specification) %>%
  kbl(
    caption = "Selection model parameter estimates fit to science intervention effects from Taylor et al. (2018)",
    col.names = c("Parameter", rep(c("Est. (SE)", "95\\% CI"), times = 3)),
    booktabs = TRUE,
    escape = FALSE,
    table.attr = "tb"
  ) %>%
  kable_styling() %>%
  add_header_above(c(" ", "Beta density" = 2, "One-step" = 2, "Two-step" = 2)) %>%
  pack_rows(index = Specification_group) %>%
  footnote(
    general = c("Est. = estimate; SE = cluster-robust standard error; CI = cluster-robust confidence interval.")
  )
```


The upper panel of Table \@ref(tab:science-learning-res) reports parameter estimates from all three models, along with cluster-robust standard errors and 95% confidence intervals.
Across the three selection models, the estimated average effect sizes are all positive and significant, but they vary in magnitude and interpretation. Compared to the unadjusted model, the average adjusted effect is 25% smaller when using the beta-density selection model ($ES$ = `r round(sci_adj_beta_mean$est$Est[sci_adj_beta_mean$est$param=="beta"], 3)`), versus 32% larger when using the single-step model ($ES$ = `r round(sci_adj_step3_mean$est$Est[sci_adj_step3_mean$est$param=="beta"], 3)`) and 12% larger when using the two-step selection model ($ES$ = `r round(sci_adj_step4_mean$est$Est[sci_adj_step4_mean$est$param=="beta"], 3)`).
These differences are due to differences in the form of the selection curve estimated from each model specification, as depicted in figure \@ref(fig:science-selection-curves).
The beta-density model estimates of $\lambda_1$ and $\lambda_2$ imply that positive but non-significant results are slightly more likely to be reported compared to affirmative results, but that negative effects are increasingly less likely to be be reported.
On balance, this leads to an adjusted average effect size estimate that is smaller than under the unadjusted random effects model.
In contrast, the step-function models both estimate that non-significant results are more likely to be reported compared to affirmative results, leading to upward adjustment in the average effect size.

```{r science-selection-curves}
#| echo: false
#| fig.retina: 2
#| fig.width: 5
#| fig.height: 5.5
#| out.width: 75%
#| fig.cap: "Estimated selection functions based on beta density, one-step, and two-step selection models"
#| fig.pos: tb 

pvals <- seq(0.001, 0.999, .001)

selection_curves <- 
  bind_rows(
    beta = selection_wts(sci_adj_beta_mean, pvals = pvals),
    `one-step` = selection_wts(sci_adj_step3_mean, pvals = pvals),
    `two-step` = selection_wts(sci_adj_step4_mean, pvals = pvals),
    .id = "Model"
  )

ggplot(selection_curves) + 
  aes(p, wt, color = Model, fill = Model) + 
  geom_vline(xintercept = c(.025, .975), linetype = "dashed") + 
  geom_area(alpha = 0.5) + 
  facet_wrap(~ Model, ncol = 1) + 
  scale_x_continuous(
    limits = c(0,1), 
    expand = expansion(0,0), 
    transform = "asn",
    breaks = c(0,.025, .10, .25, .5, .75, .90, .975, 1)
  ) + 
  theme_light() + 
  labs(x = "p-value (one-sided)", y = "Selection weight") + 
  theme(legend.position = "none", strip.text = element_text(color = "black"))
```

A key advantage of selection models over other methods for diagnosing and adjusting for selective reporting bias is that they allow for inclusion of potential moderator variables as predictors of average effect size. This feature allows meta-analysts to distinguish selective reporting bias from systematic variation in effect sizes that could be explained by primary study characteristics. 
Past work has demonstrated that evaluations of educational interventions tend to produce larger effects on assessments developed by study authors or developers of the intervention (i.e., non-independent groups) than on assessments developed by groups independent of the evaluation [@Wolf2023making].
We investigated whether this pattern holds in the BSCS Science Learning meta-analysis data, which includes an indicator for assessment type, equal to one for assessments developed by non-independent groups and equal to zero otherwise.

We reran all four models using assessment type as a moderator [@taylor2018science], with assessments developed by independent groups as the reference category.
Based on a random effects meta-regression without adjustment for selective reporting, assessments developed by independent groups produced an average effect size of `r formatC(sci_unadj_mod$beta[1], format = "f", digits = 2)`, 95% CI [`r formatC(sci_unadj_mod$ci.lb[1], format = "f", digits = 2)`, `r formatC(sci_unadj_mod$ci.ub[1], format = "f", digits = 2)`].
The average effect on assessments developed by non-independent groups was larger by `r formatC(sci_unadj_mod$beta[2], format = "f", digits = 2)` SD, 95% CI [`r formatC(sci_unadj_mod$ci.lb[2], format = "f", digits = 2)`, `r formatC(sci_unadj_mod$ci.ub[2], format = "f", digits = 2)`], which was not statistically distinct from zero $(p = `r format.pval(sci_unadj_mod$pval[2], digits = 3, eps = 1e-3)`)$. 
The unadjusted estimate of heterogeneity is $\tau = `r formatC(sqrt(sci_unadj_mod$tau2), format = "f", digits = 2)`$, just as in the summary meta-analysis.

The lower panel of Table \@ref(tab:science-learning-res) reports parameter estimates from the corresponding selection models that include assessment type as a predictor.
The $\beta_1$ coefficient estimates represent differences in average effects between non-independent and independent assessment developers, which were all non-significant and similar in magnitude to the difference based on the unadjusted model.
However, the selection models lead to different about the average effects on independently developed assessments.
The estimate from the beta-density model is not statistically distinct from zero and is smaller than the estimate from the unadjusted random effects meta-regression. 
In contrast, the estimated average effects from the step-function selection models are both statistically significant and larger than the estimate from the unadjusted model.
Selection parameter estimates are very similar to the estimates from the corresponding summary meta-analysis models.

The model estimates reveal some key differences between the selection models. In this dataset, the beta-function selection model adjusts the estimates downward---so much so in the moderator analysis that neither estimate remains statistically significant. The step-function selection model, on the other hand, adjusts the estimates upward, though the adjustment is minimal when using the two-step model. 
These differences suggest that the models may behave differently in different conditions. 
<!-- JEP: This is pretty vague. -->
To investigate this, we conducted simulations across a wide range of conditions. The simulations will allow us to draw firmer conclusions about the comparative performance of these methods, including their robustness to misspecification of the selection function.
=======
```

A key advantage of selection models over other methods for diagnosing and adjusting for selective reporting bias is their ability to include both discrete and continuous moderators. This allows meta-analysts to distinguish selective reporting bias from systematic variation in effect sizes that can be explained by primary study characteristics. The BSCS Science Learning meta-analytic data includes a number of potential moderators of effect size. Among these is an indicator for assessment author type, defined as 1 for assessment developer is the author or researcher and 0 otherwise. We reran all four models using assessment author type as a moderator [@taylor2018science].

When the assessment developer is the author or researcher, the unadjusted estimate is `r round(as.numeric(sci_unadj_mod$beta[2]), 3)`, 95% CI [`r round(as.numeric(sci_unadj_mod$ci.lb[2]), 3)`, `r round(as.numeric(sci_unadj_mod$ci.ub[2]), 3)`], which is not significantly different from zero ($p$ = `r round(sci_unadj_mod$pval[2], 3)`). The estimates from the selection models are also all nonsignificant and ranged from 
`r round(sci_adj_step3_mod$est$Est[sci_adj_step3_mod$est$param=="beta_outcome_type_author"], 3)` to `r round(sci_adj_beta_mod$est$Est[sci_adj_beta_mod$est$param=="beta_outcome_type_author"], 3)`. However, when the assessment developer is not the author or researcher, the models lead to different conclusions. The estimate from the unadjusted model is `r round(as.numeric(sci_unadj_mod$beta[1]), 3)` and significant (95% CI [`r round(as.numeric(sci_unadj_mod$ci.lb[1]), 3)`, `r round(as.numeric(sci_unadj_mod$ci.ub[1]), 3)`], $p$ = `r round(sci_unadj_mod$pval[1], 3)`). The estimate from the beta-function selection model is `r round(sci_adj_beta_mod$est$Est[sci_adj_beta_mod$est$param=="beta_(Intercept)"], 3)` and nonsignificant ($p$ = `r round(sci_adj_beta_mod$est$p_value[sci_adj_beta_mod$est$param=="beta_(Intercept)"], 3)`).
The estimates from the step-function selection models are both significant, with estimates of `r round(sci_adj_step3_mod$est$Est[sci_adj_step3_mod$est$param=="beta_(Intercept)"], 3)` for the single-step selection model and `r round(sci_adj_step4_mod$est$Est[sci_adj_step4_mod$est$param=="beta_(Intercept)"], 3)` for the two-step selection model.

<!-- I'd love to put the estimates in a table, I just don't know how. -->
The model estimates reveal some key differences between estimation methods. In this dataset, the beta-function selection model adjusts the estimates downward---so much so in the moderator analysis that neither estimate remains statistically significant. The step-function selection model, on the other hand, adjusts the estimates upward, though the adjustment is minimal when using the two-step model. These differences suggest that the models may behave differently in different conditions. To investigate this, we conducted simulations across a wide range of conditions. The simulations will allow us to draw firmer conclusions about the comparative performance of these methods, including their robustness to misspecification of the selection function.
>>>>>>> step-function-paper

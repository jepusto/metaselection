---
title: "Introduction"
output: pdf_document
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: sentence
---

Meta-analysis is a critical tool for synthesizing evidence across studies to draw generalizable conclusions in fields such as education, psychology, and medicine. However, the validity of meta-analytic findings depends on the completeness and representativeness of the available data. A persistent threat to validity is selective reporting, where effect size estimates are more likely to be published, and thus included in a meta-analysis, if they are statistically significant or consistent with researchers' hypotheses [@carter2019correcting]. Extensive evidence from diverse domains suggests that such reporting biases are widespread [e.g., @oBoyle2017chrysalis; @chan2004empirical; @franco2016underreporting; @john2012measuring; @lancee2017outcome; @pigott2013outcome], resulting in distorted estimates of intervention effects and potentially misleading conclusions.

To address selective reporting bias, a variety of statistical approaches have been developed.<!-- Cite examples/papers? --> Among the most promising are $p$-value selection models, which assume that the probability of an effect being reported depends on its statistical significance. These models are appealing because they are based on explicit assumptions about the selection mechanism and can be integrated into conventional meta-analytic frameworks (e.g., meta-regression). The step-function selection model, originally developed by @hedges1992modeling and later extended by @vevea1995general, assumes a piece-wise constant selection probability across different $p$-value thresholds (e.g., $\alpha = .05$). This approach captures plausible patterns of selective reporting and has been shown to outperform simpler diagnostics or regression-based adjustments, particularly when effect sizes are heterogeneous [@carter2019correcting; @Terrin2003heterogeneity].

Most existing methods for assessing and/or correcting for selective reporting---including $p$-value selection models---have been developed under the assumption that each study contributes a single, independent effect size. However, this assumption is increasingly unrealistic. Many meta-analyses now include multiple, dependent effect sizes per study, such as estimates derived from different outcomes, time points, or treatment comparisons. Our recent work [@pustejovsky2025step] addressed this gap by integrating the step-function selection model with robust variance estimation (RVE) and bootstrap methods to account for dependent effect sizes. Through simulation studies, we demonstrated that this approach reduces bias in the estimate of the overall effect size, but that there is a bias variance trade-off relative to the unadjusted meta-analytic model. Moreover, cluster bootstrapping leads to confidence intervals with coverage rates that are close to the nominal level of 0.95.

While the step-function selection model provides a structured and intuitive way to characterize selective reporting, it relies on the meta-analyst to specify "psychologically salient" $p$-value thresholds, such as 0.05 or 0.01. In practice, the true pattern of selection may not conform neatly to such step-wise forms. To address this limitation, the present paper introduces a beta-function selection model that allows the selection probability to vary smoothly as a function of the $p$-value by using the beta density to model the selection process. This model builds on earlier work by @citkowicz2017parsimonious, extending it to the context of dependent effect sizes by incorporating RVE and bootstrap methods. The beta-function selection model both offers greater flexibility in capturing diverse forms of selection in meta-analyses with dependent effect sizes and allows meta-analysts to assess whether the form of selection matters.


We begin the paper by formally describing the beta-function selection model and outlining our proposed estimation and inference procedures. We then illustrate the model using a previously published meta-analysis, highlighting how it can reveal patterns of selective reporting not captured by the step-function selection model. Next, we report findings from an extensive simulation study evaluating the modelâ€™s performance under a variety of conditions. We conclude with summary findings, limitations, directions for future research, and implications for practice.
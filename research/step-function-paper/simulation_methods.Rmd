---
title: "Simulation Methods"
output: pdf_document
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: sentence
---

We conducted Monte Carlo simulation studies to examine the performance
of the CML and ARGL estimators for a step-function selection model with a single step, under a wide range of conditions where primary studies contribute multiple, statistically dependent effect size estimates. 
We compare the performance of these novel estimators to two available alternatives: a summary meta-analysis that addresses the dependency structure but does not correct for selective reporting and the PET-PEESE adjustment [@stanley2014meta].
We evaluated the estimators in terms of convergence rates, bias, accuracy, and confidence interval coverage for recovering the average effect size prior to selective reporting.
To limit the computational burden, we evaluated the performance of bootstrap confidence intervals only for a subset of the conditions. 

We ran the simulation in R Version 4.4.1 [@rcoreteam] using the high-throughput computing cluster at the University of Wisconsin - Madison [@CHTC].
The simulation code made use of several R packages, including `metafor` [@Viechtbauer2010conducting], `clubSandwich` [@clubSandwich], `simhelpers` [@simhelpers], `optimx` [@optimx], `nleqslv` [@nleqslv], and `tidyverse` [@tidyverse].

## Data generation

We simulated meta-analyses based on a CHE working model, with individual effect size estimates selected
for inclusion based on the step-function selection model with a single step at $\alpha_1 = .025$. 
For each replication, we generated a total of $J^*$ studies with a two-group comparison
design, where study $j$ had an effective sample size of $N_j$ and contributed $k_j^* \geq 1$ effect size estimates prior to selective reporting.
To generate each meta-analytic dataset, we sampled effective sample sizes[^ESS] and numbers of effect sizes per study from an empirical distribution based on the What Works Clearinghouse database. 
The total effective sample size per study was divided equally into two groups, treatment and control. 
We then generated $r_j$, an outcome correlation for study $j$, by drawing from a beta distribution with mean $\rho$ and standard deviation 0.05. 
We assumed constant correlation between pairs of outcomes within a study but allowed the correlation to vary from study to study.

[^ESS]: For studies that involved cluster-level treatment assignment, we computed effective sample sizes that account for the dependence of observations nested within clusters rather than using the raw participant-level sample sizes.

We then simulated raw outcomes for each primary study included in the
meta-analytic dataset. To do so, we first generated an average effect
size per study, $\delta_j$, from a normal distribution with mean $\mu$ and variance
of $\tau_B^2$. Given $\delta_j$, we generated $k_j^*$ effect size parameters per study, $\bs\delta_j = \left(\delta_{1j},...,\delta_{k_j^* j}\right)'$, from a normal distribution with mean of $\delta_j$ and
variance of $\omega^2$.
For each of the $N_j / 2$ participants in the treatment and control groups, we then simulated vectors of multivariate normal outcomes as $\mat{Y}_{hj}^T \sim N(\bs\delta_j, \boldsymbol\Psi_j)$ and $\mat{Y}_{hj}^C \sim N(\mat{0}, \boldsymbol\Psi_j)$, where $\mat{Y}_{hj}^T$ and $\mat{Y}_{hj}^C$ are
$k_j^* \times 1$ vectors of outcomes for participant $h$ in study $j$ in the treatment and control group, respectively, and $\boldsymbol\Psi_j$ is the covariance matrix for outcomes in study $j$, with diagonal elements equal to 1 and off-diagonal elements equal to $r_j$.
From the raw outcome data, we calculated standardized mean
differences using Hedges's $g$ bias correction for each of the correlated outcomes, yielding effect size estimates $y^*_{ij}$ for $i=1,...,k_j^*$ and $j = 1,...,J^*$.
We calculated the sampling variances using conventional formulas [@borenstein2019effect] and computed one-sided $p$-values based on two-sample $t$-tests for the null of $H_0: \delta_{ij} \leq 0$.

After simulating results for study $j$, we applied a step-function selection
model to the individual effect size estimates, with a one-sided selection
threshold at $\alpha = 0.025$. Specifically, we let result $(y^*_{ij},
\sigma_{ij}^*, p_{ij}^*)$ be included in the observed dataset with probability
1 if $p_{ij}^*$ was less than 0.025 and with probability $0 < \lambda_1 \leq 1$ if $p_{ij}^* \ge 0.025$. 
We repeated the process of generating studies until the database included a total of $J$ studies with at least one observed result, where observed study $j$ includes $k_j$ effect size estimates for $1 \leq k_j \leq k_j^*$.

## Estimation methods

We estimated step-function selection models with a single step at $\alpha_1 = 0.025$, so that the assumed marginal selection process is consistent with the actual selective reporting process used to generate meta-analytic datasets.
We estimated CML and ARGL estimators and calculated cluster-robust confidence intervals as described in Section \@ref(estimation-methods). 
For a subset of simulation conditions, we also examined percentile, basic, studentized, and bias-corrected-and-accelerated bootstrap confidence intervals based on the non-parametric two-stage bootstrap, clustered bootstrap, and fractional random weight bootstrap as described in `r appendix_prefix` \@ref(bootstrap-details).
To maintain computational feasibility, we used $B = 399$ bootstrap replications of each estimator.

We compared the performance of the step-function CML and ARGL estimators to that of two alternative methods.
First, we estimated a summary meta-analysis model without any correction for selective reporting, using a method that accounts for effect size dependency. 
Specifically, we used the CHE working model with inverse sampling-covariance weights (CHE-ISCW) [@chen2024adapting].
We estimated the variance components using restricted maximum likelihood and assumed a sampling correlation of 0.8 for all pairs of effect size estimates from the same study, which leads to a degree of mis-specification when the average correlation used in the data-generating process differs from 0.80.
Second, we implemented a variation of the PET/PEESE estimator originally proposed by Stanley and Doucouliagos[@stanley2014meta], adapted to accommodate dependent effect sizes.
Following convention, we combined the estimators by using PEESE if the PET estimator is statistically distinct from zero at an $\alpha$-level of 0.10, and otherwise using PET.[@stanley2014meta]
For the PET/PEESE and CHE-ISCW estimators, we calculated confidence intervals using cluster-robust variance estimation with the CR2 small-sample correction and Satterthwaite degrees of freedom [@chen2024adapting].

## Experimental design

Table \@ref(tab:sim-design) summarizes the experimental design for this study. 
Manipulated parameters included overall average standardized mean difference
($\mu$), between-study heterogeneity ($\tau_B$), within-study heterogeneity ratio
($\omega^2 / \tau_B^2$), average correlation between outcomes ($\rho$),
probability of selection for non-affirmative results ($\lambda_1$), number of observed studies ($J$), and primary study sample size. 
We examined values for the overall average SMD ($\mu$) ranging from 0.0 to 0.80, which covers the range of effects observed in a review of 747 randomized control trials of education interventions [@kraft2020interpreting].
We used values of $\tau$ ranging from 0.05 (a very small degree of heterogeneity) to 0.45 (a large degree of heterogeneity). 
We specified the degree of within-study heterogeneity in relative terms, by setting the ratio of $\omega^2$ to between-study heterogeneity $\tau^2$ at either 0 (i.e., no within-study heterogeneity) or 0.5. 
For the average correlation between outcomes from the same study, we examined the values of 0.40 or 0.80. The default value of the average
correlation in software packages that implement RVE is 0.80. 
Thus, in conditions where $\rho$ is 0.80, the working model is approximately correctly specified. 
We included conditions where $\rho = 0.4$ to examine performance when the working model is not correctly specified. 

```{r sim-design, tab.cap="Simulation design parameters"}
library(kableExtra)
library(tidyverse)

dat <- tibble(
  Parameter = c(
    "Overall average SMD ($\\mu$)",
    "Between-study heterogeneity ($\\tau_B$)",
    "Heterogeneity ratio ($\\omega^2 / \\tau_B^2$)",
    "Average correlation between outcomes ($\\rho$)",
    "Probability of selection for non-affirmative effects ($\\lambda_1$)",
    "Number of observed studies ($J$)",
    "Primary study sample size"
  ),
  `Full Simulation` = c(
    "0.0, 0.2, 0.4, 0.8",
    "0.05, 0.15, 0.30, 0.45",
    "0.0, 0.5",
    "0.40, 0.80",
    "0.02, 0.05, 0.10, 0.20, 0.50, 1.0",
    "15, 30, 60, 90, 120",
    "Typical, Small"
  ),
  `Bootstrap Simulation` = c(
    "0.0, 0.2, 0.4, 0.8",
    "0.15, 0.45",
    "0.0, 0.5",
    "0.80",
    "0.05, 0.20, 1.0",
    "15, 30, 60",
    "Typical, Small"
  )
)

kable(
  dat, 
  caption = "Parameter values examined in the simulation study",
  booktabs = TRUE, 
  escape = FALSE,
  position = "tb"
) |>
  kable_styling() |>
  column_spec(1, width = "2.5in")
  
```

We examined a wide range of values for the probability of selection for non-affirmative effect sizes, ranging from no selective reporting ($\lambda_1 = 1$) to very severe selective reporting $(\lambda_1 = 0.02)$.
We also examined a wide range of conditions for the number of primary studies included in the meta-analysis, ranging from relatively small databases of $J = 15$ to very large databases with $J = 120$ studies. 
We chose these values to cover the conditions found in real meta-analyses of education and psychology research [@tipton2019current]. 

Lastly, we investigated the primary study sample size. 
For the typical primary study sample sizes, we used the empirical distribution of sample sizes in the What Works Clearinghouse database of findings from educational intervention studies.
<!--# did we restrict the data? -->
The sample sizes in the database ranged from 37 to 2,295 with a median of 211. 
The number of effect sizes ranged from 1 to 48 with a median of 3.
To explore the influence of the effective sample size distribution, we also ran conditions in which we divided the sample
sizes from the What Works Clearinghouse database by three to represent primary studies with smaller sample sizes,
such as those used in psychology laboratory studies.

Parameters were fully crossed for a total of $4 \times 4 \times 2 \times 2 \times 6 \times 5 \times 2 = `r formatC(4 * 4 * 2 * 2 * 6 * 5 * 2, big.mark = ",")`$ conditions in the full simulation study. 
Due to the computational demands of bootstrapping, we focused the bootstrap simulations on conditions with fewer studies per meta-analysis, for which we expected large-sample cluster-robust CIs to be relatively less effective.
We also reduced the number of parameter values for factors where we did
not observe much variation in results in the full simulation
(e.g., excluding $\tau = 0.30$). 
This resulted in $4 \times 2 \times 2 \times 1 \times 3 \times 3 \times 2 = `r formatC(4 * 2 * 2 * 1 * 3 * 3 * 2, big.mark = ",")`$ conditions for the bootstrap simulations. 
For each condition, we generated 2,000 replications. 

## Performance criteria

We evaluated the performance of these methods in terms of convergence rates, bias,
scaled root mean-squared error (RMSE), and 95% confidence interval coverage for the overall average effect size $\mu$.
Because we expected that RMSE would decrease proportionally with the square-root of the number of studies, we scaled the RMSE of each estimator by $\sqrt{J}$ to reduce variation across the number of studies included in each meta-analysis. 
Because the sampling distribution of the CML and ARGL estimators sometimes included extreme outlying values, we calculated bias and scaled RMSE after winsorizing the distribution.
Specifically, we defined a lower fence of 2.5 times the inter-quartile range below the 25th percentile and an upper fence of 2.5 times the inter-quartile range above the 75th percentile. 
Estimates falling below the lower fence or above the upper fence were set to the corresponding fence values.

For confidence intervals based on cluster-robust variance estimation, we calculated coverage rates as the proportion of simulated intervals that included the true parameter. 
For bootstrap confidence intervals, estimation of coverage rates is complicated by the fact that coverage is affected by the number of bootstrap replications. 
To mitigate computational demand, we used few bootstraps per replication than recommended for analysis of real data. 
To estimate coverage rates for confidence intervals as would be used in practice, we used an extrapolation technique @boos2000montecarlo. 
For each replication, we computed bootstrap confidence intervals not only for $B = 399$, but also for $B = 49$, 99, 199, and 299 bootstraps, randomly selected without replacement.
We computed coverage rates separately for each value of $B$, fit a linear regression of the coverage rate on $1 / B$, and then used this regression to predict the coverage rate of confidence intervals based on $B = 1999$ bootstraps.

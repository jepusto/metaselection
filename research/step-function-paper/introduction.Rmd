---
title: "Introduction"
output: pdf_document
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: sentence
---

The validity of conclusions from meta-analytic syntheses depend critically on the reporting practices of researchers, journal editors, and peer reviewers.
If the findings accessible to meta-analysts are not a representative record of the research that has been conducted on a topic, then meta-analytic summaries may be systematically biased.[@Rothstein2005publication]
Of particular concern is the possibility that results from primary studies are selectively reported in ways that can distort the evidence available for synthesis, such as reporting findings that are statistically significant but omitting findings that are null or not consistent with researchers' hypotheses.[@carter2019correcting]

Evidence from medical, educational, and social sciences provides indications of the prevalence of selective reporting. For example, studies have found that statistically significant outcomes were 2.4 to 4.7 times more likely to be published than non-significant outcomes in the medical sciences [@chan2004empirical] and 2.4 times more likely in education.[@pigott2013outcome] Research across various fields, from clinical trials of antipsychotics [@lancee2017outcome] to psychology studies [@john2012measuring; @franco2016underreporting] and management research [@oBoyle2017chrysalis], indicates that selective outcome reporting is widespread and often driven by statistical significance.

Because selective reporting is of such central concern for meta-analysis, a wide range of statistical tools have been developed for assessing the presence of selective reporting and reducing the biases it creates. [@Rothstein2005publication; @marksanglin2020historical]
One common graphical diagnostic is the funnel plot, a simple scatterplot of effect size estimates versus a measure of their precision. [@light1984Summing; @Sterne2005funnel; @kossmeier2020PowerEnhanced] Widely used statistical diagnostics include the rank correlation test [ @begg1994operating]; Egger's regression test [@egger1997bias; @harbord2006modified; @moreno2012generalized; @pustejovsky2019testing]; the trim-and-fill adjustment [@duval2000nonparametric; @duval2000trim]; and various regression adjustment methods including the precision effect test (PET), precision effect estimate with standard error (PEESE), and PET-PEESE technique [@stanley2014meta], and the endogenous kink meta-regression [@bom2019kinked].

Another class of methods for assessing and correcting selective reporting are $p$-value selection models.
Such models build on summary meta-analysis or meta-regression models by making specific, explicit assumptions about the selection function, or how the probability that an effect size estimate is reported relates to sign and statistical significance level of the effect. Building on earlier proposals [@hedges1984estimation; @iyengar1988selection; @dear1992approach], Vevea and Hedges proposed _step-function_ models where the selection function is piece-wise constant, with steps at psychologically salient significance levels such as $\alpha = .05$.[@hedges1992modeling; @vevea1995general]
Other forms involve selection functions based on beta densities [@citkowicz2017parsimonious], power curves, and a variety of other parametric forms. [@preston2004adjusting]

Selection models have several advantages over other available methods for diagnosing and adjusting for selective reporting bias.
First, they are generative models with parameters that directly describe the selective reporting process; they are therefore more interpretable than tests or adjustments for small-study effects, which are agnostic with respect to the specific mechanism of selective reporting. 
Second, selection models can allow for effect heterogeneity with a random effect term. 
Findings from simulations indicate that selection models outperform simpler alternative methods when effect sizes are heterogeneous. [@carter2019correcting; @Terrin2003heterogeneity]
Third, selection models can incorporate both discrete and continuous moderators, enabling one to distinguish between selective reporting bias and systematic differences in effect size that can be predicted by primary study characteristics.[@hedges2005selection]
In addition to these features, the step-function selection model is particularly useful because it allows one to specify cut points that capture simple but plausible forms of selective reporting (e.g., p < 0.01 and p < 0.05).[@greenwald1975prejudice; @Nelson1986significance; @Rosenthal1963significance; @Rosenthal1964significance]

## Dependent effect sizes

The vast majority of the work on selective reporting has focused on methods appropriate for relatively simple summary meta-analyses in which each included study contributes a single independent effect size estimate.
This presents a problem for syntheses in education, psychology, and many other areas, where meta-analyses routinely include studies with multiple, dependent effect sizes. 
Effect size dependencies occur when multiple effect sizes are extracted from the same sample, resulting in statistically dependent estimates.
Dependent effect size estimates commonly occur (1) when multiple outcome measures are collected on the same sample; (2) when the same sample is measured over multiple time points; or (3) when multiple treatment groups are compared to the same control group. [@Becker2000multivariate]
Dependence can also arise when effect sizes are extracted from multiple samples involving the same operational features, such as multiple studies conducted by the same research group. [@Hedges2010robust]

Effect size dependencies are very common in social science synthesis, as well as in other research areas.
For example, for the more than 1,000 educational intervention studies reviewed by the What Works Clearinghouse since 2017, most (73%) included more than one intervention effect estimate, with a median of four effect sizes per study (WWC, 2020).
Surveys of systematic reviews on topics in psychology and education[@tipton2019current], environmental sciences [@nakagawa2023quantitative], and neurobiology [@yang2023advanced] have also documented a high prevalence of dependent effect sizes.
Thus, multiple effects are the norm, rather than the exception, in many fields that use quantitative synthesis.

Meta-analysts now have access to an array of methods for summarizing and modeling dependent effect sizes, including multi-level meta-analyses [@konstantopoulos2011fixed; @vandennoortgate2013threelevel; @vandennoortgate2015metaanalysis], robust variance estimation [RVE, @Hedges2010robust], and combinations thereof.[@pustejovsky2022preventionscience]
Among these, RVE has proven to be an attractive strategy because it provides a means to assess uncertainty in model parameter estimates that does not rely on strong assumptions about the exact dependence structure of the effect size estimates. 
Instead, RVE involves specifying a tentative working model for the dependence, but calculating standard errors, hypothesis tests, and confidence intervals using sandwich estimators that do not require the working model to be correct. 
Although the original form of RVE required a relatively large number of independent studies, subsequent work has provided refinements to standard errors and hypothesis tests based on RVE to provide accurate inferences even when the number of studies is small.[@tipton2015small; @tiptonpusto2015small]
A closely related strategy is to use bootstrap re-sampling to approximate the distribution of test statistics.[@joshi2022cluster]
However, extant developments in RVE and bootstrapping methods are limited to summary meta-analysis and meta-regression models.
Applications to selection models remain to be explored.

## Investigating selective reporting with dependent effect sizes

Methodologists have only recently begun to examine selective reporting detection or bias correction methods in meta-analysis involving dependent effect sizes.
Mathur and VanderWeele proposed a sensitivity analysis based on the simplest possible form of the step-function selection model.[@mathur2020sensitivity]
This sensitivity analysis provides an estimate of the average effect size after correcting for selective reporting based on a single, threshold statistical significance level, where the maximum strength of selection is pre-specified by the analyst. 
It handles effect size dependence using RVE.[@Hedges2010robust]
However, this approach is premised on an assumed degree of selective reporting; thus, it does not estimate the strength of selection, nor does it have extensions to more complex forms of selection models (such as step functions with multiple thresholds).

Another alternative is to use a regression test for small-study effects, or association between effect sizes and standard errors, combined with RVE or multilevel meta-analysis to handle dependent effect sizes. [@fernandezcastilla2019detecting; @rodgers2021evaluating]
This method has limited power to detect selective reporting under common meta-analytic scenarios.[@rodgers2021evaluating]
It also has the same limitations as univariate Egger's regression, in that it tests for small-study effects (or asymmetry in the funnel plot distribution), which could have causes other than selective reporting.
Further, Egger's regression is not based on a generative model and is therefore not directly informative about the degree or pattern of selective reporting.

Chen and Pustejovsky reviewed a range of existing techniques for estimating average effect sizes in the presence of selective reporting and proposed adaptations of some existing methods to accommodate dependent effect sizes.[@chen2024adapting]
The proposed adaptations pertain to methods that can be formulated as meta-regressions, such as PET/PEESE and the endogenous kink method, with dependence addressed using a particular working model combined with RVE.
In an extensive simulation study, they examined the performance of proposed adaptations alongside existing methods that ignore effect size dependence. 
Although no single bias-correction method performed best across all conditions examined, simple forms of the step-function selection model emerged as strong candidates.
Across a wide range of conditions, one-step and two-step selection models yielded average effect size estimates with low bias that were usually more accurate than alternative bias-adjusted estimators, even though the selection models ignore the dependence structure of the data.
However, because selection models rely on the assumption that all effect sizes are independent, confidence intervals generated from the selection models did not have accurate coverage. 
These findings indicate a clear need to further develop selection models that can account for dependent effect sizes.[@chen2024adapting]

To address this need, we investigate how to estimate selection models and provide valid assessments of uncertainty in parameter estimates for meta-analyses that involve dependent effect sizes. 
In the next section, we describe the step-function selection model and detail two different strategies for estimating model parameters: one using conventional maximum likelihood estimation methods and a novel strategy based on a re-weighted random effects model with inverse probability of selection weights. 
We also describe extensions of RVE and bootstrap re-sampling techniques to assess uncertainty in selection model parameter estimates.
In the following section, we provide an empirical example that illustrates the methods by re-analyzing data from a previously reported meta-analysis. 
In subsequent sections, we describe the methods and results from a simulation study that evaluates the performance of point estimators and confidence intervals across a wide range of meta-analytic conditions.
In the final section, we discuss findings, limitations, and initial implications for practice.
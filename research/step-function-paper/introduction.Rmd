---
title: "Introduction"
output: pdf_document
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: sentence
---

Meta-analysis methods are used to synthesize quantitative findings across multiple sources of evidence, such as multiple studies that evaluate the effects of the same intervention.
Because they rely on findings from primary studies as input data, the validity of conclusions from a meta-analytic synthesis depends critically on the reporting practices of researchers, journal editors, and peer reviewers.
If the findings accessible to meta-analysts are not a complete or representative record of the research that has been conducted on a topic, then meta-analytic summaries could be systematically biased [@Rothstein2005publication].
Of particular concern is the possibility that results from primary studies are selectively reported in ways that can distort the evidence available for synthesis, such as reporting findings that are statistically significant but omitting findings that are null or not consistent with researchers' hypotheses [@carter2019correcting].

Evidence from medical, educational, and social sciences provides indications of the prevalence of selective reporting. For example, studies have found that statistically significant outcomes were 2.4 to 4.7 times more likely to be published than non-significant outcomes in the medical sciences [@chan2004empirical] and 2.4 times more likely in education [@pigott2013outcome]. Research across various fields, from clinical trials of antipsychotics [e.g., @lancee2017outcome] to psychology studies [e.g., @john2012measuring; @franco2016underreporting] and management research [e.g., @oBoyle2017chrysalis], indicates that selective outcome reporting is widespread and often driven by statistical significance.

Because selective reporting is of such central concern for meta-analysis, a wide range of statistical tools have been developed for assessing the presence of selective reporting and reducing the biases it creates [@Rothstein2005publication; @marksanglin2020historical].
One common graphical diagnostic is the funnel plot, a simple scatterplot of effect size estimates versus a measure of their precision [@light1984Summing; @Sterne2005funnel; @kossmeier2020PowerEnhanced]. Widely used statistical diagnostics include the rank correlation test by @begg1994operating; Egger's regression test [@egger1997bias; @harbord2006modified; @macaskill2001comparison; @moreno2012generalized; @peters2006comparison; @pustejovsky2019testing; @stanley2008meta; @thompson1999explaining]; the trim-and-fill adjustment [@duval2000nonparametric; @duval2000trim]; and various regression adjustment methods including the precision effect test (PET), precision effect estimate with standard error (PEESE), and PET-PEESE technique by @stanley2014meta, and the endogenous kink meta-regression [@bom2019kinked].

Another class of methods for assessing and correcting selective reporting are $p$-value selection models.^[A further class of selection models exists that depends both on the effect size estimate and its standard error [e.g., @copas1999what; @copas1997inference; @Copas2001sensitivity]. We focus here on the selection models that depend solely on the $p$-value because they have shown more promise [@carter2019correcting; @Terrin2003heterogeneity] and because the second set of selection models have identification issues that make them more useful as sensitivity analyses than as estimation methods [@hedges2005selection; @sutton2009publication].]
Such models build on summary meta-analysis or meta-regression models by making specific, explicit assumptions about the selection function, or how the probability that an effect size estimate is reported relates to sign and statistical significance level of the effect. Early proposals of this form include @hedges1984estimation, @iyengar1988selection, and @dear1992approach. @hedges1992modeling and @vevea1995general proposed _step-function_ models where the selection function is piece-wise constant, with steps at psychologically salient significance levels such as $\alpha = .05$. 
Other forms involve selection functions based on beta densities [@citkowicz2017parsimonious], power curves, and a variety of other parametric forms [@preston2004adjusting].

Selection models have several advantages over other available methods for diagnosing and adjusting for selective reporting bias.
First, they are generative models with parameters that directly describe the selective reporting process; they are therefore more interpretable than tests or adjustments for small-study effects, which are agnostic with respect to the specific mechanism of selective reporting. 
Second, selection models can allow for effect heterogeneity with a random effect term. 
Findings from simulations indicate that selection models outperform simpler alternative methods when effect sizes are heterogeneous [@carter2019correcting; @Terrin2003heterogeneity].
Third, selection models can incorporate both discrete and continuous moderators, enabling one to distinguish between selective reporting bias and systematic differences in effect size that can be predicted by primary study characteristics.
In addition to these features, @vevea1995general step-function's model is particularly useful because it allows one to specify cut points that capture simple but plausible forms of selective reporting [e.g., p < 0.01 and p < 0.05, see, e.g., @greenwald1975prejudice; @Nelson1986significance; @Rosenthal1963significance; @Rosenthal1964significance].

## Dependent effect sizes

The vast majority of the work on selective reporting---including the development of selection models---has focused on methods appropriate for relatively simple summary meta-analyses in which each included study contributes a single independent effect size estimate.
This presents a problem for syntheses in education, psychology, and many other areas, where meta-analyses routinely include studies with multiple, dependent effect sizes. 
Effect size dependencies occur when multiple effect sizes are extracted from the same sample, resulting in statistically dependent estimates.
Dependent effect size estimates commonly occur (1) when multiple outcome measures are collected on the same sample; (2) when the same sample is measured over multiple time points; or (3) when multiple treatment groups are compared to the same control group.
Dependence can also arise when effect sizes are extracted from multiple samples involving the same operational features, such as multiple studies conducted by the same research group.

Effect size dependencies are very common in social science synthesis, as well as in other research areas.
For example, for the more than 1,000 educational intervention studies reviewed by the What Works Clearinghouse since 2017, most (73%) included more than one intervention effect estimate, with a median of four effect sizes per study (WWC, 2020).
In a survey of systematic reviews published in 2016 across several prominent journals, @tipton2019current found that primary studies contributed an average of 3.1 effect sizes to systematic reviews published in _Psychological Bulletin_; 11.0 effect sizes to reviews published in _Journal of Applied Psychology_; and 5.0 effect sizes to reviews published in _Review of Educational Research_. 
Surveys of recent meta-analyses on topics in environmental sciences [@nakagawa2023quantitative] and neurobiological research involving animal models [@yang2023advanced] have also documented a high prevalence of dependent effect sizes.
Thus, multiple effects are the norm, rather than the exception, in many fields that use quantitative synthesis.

Meta-analysts now have access to an array of methods for summarizing and modeling dependent effect sizes, including multi-level meta-analyses [@konstantopoulos2011fixed; @vandennoortgate2013threelevel; @vandennoortgate2015metaanalysis], robust variance estimation [RVE, @Hedges2010robust; @tipton2015small; @tiptonpusto2015small], and combinations thereof [@pustejovsky2022preventionscience].
Among these, RVE has proven to be an attractive strategy because it provides a means to assess uncertainty in model parameter estimates that does not rely on strong assumptions about the exact dependence structure of the effect size estimates. 
Instead, RVE involves specifying a tentative working model for the dependence, but calculating standard errors, hypothesis tests, and confidence intervals using sandwich estimators that do not require the working model to be correct. 

The original form of RVE was based on asymptotic approximations that required a relatively large number of independent studies.
@tipton2015small and @tiptonpusto2015small developed small-sample adjustments for standard errors and hypothesis tests based on RVE so that the desirable asymptotic properties are maintained even when the number of studies is small.
A closely related strategy is to use bootstrap re-sampling to approximate the distribution of test statistics.
For instance, @joshi2022cluster examined a cluster-wild bootstrap method for hypothesis testing in meta-regression with dependent effects, finding that it led to refined type I error rates and improved power compared to analytic approximations.
However, extant developments in RVE and bootstrapping methods are limited to summary meta-analysis and meta-regression models.
Applications to selection models remain to be explored.

## Investigating selective reporting with dependent effect sizes

Methodologists have only recently begun to examine selective reporting detection or bias correction methods in meta-analysis involving dependent effect sizes.
@mathur2020sensitivity proposed a sensitivity analysis based on the simplest possible form of the @vevea1995general step-function selection model.
This sensitivity analysis provides an estimate of the average effect size after correcting for selective reporting based on a single, threshold statistical significance level, where the maximum strength of selection is pre-specified by the analyst. It handles effect size dependence using RVE methods [@Hedges2010robust].
However, this approach is premised on an assumed degree of selective reporting; thus, it does not estimate the strength of selection, nor does it have extensions to more complex forms of selection models (such as step functions with multiple thresholds).

Another alternative is to use a regression test for small-study effects, or association between effect sizes and standard errors  [as in @egger1997bias], combined with RVE or multilevel meta-analysis to handle dependent effect sizes [@fernandezcastilla2019detecting; @rodgers2021evaluating].
@rodgers2021evaluating found that this method has limited power to detect selective reporting under common meta-analytic scenarios.
It also has the same limitations as univariate Egger's regression, in that it tests for small-study effects (or asymmetry in the funnel plot distribution), which could have causes other than selective reporting.
Further, Egger's regression is not based on a generative model and is therefore not directly informative about the degree or pattern of selective reporting.

@chen2024adapting reviewed a range of existing techniques for estimating average effect sizes in the presence of selective reporting and proposed adaptations of some existing methods to accommodate dependent effect sizes.
The proposed adaptations applied to methods that could be formulated as meta-regressions, such as PET/PEESE and the endogenous kink method, with dependence addressed using a particular working model combined with RVE.
In an extensive simulation study, they examined the performance of proposed adaptations alongside existing methods that ignore effect size dependence. 
Although no single bias-correction method performed best across all conditions examined, simple forms of the step-function selection model emerged as strong candidates.
Across a wide range of conditions, one-step and two-step selection models yielded average effect size estimates with low bias that were usually more accurate than alternative bias-adjusted estimators, even though the selection models ignore the dependence structure of the data.
However, because selection models rely on the assumption that all effect sizes are independent, confidence intervals generated from the selection models did not have accurate coverage. 
In light of these findings, @chen2024adapting noted a need to further develop selection models that can account for dependent effect sizes.

To address this need, we investigate how to estimate selection models and provide valid assessments of uncertainty in parameter estimates for meta-analyses that involve dependent effect sizes. 
We make three main contributions.
First, we describe extensions of RVE and bootstrap re-sampling techniques to assess uncertainty in selection model parameter estimates.
Second, we describe and evaluate two different strategies for estimating model parameters: one using conventional maximum likelihood estimation methods and a novel strategy based on a reweighted random effects model with inverse probability of selection weights.
Third, we use simulation to evaluate the performance of RVE and bootstrap re-sampling for constructing confidence intervals for average effect sizes.

The remainder of the paper is organized as follows. 
In the next section, we describe the step-function selection model and detail our estimation and inference strategies. 
In the following section, we provide an empirical example that illustrates the methods by re-analyzing data from a previously reported meta-analysis. 
In subsequent sections, we describe the methods and results from a simulation study that rigorously evaluates the estimators and confidence intervals across a wide range of meta-analytic conditions.
In the final section, we discuss findings, limitations, and initial implications for practice.
---
title: "Empirical Application"
output: pdf_document
bibliography: references.bib
---

```{r, echo = F, warning = F, message = F}
library(tidyverse)
library(knitr)
library(kableExtra)

pub_studies_res <- readRDS(file = "ego-depletion/pub_studies_res.rds")

step_mod_res <- 
  pub_studies_res %>%
  filter(str_detect(estimator, "-step"), param == "beta")

min_est <- min(step_mod_res$Est)
max_est <- max(step_mod_res$Est)
```

To demonstrate the proposed modeling strategy and examine potential differences between CML and ARGL estimation methods, we re-analyzed the data from a meta-analysis reported by @carter2015series. 
The original meta-analysis examined a large corpus of primary studies on the ego depletion effect, which refers to the theory that an individual's ability to exercise self-control diminishes with repeated exertion [@hagger2010ego]. 
@carter2015series argued that the apparent strength of ego-depletion effects may be overstated due to selective reporting. 
Their review included a variety of self-control manipulation tasks as well as a range of outcomes. 
Some primary studies reported effects for multiple outcome tasks, leading to dependent effects. 
Effect sizes were measured as standardized mean differences, defined so that positive effects correspond to depletion of self-control (i.e., consistent with the theory of ego depletion).

To mitigate possible effects of selective reporting, @carter2015series included many unpublished studies, so that the full meta-analysis included 116 effects from 66 studies. 
For illustrative purposes, we re-analyzed the findings from the subset of published studies only; we also excluded a single outlying effect size estimate that was greater than 2. This analytic sample includes 66 effect size estimates from 45 distinct studies.
We conducted the analyses using R Version 4.4.3 [@rcoreteam]. 

If selective reporting were not a concern, a correlated-and-hierarchical effects model [CHE, @pustejovsky2022preventionscience] would be one way to summarize the distribution of ego depletion effects.
Based on a CHE model, the overall average effect estimate was `r round(as.numeric(pub_studies_res$Est[pub_studies_res$estimator == "CHE"]), 2)`, 95% CI [`r round(as.numeric(pub_studies_res$CI_lo[pub_studies_res$estimator == "CHE"]), 2)`, `r round(as.numeric(pub_studies_res$CI_hi[pub_studies_res$estimator == "CHE"]), 2)`]. 
Alternately, @chen2024adapting proposed estimating average effect sizes using a CHE model with inverse sampling covariance weighting (CHE-ISCW), which places relatively more weight on larger studies (those with smaller sampling variances) and thus is less biased by selective reporting.
Applying CHE-ISCW reduces the overall effect estimate to `r round(as.numeric(pub_studies_res$Est[pub_studies_res$estimator == "FEC"]), 2)`, 95% CI [`r round(as.numeric(pub_studies_res$CI_lo[pub_studies_res$estimator == "FEC"]), 2)`, `r round(as.numeric(pub_studies_res$CI_hi[pub_studies_res$estimator == "FEC"]), 2)`]. 
As a further point of comparison, we estimated the overall average effect using the PET/PEESE regression adjustment [@stanley2014meta], clustering the standard errors by study. This yielded an overall average effect of `r round(as.numeric(pub_studies_res$Est[pub_studies_res$estimator == "PET/PEESE"]), 2)`, 95% CI [`r round(as.numeric(pub_studies_res$CI_lo[pub_studies_res$estimator == "PET/PEESE"]), 2)`, `r round(as.numeric(pub_studies_res$CI_hi[pub_studies_res$estimator == "PET/PEESE"]), 2)`]. The CHE and CHE-ISCW estimates are both positive, significant, and similar in magnitude. The PET-PEESE estimate is negative and much smaller than the CHE and CHE-ISCW estimates, indicating a pattern of small study effects. 

We used the `selection_model()` function from the `metaselection` package to fit single-step and two-step selection models [@metaselection]. 
The single-step model used a threshold at $\alpha_1 = 0.025$; the two-step model used thresholds at $\alpha_1 = 0.025$ and $\alpha_2 = 0.5$. 
For comparison purposes, we estimated model parameters using both CML and ARGL and computed cluster-robust and percentile bootstrap confidence intervals.
For bootstrapping, we used two-stage cluster bootstrap re-sampling with 1999 replicates.

```{r empirical, echo=F, message=FALSE, warning=FALSE, tab.cap="Carter et al. (2015) Re-Analysis: Ego-Depletion"}
format_it <- function(x, trunc = Inf) {
  
  ifelse(
    x < trunc,
    format(round(x, 2), nsmall = 2),
    paste0(">",trunc)
  )
  
}

table_step <- 
  pub_studies_res %>%
  filter(str_detect(estimator, "-step")) %>%
  separate(estimator, sep = " ", into = c("step","estimator")) %>%
  arrange(step) %>%
  mutate(
    across(
      c(Est, SE, CI_lo, CI_hi, percentile_lower, percentile_upper), 
      format_it, trunc = 100
    )
  ) %>%
  select(-p_value, -df) %>%
  mutate(
    est_se = paste0(Est, " (", SE, ")"),
    large_sample_ci = paste0("[", CI_lo, ",", CI_hi, "]"),
    bootstrap_ci = paste0("[", percentile_lower, ", ", percentile_upper, "]")
  ) %>%
  select(-c(Est:percentile_upper)) %>%
  pivot_wider(
    names_from = estimator,
    values_from = c(est_se, large_sample_ci, bootstrap_ci)
  ) %>%
  select(
    step, param,
    est_se_CML, large_sample_ci_CML, bootstrap_ci_CML, 
    est_se_ARGL, large_sample_ci_ARGL, bootstrap_ci_ARGL
  ) %>%
  mutate(
    param_order = case_match(
      param, 
      "beta" ~ 1,
      "tau2" ~ 2,
      "lambda1" ~ 3, 
      "lambda2" ~ 4
    ),
    param = case_match(
      param,
      "beta" ~ "$\\beta$",
      "tau2" ~ "$\\tau^2$",
      "lambda1" ~ "$\\lambda_1$",
      "lambda2" ~ "$\\lambda_2$"
    )
  ) %>%
  arrange(step, param_order) %>%
  select(-param_order)


table_step %>%
  select(-step) %>%
kbl(
  caption = "Single-step and two-step selection model parameter estimates fit to ego depletion effects data from Carter et al. (2015)",
  col.names = c("Parameter", "Estimate (SE)", "Cluster-Robust CI", "Percentile Bootstrap CI", "Estimate (SE)", "Cluster-Robust CI", "Percentile Bootstrap CI"),
  booktabs = TRUE,
  escape = FALSE
) %>%
  kable_styling(latex_options = c("scale_down")) %>%
  add_header_above(c(" ", "CML estimator" = 3, "ARGL estimator" = 3)) %>%
  pack_rows(index = table(table_step$step)) %>%
  column_spec(2:7, width = "5.5em") %>%
  footnote(
    general = c("ARGL = augmented, reweighted gaussian likelihood; CML = composite maximum likelihood; CI = confidence interval; SE = standard error."),
    threeparttable = TRUE
  )

```

Table \@ref(tab:empirical) presents the parameter estimates from the one-step and two-step selection models.
The estimated selection parameters are similar across the one- and two-step models and across both estimators, all indicating that non-significant or negative effect size estimates were less likely to be reported than statistically significant, affirmative ones. 
The one-step and two-step selection model estimates of average effect size are positive but substantially smaller than the CHE-ISCW estimates, ranging from `r round(min_est, 2)` to `r round(max_est, 2)` depending on the model specification and estimation method. 
In contrast to the PET/PEESE estimate, the selection model estimates using CML are positive and statistically distinct from zero. 
Thus, an analyst would reach different conclusions about overall average effect size depending on whether they use an unadjusted model, a step-function model, or the PET/PEESE adjustment. 

The estimates in Table \@ref(tab:empirical) point towards some potential differences between estimation methods.
Generally, the CML and ARGL parameter estimates are similar in magnitude, but the confidence intervals based on the CML estimator are narrower than those for the ARGL estimator.
For the CML estimator, the bootstrap CIs are similar or slightly wider than the cluster-robust CIs.
<!-- For the ARGL estimator, the bootstrap CIs were similar to the cluster-robust CIs for the one-step model but quite discrepant for the two-step model.  -->
<!-- For the two-step model, the cluster-robust CIs were so wide as to be entirely uninformative, whereas the bootstrap CIs were not as extreme. -->
These patterns suggest that there could be differences in the performance of the estimators, as well as differences in performance between the step-function estimators and alternative adjustment methods such as PET/PEESE.
However, these results are based on a single empirical dataset where the true data-generating process is unknown. 
To draw firmer conclusion about these methods, we conducted simulations to evaluate their performance characteristics across a range of conditions. 

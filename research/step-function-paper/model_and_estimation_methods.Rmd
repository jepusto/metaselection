---
title: "Modified Step-Function Selection Model"
output: pdf_document
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: sentence
---

Selection models involve two components [@hedges2005selection]. 
The first component, which we shall call the evidence-generating process, involves assumptions about the distribution of effect size estimates prior to selective reporting.
This component is usually a random effects model or meta-regression model.
The second component, which we shall call the selection process, involves assumptions about how effect size estimates come to be observed and therefore available for inclusion in the meta-analysis. 
Combining the assumptions of both components leads to a model for the distribution of observed effect size estimates, with interpretable parameters describing both the evidence-generating process and the selection process. 

In applying selection models to datasets involving dependent effects, we propose to model the _marginal_ distribution of the effect size estimates rather than their joint distribution. This means we model each estimate as a single observation, without explicitly accounting for its dependence on other effect sizes from the same study. 
To account for dependence, we consider cluster-robust variance estimation or clustered bootstrap inference methods that allow for dependent observations even though the dependence is not explicitly modeled. 
This strategy does have the limitation that the model parameter estimates pertain only to the marginal distribution and so would not, for instance, allow for a decomposition of heterogeneity into between-study and within-study variance. 
Likewise, models for the marginal distribution do not allow one to distinguish between selective publication of full studies versus selective reporting of individual outcomes.
Nonetheless, we believe that the strategy is worth pursuing both because it is feasible and because it captures a plausible form of selection, in which reporting is influenced by the significance level of individual effect size estimates.

We will use the following notation to describe the model and estimation methods. Consider a meta-analytic sample consisting of $J$ studies, in which study $j$ reports $k_j$ effect size estimates. Let $y_{ij}$ denote observed effect size estimate $i$ from study $j$, defined so that positive values are consistent with theoretical expectations. Each estimate is accompanied by a standard error $\sigma_{ij}$ and corresponding one-sided $p$-value $p_{ij}$, where the one-sided $p$-value is defined with respect to the null hypothesis that the effect is less than or equal to zero and the alternative hypothesis that the effect is positive. Let $\mat{x}_{ij}$ be a $1 \times x$ row-vector of predictors that encode characteristics of the effect sizes, samples, or study procedures. Let $\Phi()$ denote the standard normal cumulative distribution function and $\phi()$ denote the standard normal density function.

## Evidence-generating process

We consider an evidence-generating process based on a standard meta-regression model. 
Let $Y^*$ denote an effect size estimate that has been generated from primary study data but might or might not be reported; let $\sigma^*$, $p^*$, and $\mat{x}^*$ denote the corresponding standard error, one-sided $p$-value, and predictor vector.
The evidence-generating process can then be expressed as
\begin{equation}
\label{eq:meta-mean-regression}
\left(Y^* | \sigma^*, \mat{x}^*\right) \sim N\left(\mat{x}^* \bs\beta, \ \tau^2 + \sigma^{*2}\right),
\end{equation}
where $\bs\beta$ is an $x \times 1$ vector of regression coefficients that relate the predictors to average effect size and $\tau^2$ is the variance of the distribution of effect size parameters. 

This random-effects meta-regression treats each observed effect size as if it were independent, even though the data may include multiple, statistically dependent effect size estimates generated from the same sample. As a result, the regression coefficients $\boldsymbol\beta$ describe the overall expected effect size (given the predictors) and the variance parameter $\tau^2$ describes the marginal or _total_ heterogeneity of the effect size distribution, rather than decomposing the heterogeneity into within-study and between-study components. 

## Selection process

At a general level, a $p$-value selection process is defined by a selection function, which specifies the probability that an effect size estimate is reported given its $p$-value. 
Letting $O$ be an indicator for whether the effect size estimate $Y^*$ is observed, the selection process defines $\text{Pr}(O = 1 \ | \ p^*) = \text{Pr}(O = 1 \ | \ Y^*, \sigma^*)$. 
In particular, such models assume that 
\begin{equation}
\label{eq:selection-process}
\Pr\left(O = 1 | p^* \right) \propto w\left(p^*; \bs\lambda \right)
\end{equation}
for some known function $w\left(.; \bs\lambda\right)$ that maps $p$-values in the interval $[0,1]$ to strictly positive weights and that involves an unknown $h \times 1$ parameter vector $\bs\lambda$. 

Many different specific selection functions have been proposed in the literature. 
Building on work by @hedges1992modeling on random effects models without predictors, @vevea1995general described a random effects meta-regression model where selection probabilities vary depending on a set of pre-specified thresholds for the one-sided $p$-value. 
The thresholds are chosen based on conventional, psychologically salient cut-offs for judging statistical significance.
Let $\alpha_1,...,\alpha_H$ be a set of thresholds for the one-sided p-values, and set $\alpha_0 = 0$, $\alpha_{H+1} = 1$, and $\lambda_0 = 1$. 
A step function selection model is then given by 
\begin{equation}
\label{eq:step-function-p}
w(p^*; \bs\lambda) =  \lambda_h \quad \text{if} \quad \alpha_{h} < p^* \leq \alpha_{h+1}, 
\end{equation}
for $h = 0,...,H$ and $\bs\lambda = \left(\lambda_1,...,\lambda_H\right)$. 
Equation (\ref{eq:step-function-p}) can be written equivalently as 
\begin{equation}
\label{eq:step-function-y}
w(Y^*, \sigma^*; \bs\lambda) =  \lambda_h \quad \text{if} \quad \sigma^* \Phi^{-1}\left(1 - \alpha_{h+1}\right) \leq Y^*  < \sigma^* \Phi^{-1}\left(1 - \alpha_h\right). 
\end{equation}
Setting $\lambda_0 = 1$ is necessary for identification because the absolute probabilities of selection cannot be estimated. 
The remaining parameters are therefore interpreted as _relative_ probabilities of selection, compared to the probability of selection for an effect size with $p^* \leq \alpha_1$. 

In practice, meta-analysts will often use only a small number of steps in the selection model. 
One common choice is the three-parameter selection model, which has a single step at $\alpha_1 = .025$, as depicted in Figure \@ref(fig:step-functions)a. 
With this choice of threshold, positive effects that are statistically significant at the two-sided level of $p < .05$ have a different probability of selection than effects that are not statistically significant or not in the anticipated direction.
Another possibility is to use two steps at $\alpha_1 = .025$ and $\alpha_2 = .500$, which allows for different probabilities of selection for effects that are positive but not statistically significant and effects that are negative (i.e., in the opposite the intended direction), as depicted in Figure \@ref(fig:step-functions)b.

```{r step-functions}
#| echo: false
#| fig.retina: 2
#| fig.width: 5
#| fig.height: 3.5
#| out.width: 49%
#| fig.cap: "Examples of step functions"
#| fig.subcap:
#|    - "One-step selection with $\\lambda_1 = 0.4$" 
#|    - "Two-step selection with $\\lambda_1 = 0.4, \\lambda_2 = 0.2$"
#| fig.pos: tb 

library(metaselection)
library(ggplot2)
lambda1 <- 0.4
lambda2 <- 0.2
pvals <- seq(0,1,.005)
PSM3 <- step_fun(cut_vals = .025, weights = lambda1)
PSM4 <- step_fun(cut_vals = c(.025, .500), weights = c(lambda1, lambda2))

dat <- data.frame(p = pvals, PSM3 = PSM3(pvals), PSM4 = PSM4(pvals))

PSM3 <- 
  ggplot(dat, aes(x = pvals)) + 
  scale_y_continuous(limits = c(0,1.1), expand = expansion(0,0)) + 
  scale_x_continuous(breaks = seq(0,1,0.2), expand = expansion(0,0)) + 
  geom_vline(xintercept = 0.025, linetype = "dashed") + 
  geom_hline(yintercept = 0) + 
  annotate(geom = "text", x = .06, y = 1.03, label = ".025") + 
  geom_area(aes(y = PSM3), fill = "darkred", alpha = 0.6) +   
  theme_minimal() + 
  labs(x = "p-value (one-sided)", y = "Selection probability")

PSM4 <- 
  ggplot(dat, aes(x = pvals)) + 
  scale_y_continuous(limits = c(0,1.1), expand = expansion(0,0)) + 
  scale_x_continuous(breaks = seq(0,1,0.2), expand = expansion(0,0)) + 
  geom_vline(xintercept = c(0.025, .500), linetype = "dashed") + 
  geom_hline(yintercept = 0) + 
  annotate(geom = "text", x = .06, y = 1.03, label = ".025") + 
  annotate(geom = "text", x = .535, y = 1.03, label = ".500") + 
  geom_area(aes(y = PSM4), fill = "purple", alpha = 0.6) +   
  theme_minimal() + 
  labs(x = "p-value (one-sided)", y = "Selection probability")

PSM3
PSM4
```

## Distribution of observed effect size estimates

Combining the assumptions of the evidence-generating process and the selection process leads to a model for an observed effect size estimate. The distribution of an observed effect size estimate is equivalent to the distribution of $Y^*$ given that $O = 1$. By Bayes rule,
$$
\Pr(Y^* = y | O = 1, \sigma^* = \sigma) = \frac{\Pr(O = 1 | Y^* = y, \sigma^* = \sigma) \times \Pr(Y^* = y | \sigma^* = \sigma)}{\Pr(O = 1 | \sigma^* = \sigma)},
$$
where the selection process defines the first term in the numerator and the evidence-generating process defines the second term in the numerator. The marginal density of an observed effect size estimate $Y$ with standard error $\sigma$ therefore has the form
\begin{equation}
\label{eq:generic-selection}
f(Y = y | \sigma, \mat{x}) = \frac{1}{A(\mat{x}, \sigma; \bs\beta, \tau^2, \bs\lambda)} \times w\left(y, \sigma; \bs\lambda \right) \times \frac{1}{\sqrt{\tau^2 + \sigma^2}} \phi\left(\frac{y - \mat{x} \bs\beta}{\sqrt{\tau^2 + \sigma^2}}\right),
\end{equation}
where 
\begin{equation}
\label{eq:generic-selection-A}
A(\mat{x}, \sigma; \bs\beta, \tau^2, \bs\lambda) =  \int_\mathbb{R} w\left(y, \sigma; \bs\lambda \right) \times  \frac{1}{\sqrt{\tau^2 + \sigma^2}}\phi\left(\frac{y - \mat{x}\bs\beta}{\sqrt{\tau^2 + \sigma^2}}\right) dy.
\end{equation}
If $w(y, \sigma; \bs\lambda) = 1$, then there is no selective reporting, $A(\mat{x}, \sigma; \bs\beta, \tau^2, \bs\lambda) = 1$, and the density reduces to the unweighted density of the evidence-generating process (i.e., the density of a random-effects meta-regression).

For the step-function selection process, the $A(\mat{x}, \sigma; \bs\beta, \tau^2, \bs\lambda)$ term in the step-function composite likelihood can be computed using the closed-form expression
\begin{equation}
\label{eq:step-function-A}
A_{ij} = A(\mat{x}_{ij}, \sigma_{ij}; \bs\beta, \tau^2, \bs\lambda) = \sum_{h=0}^H \lambda_h B_{hij}
\end{equation}
where 
\begin{equation}
\label{eq:step-function-Bhij}
B_{hij} = \Phi\left(c_{hij}\right) - \Phi\left(c_{h+1,ij}\right),
\end{equation}
and $c_{hij} = \left(\sigma_{ij} \Phi^{-1}\left(1 - \alpha_h\right) - \mat{x}_{ij}\bs\beta\right) / \sqrt{\tau^2 + \sigma_{ij}^2}$ for $h = 0,...,H$ [@vevea1995general]. 
The quantity $B_{hij}$ corresponds to the probability that, prior to selection, a generated effect size estimate with predictor $\mat{x}_{ij}$ and standard error $\sigma_{ij}$ will have a p-value falling in the interval $\alpha_h < p^* \leq \alpha_{h+1}$.


## Estimation Methods {#estimation-methods}

Past developments of selection models have focused either on maximum likelihood estimation under the assumption that all effect sizes are mutually independent [@hedges1992modeling; @vevea1995general; @citkowicz2017parsimonious] or on sensitivity analysis methods that treat the selection model as known [@vevea2005publication; @mathur2020sensitivity]. We consider several estimation and inference strategies that build upon and generalize past approaches, including maximum composite marginal likelihood and an alternative based on re-weighting the Gaussian likelihood of the evidence-generating process.
With both approaches, we allow for incorporation of prior weights, which permits efficient calculation for a variety of bootstrapping techniques.[^efficiency-weights]
Thus, let $a_{11},...,a_{J k_j}$ be an arbitrary set of prior weights assigned to each effect size estimate; in a typical, unweighted analysis, all weights will will be equal to $a_{ij} = 1$.

[^efficiency-weights]: Another reason to consider weights is that analytic weights could be used to improve the efficiency of the parameter estimators, similar to the working model weights proposed by @Hedges2010robust for random effects meta-analysis and meta-regression. For instance, consider the basic meta-analysis context with no predictors. If effect size estimates from the same study are correlated, then a study with $k_j$ observed effect size estimates could contribute somewhat less than $k_j$ independent pieces of information. Down-weighting the effect sizes from study $j$ based on the number of reported effect sizes $k_j$ might therefore improve the efficiency of the estimator for the average effect size $\beta$ and variance $\tau^2$. 

### Maximum composite marginal likelihood

Composite marginal likelihood techniques involve working with the marginal distribution of each observation (here, each observed effect size estimate) as if all observations were mutually independent [@cox2004note; @lindsay1988composite; @varin2008composite]. 
Thus, we assume that the observed effect size estimates were generated from Equation \@ref(eq:generic-selection). 
For purposes of estimation, we write the likelihoods using natural log transformations of the variance parameter and selection parameters, with $\gamma = \log \tau^2$, $\zeta_h = \log \lambda_h$, and $\bs\zeta = \left[\zeta_1,...,\zeta_H\right]'$. 
The log of the marginal likelihood contribution for effect size estimate $i$ from study $j$ is given by
\begin{align}
l^M_{ij}\left(\bs\beta, \gamma, \bs\zeta \right) &= \log f\left(Y = y_{ij} | \sigma_{ij}, \mat{x}_{ij}\right) \nonumber \\
&\propto \log w\left(y_{ij}, \sigma_{ij}; \bs\zeta \right) - \frac{1}{2} \frac{\left(y_{ij} - \mat{x}_{ij} \bs\beta\right)^2}{\exp(\gamma) + \sigma_{ij}^2} \nonumber\\
& \qquad \qquad  - \frac{1}{2}\log\left(\exp(\gamma) + \sigma_{ij}^2\right) - \log A\left(\mat{x}_{ij}, \sigma_{ij}; \bs\beta, \gamma, \bs\zeta \right). \label{eq:log-like-ij}
\end{align}
The weighted composite marginal log-likelihood across all $J$ studies is then
\begin{equation}
\label{eq:marginal-likelihood}
l^M\left(\bs\beta, \gamma, \bs\zeta\right) = \sum_{j=1}^J \sum_{i=1}^{k_j} a_{ij} l^M_{ij}\left(\bs\beta, \gamma, \bs\zeta\right).
\end{equation}
The composite marginal likelihood (CML) estimators, denoted as $\bs{\hat\beta}$, $\hat\gamma$, and $\bs{\hat\zeta}$, are obtained as the set of parameter values that maximize the composite likelihood for the observed data, as given in Equation (\ref{eq:marginal-likelihood}).

Under the assumption that the true parameter values are not at the extremes of their ranges, the CML estimator can also be defined as the solution of the weighted score equations, 
\begin{equation}
\sum_{j=1}^J \mat{S}_{j}\left(\bs{\hat\beta}, \hat\gamma, \bs{\hat\zeta}\right) = \mat{0}
\end{equation}
where $\mat{S}_j = \left(\mat{S}_{\bs\beta j}' \  S_{\gamma j} \ \mat{S}_{\bs\zeta j}'\right)'$ denotes the score vector from study $j$, consisting of the derivatives of the likelihood contributions for each study with respect to the component parameters:
\begin{align}
\mat{S}_{\bs\beta j}\left(\bs{\beta}, \gamma, \bs{\zeta}\right) &= \sum_{i=1}^{k_j} a_{ij} \frac{\partial l^M_{ij}\left(\bs\beta, \gamma, \bs\zeta\right)}{\partial \bs\beta} \label{eq:score-M-beta} \\
S_{\gamma j}\left(\bs{\beta}, \gamma, \bs{\zeta}\right) &= \sum_{i=1}^{k_j} a_{ij} \frac{\partial l^M_{ij}\left(\bs\beta, \gamma, \bs\zeta\right)}{\partial \gamma} \label{eq:score-M-gamma} \\
\mat{S}_{\bs\zeta j}\left(\bs{\beta}, \gamma, \bs{\zeta}\right) &= \sum_{i=1}^{k_j} a_{ij} \frac{\partial l^M_{ij}\left(\bs\beta, \gamma, \bs\zeta\right)}{\partial \bs\zeta}. \label{eq:score-M-zeta} 
\end{align}
`r appendix_prefix` \@ref(CML-derivatives) provides exact expressions for the score vectors of the step-function selection model.

Robust variance estimators, or sandwich estimators, are a commonly used technique for quantifying the uncertainty in CML estimators. Let $\mat{H}$ denote the Hessian matrix of the composite log-likelihood,
\begin{equation}
\mat{H}(\bs\beta, \gamma, \bs\zeta) = \sum_{j=1}^J \frac{\partial \mat{S}_j(\bs\beta, \gamma, \bs\zeta)}{\partial \left(\bs\beta' \ \gamma \ \bs\zeta'\right)},
\end{equation}
exact expressions for which are given in `r appendix_prefix` \@ref(CML-derivatives).
Let $\mat{\hat{S}}_j = \mat{S}_j(\bs{\hat\beta}, \hat\gamma, \bs{\hat\zeta})$ and $\mat{\hat{H}} = \mat{H}(\bs{\hat\beta}, \hat\gamma, \bs{\hat\zeta})$ denote the score vectors and Hessian matrix evaluated at the maximum of the composite likelihood.
We then estimate the sampling variance of the CML estimator using a cluster-robust sandwich formula:
\begin{equation}
\label{eq:sandwich-variance}
\mat{V}^{CML} = \mat{\hat{H}}^{-1}\left(\sum_{j=1}^J \mat{\hat{S}}_j {\mat{\hat{S}}_j}'\right) \mat{\hat{H}}^{-1}.
\end{equation}
We construct confidence intervals for model parameters using $\mat{V}^{CML}$ with Wald-type large sample approximations. For instance, the $(1 - 2\alpha)$-level large-sample confidence interval for a meta-regression parameter $\beta_g$ is constructed as
$$
\hat\beta_g \ \pm \ \Phi^{-1}(1 - \alpha) \times \sqrt{V^{CML}_{gg}},
$$
where $V^{CML}_{gg}$ is the $g^{th}$ diagonal entry of $\mat{V}^{CML}$.


### Augmented, re-weighted Gaussian likelihood

Composite marginal likelihood is not the only possible basis for deriving estimators of selection model parameters. 
In the framework of a sensitivity analysis for worst-case selection bias, @mathur2020sensitivity proposed using regular meta-analytic estimators for $\bs\beta$, but with weights defined by the inverse probability of selection under a step-function selection model with a single step at $\alpha_1 = .025$.
Because they were working in the context of sensitivity analysis, they assumed a maximum plausible degree of selection rather than estimating the parameters of a selection model, so that the weights were fixed and known quantities. 
In contrast, here we will consider a more general model, possibly with multiple steps, using weights derived by estimating the selection model parameters.
We describe the estimators as augmented, re-weighted Gaussian likelihood (ARGL) estimators because the Gaussian likelihood of the evidence-generating process is re-weighted based on the selection process, with selection process parameters identified by augmenting the likelihood with an additional estimating equation.

Given the parameters of the selection process, we can calculate relative probabilities of selection for each effect size estimate, $w_{ij} = w(y_{ij}, \sigma_{ij}; \bs\zeta)$. Following @mathur2020sensitivity, we use these selection probabilities to form weighted estimating equations for the evidence-generating process. Under the evidence-generation model, the marginal log-likelihood of effect size estimate $i$ from study $j$ is Gaussian, given by
$$
l^G_{ij}(\bs\beta, \gamma) \propto - \frac{1}{2}\frac{(y_{ij} - \mat{x}_{ij} \bs\beta)^2}{\exp(\gamma) + \sigma_{ij}^2} - \frac{1}{2}\log(\exp(\gamma) + \sigma_{ij}^2).
$$
Allowing for prior weights, the inverse selection-weighted Gaussian log-likelihood is therefore
$$
l^G(\bs\beta, \gamma, \bs\zeta) = \sum_{j=1}^J \sum_{i=1}^{k_j} \frac{a_{ij}} {w_{ij}} \times l^G_{ij}(\bs\beta, \gamma).
$$
If the parameters of the selection process were known, we could find estimators for $\bs\beta$ and $\gamma$ by maximizing $l^G(\bs\beta, \gamma, \bs\zeta)$ for a fixed value of $\bs\zeta$. 
Equivalently, we could find the estimators as the solutions to the weighted score equations 
\begin{align}
\sum_{j=1}^J \mat{S}^G_{\bs\beta j} (\bs\beta, \gamma, \bs\zeta) &= 0 \label{eq:hybrid-score-beta} \\
\sum_{j=1}^J \mat{S}^G_{\gamma j} (\bs\beta, \gamma, \bs\zeta) &= 0, \label{eq:hybrid-score-gamma}
\end{align}
where the score contributions for study $j$ are
\begin{align}
\mat{S}^G_{\bs\beta j} (\bs\beta, \gamma, \bs\zeta) &= \sum_{i=1}^{k_j} a_{ij} \times \mat{x}_{ij}' \frac{y_{ij} - \mat{x}_{ij} \bs\beta}{w_{ij} \left(\exp(\gamma) + \sigma_{ij}^2\right)} \\
\mat{S}^G_{\gamma j} (\bs\beta, \gamma, \bs\zeta) &= \sum_{i=1}^{k_j} a_{ij} \times \frac{\exp(\gamma)}{2 w_{ij}} \left(\frac{(y_{ij} -\mat{x}_{ij} \bs\beta)^2}{\left(\exp(\gamma) + \sigma_{ij}^2\right)^2} - \frac{1}{\exp(\gamma) + \sigma_{ij}^2}\right).
\end{align}
The question remains how to obtain an estimator for $\bs\zeta$.
To estimate $\bs\zeta$, we augment the Gaussian log-likelihood with the marginal score equation with respect to $\bs\zeta$.
Specifically, we define the ARGL estimators as the values that simultaneously solve Equations \@ref(eq:hybrid-score-beta) and \@ref(eq:hybrid-score-gamma) together with the estimating equation for $\bs\zeta$ from the composite marginal likelihood approach.
 
With $\mathbf{S}_{\bs\zeta j}\left(\bs{\beta}, \gamma, \bs{\zeta}\right)$ as given in Equation  \@ref(eq:score-M-zeta), the full set of estimating equations is
\begin{equation}
\label{eq:hybrid-score}
\mat{M}_j(\bs\beta, \gamma, \bs\zeta) = \left[\begin{array}{c} \mat{S}^G_{\bs\beta j}(\bs\beta, \gamma, \bs\zeta) \\ S^G_{\gamma j}(\bs\beta, \gamma, \bs\zeta) \\ \mat{S}_{\bs\zeta j}(\bs\beta, \gamma, \bs\zeta) \end{array}\right],
\end{equation}
based on which we define the ARGL estimator as the solution to the estimating equations 
\begin{equation}
\label{eq:hybrid-total-score}
\sum_{j=1}^J \mat{M}_j(\bs\beta, \gamma, \bs\zeta) = \mat{0}.
\end{equation}
We will denote the ARGL parameter estimators as $\bs{\tilde\beta}$, $\tilde\gamma$, and $\bs{\tilde\zeta}$.[^profiling]

[^profiling]: For computational purposes, it is useful to observe that the solution to Equation \@ref(eq:hybrid-score-beta) involves a weighted least squares estimator for $\bs\beta$. Given values of $\gamma$ and $\bs\zeta$, $\bs{\tilde\beta}$ is 
    $$
    \bs{\tilde\beta}(\gamma, \bs\zeta) = \left(\sum_{j=1}^J \sum_{i=1}^{k_j} v_{ij} \mat{x}_{ij}' \mat{x}_{ij}\right)^{-1} \sum_{j=1}^J \sum_{i=1}^{k_j} v_{ij} \mat{x}_{ij}' \mat{y}_{ij},
    $$
    where $v_{ij} = \frac{a_j}{w_{ij} \left(\exp(\gamma) + \sigma_{ij}^2\right)}$.
    The weighted least squares solution allows $\bs\beta$ to be profiled out of the estimating equations, so that Equation \@ref(eq:hybrid-total-score) need be solved only for $\gamma$ and  $\bs\zeta$.

For the step-function selection process, the score equation with respect to $\bs\zeta$ has an interesting and intuitively interpretable form. 
Observe that the probability that an observed effect size estimate with predictor $\mat{x}_{ij}$ and standard error $\sigma_{ij}$ will have a p-value falling in the interval $\alpha_h < p^* \leq \alpha_{h+1}$ is $E_{hij} = \exp(\zeta_h) \times B_{hij} / A_{ij}$, where $B_{hij}$ is given in Equation \@ref(eq:step-function-Bhij).
Accounting for the prior weights, the expected number of observed effect size estimates falling into the interval $(\alpha_h, \alpha_{h+1}]$ is therefore 
\begin{equation}
\label{eq:expected-h}
E_h = \sum_{j=1}^J \sum_{i=1}^{k_j} a_{ij} \times \frac{\exp(\zeta_h) \times B_{hij}}{A_{ij}}.
\end{equation}
Let $K_h = \sum_{j=1}^J \sum_{i=1}^{k_j} a_{ij} \times I\left(\alpha_h < p_{ij} \leq \alpha_{h+1}\right)$ denote the weighted count of observed effect size estimates with $p$-values falling into the interval $(\alpha_h, \alpha_{h+1}]$, for $h = 0,...,H$.
The score of the step-function model with respect to the $h^{th}$ component of $\bs\zeta$ can then be written simply as
\begin{equation}
S_{\zeta h} = K_h - E_h.
\end{equation}
Thus, by setting $\mat{S}_{\bs\zeta} = \mat{0}$, the estimator of $\bs\zeta$ is taken to be the values that equate the observed number of effect size estimates in each interval with the expected number of estimates under the step-function model.

We will consider conducting inferences for the ARGL estimators using cluster-robust sandwich variance estimators that have the same form as (\ref{eq:sandwich-variance}). 
`r appendix_prefix` \@ref(ARGL-derivatives) provides further details.
We construct confidence intervals for model parameters using Wald-type large sample approximations, just as with the CML estimators.

## Bootstrap inference

Sandwich estimators such as those in Equations \@ref(eq:sandwich-variance) require a large number of independent clusters (i.e., large $J$) to provide accurate assessments of uncertainty. 
For regular meta-analysis or meta-regression models, small-sample refinements are available that provide accurate inference even with a limited number of clusters [@tipton2015small; @tiptonpusto2015small]. 
However, these small-sample refinements have not been extended to step-function selection models. 
We instead consider alternative inference techniques, including several forms of bootstrapping, that might provide more accurate inference with a limited number of clusters.
Bootstrap techniques involve generating many new pseudo-samples of observations by randomly perturbing the original sample, then re-calculating an estimator using each pseudo-sample.
The distribution of the estimator across pseudo-samples is used as a proxy for the actual sampling distribution of the estimator, providing a means to calculate standard errors and confidence intervals.

Many different bootstrap sampling schemes have been described that apply to different data structures and require different assumptions [@davison1997bootstrap].
For data involving dependent observations, it is crucial that the process used to generate pseudo-samples accounts for the dependence structure. 
Techniques that do so include the non-parametric clustered bootstrap, two-stage bootstrap [@field2007bootstrapping; @leeuw2008resampling], and fractional random weight bootstrap [@xu2020applications; @rubin1981bayesian]. 
Here, we focus on the two-stage bootstrap; details about other bootstrap techniques can be found in `r appendix_prefix` \@ref(bootstrap-details).

In the two-stage bootstrap, each pseudo-sample is generated by, first, randomly drawing $J$ clusters of observations with replacement from the original sample and then, second, randomly re-sampling observations with replacement from each selected cluster. 
This process amounts to simulating set of weights. Let $a_j^{(b)}$ be a first-stage weight for cluster $j$ and $a_{ij}^{(b)}$ be the weight assigned to observation $i$ in cluster $j$ for pseudo-sample $b$.
The two-stage bootstrap is equivalent to first drawing $a_1^{(b)},...,a_J^{(b)}$ from a multinomial distribution with $J$ trials and equal probability on each of $J$ categories, then drawing $a_{1j}^{(b)},...,a_{k_j j}^{(b)}$ from a multinomial distribution with $a_j^{(b)} \times k_j$ trials and equal probability on each of $k_j$ categories, for $j = 1,...,J$.

For constructing confidence intervals, bootstrapping entails generating a total of $B$ pseudo-samples, where $B$ is a large number such as 1999, and re-calculating the estimator for each pseudo-sample. 
There are several methods for constructing confidence intervals from a bootstrap distribution.
We consider four standard methods, all as described by @davison1997bootstrap, including the percentile CI, basic CI, studentized CI, and the bias-corrected-and-accelerated CI proposed by @efron1987better. 
`r appendix_prefix` \@ref(bootstrap-details) provides further details about the bootstrap CI calculations.

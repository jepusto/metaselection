---
title: "Discussion"
output: pdf_document
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: sentence
---

We have described and evaluated several methods for estimating @vevea1995general step-function selection models while accounting for dependent effect sizes, a common feature of meta-analyses in social science fields. 
We focused on the step-function selection model because it offers a number of advantages over other available methods for diagnosing and correcting selective reporting bias. 
First, step-function models are generative, in that they include parameters describing the selective reporting process under simple yet plausible forms of selective reporting connected to statistical significance.
In contrast, regression-based estimators such as PET/PEESE [@stanley2014meta] or the endogenous kink meta-regression [@bom2019kinked] are agnostic as to selection mechanisms and thus are only indirectly informative about the strength or form of selection.
The @vevea1995general step-function model also embeds a familiar evidence-generating model that allows for heterogeneity of effects through inclusion of random effects and predictors of average effect size (i.e., meta-regression).
In contrast, well-known methods such as trim-and-fill [@duval2000nonparametric; @duval2000trim] and recent proposals such as $p$-curve [@simonsohn2014pcurve] and $p$-uniform [@VanAssen2015meta; @vanaert2016conducting] are not as flexible and have been found to perform poorly when effects are heterogeneous [@carter2019correcting].

We treated the step-function model as a description of the _marginal_ distribution of the effect size estimates, effectively ignoring the dependence structure for purposes of estimating the model but accounting for it using cluster-robust sandwich estimation or clustered bootstrap inference. 
This strategy is appealing for its feasibility and because it connects to a selection process in which each individual effect size is selected on the basis of its statistical significance.
We also implemented and studied two estimation methods for the marginal step-function model, composite marginal likelihood estimation and augmented-and-reweighted Gaussian likelihood estimation, and two inference strategies, based on either cluster-robust sandwich estimators or clustered bootstrap resampling methods.

Our simulations examined how well these estimators and inference techniques perform for recovering the average effect size under a selection process with a single step (at $\alpha_1 = .025$), relative to estimators that account for dependence but not selection (i.e., the CHE-ISCW model) and to a variant of PET/PEESE that uses RVE. 
Across an extensive range of conditions, we found that both estimators of the marginal step-function model showed little bias, which was consistently smaller than the bias of PET/PEESE and much smaller than the bias of CHE-ISCW under conditions with meaningful selective reporting.
However, the selection model estimators face a bias-variance trade-off. 
The bias reduction from using the selection model comes at the cost of increased variance from using a marginal model. As a result, the step-function estimators are less accurate than the CHE-ISCW estimator under conditions where selective reporting is not strong or does not create meaningful bias. 
The marginal step-function model estimators also have better coverage compared to the other methods, with coverage rates of the cluster-bootstrapped confidence intervals approaching the nominal level of 0.95 for larger sample sizes. 
Compared to the ARGL estimator, the CML estimator of the marginal mean was usually more accurate  and had confidence interval coverage rates closer to nominal levels, although differences are fairly small.
CML consistently out-performed ARGL for estimating between-study heterogeneity and the strength of selective reporting.

## Limitations and Further Directions

Both the methods we have developed and the findings from our simulation study have limitations that are important to recognize.
On a conceptual level, the strategy of modeling the marginal distribution of effect sizes presents several limitations.
First, such models do not attempt to describe the structure of dependence among effect size estimates drawn from the same sample, but instead describe only the overall average and overall degree of heterogeneity of the effect size distribution.
Because of this, they do not fully align with contemporary approaches to summary meta-analysis and meta-regression analysis, which emphasize use of models that align with the hierarchical structure of dependent effect sizes [@pustejovsky2022preventionscience; @vandennoortgate2013threelevel].

Second, focusing on the marginal distribution likely entails some loss of precision in parameter estimates. 
Accounting for the dependence structure would allow for construction of more efficient estimators of the parameters of the evidence-generating process.
A useful direction for further research would be to explore how to refine the CML and ARGL estimators by incorporating analytic weights connected with the dependence structure of the effect sizes.

Third, the marginal model provides no way to distinguish between study-level publication bias and effect-level selective outcome reporting.
This strategy therefore precludes examination of more nuanced forms of selection, such as one where the probability that a given effect size is reported depends on the significance levels of other effect size estimates drawn from the same sample or on some broader feature of the study's results.
This strikes us as an area in need of further theoretical development---even simply to catalog a wider variety of plausible selective reporting mechanisms. 
However, developing more nuanced models would require shifting to estimation frameworks that can handle multivariate models, such as multivariate likelihood-based estimation or pairwise composite likelihood methods [e.g., @rao2013weighted; @yi2016weighted].

In addition to conceptual limitations, our simulation findings also need to be interpreted cautiously in light of the study's scope limitations.
First, although we tried to cover a wide range of plausible conditions in the simulation study (3,840 conditions for the full simulation and 288 conditions for the bootstrap simulation), the results are only generalizable to the data-generating process presented in the study.
Of particular note, the evidence-generating process we used followed a CHE effects model with primary study sample sizes and the number of effect sizes per study drawn from an empirical distribution of educational research studies.
The performance of the step-function selection models and alternative selective reporting adjustments could vary depending on features of studies included in the synthesis, such as studies drawn from areas of research that tend to use very small or very large samples or that tend to assess a larger number of outcomes.
Likewise, there remains a need to investigate the robustness of the models to other evidence-generating processes, such as non-normal random effects distributions [@Hedges1996estimating].

Second, the simulations examined the step-function selection model using a selection process that was compatible with the assumed model, in which the probability that an effect size was reported followed a step function in the one-sided _p_-value with a threshold at $\alpha_1 = .025$.
@chen2024adapting examined the performance of one-step and two-step selection model estimators under conditions when the true data-generating process involved a two-step selection process with an additional threshold at $\alpha_2 = .500$. 
Their results indicated that a one-step model could be more accurate (i.e., lower RMSE) than a more complex two-step model---even if the former is mis-specified. 
It may require a relatively large number of primary studies to feasibly estimate models that include multiple steps in the selection function.
Nonetheless, there may be meta-analytic datasets where a more complex set of steps is more appropriate, such as when the data include a substantial number of negative effect size estimates.

Third and related to the previous point, the simulations were limited to evidence-generating processes that did not involve systematic predictors of the effect size distribution and where the strength of selective reporting was uniform and solely dependent on the p-value of each individual effect size.
There may be other factors besides statistical significance of findings that affect a study's publication status.
For instance, results from pre-registered replication studies might be insulated from selective reporting or subject to different reporting pressures than other forms of primary research [@vanaert2025metaanalyzing].
Further evaluation of the CML and ARGL estimators is warranted to assess their performance in models involving moderators and their robustness to other selection mechanisms.
In further development of step-function selection models, it may prove useful to model variation in the strength of reporting as a function of study characteristics such as pre-registration status [@Coburn2015publication].

Fourth and finally, the simulation findings we have reported here focused mostly on the performance of the estimators and confidence intervals for the overall average effect size.
We have not directly evaluated how well the estimators work as diagnostic _tests_ for the presence of selective reporting of study results, although the below-nominal coverage rates of confidence intervals for the selection parameter suggests that they may not work well diagnostically. 
In univariate random effects models, likelihood ratio tests based on step-function selection models have been found to provide much stronger power for detecting selective reporting compared to alternatives such as Egger's regression or non-parametric symmetry tests [@pustejovsky2019testing].
Extension of such tests for meta-analyses of dependent effect sizes requires further development.

## Conclusions

The potential for selective reporting of primary study results to distort and create biases in meta-analytic results is simultaneously of great concern to meta-analysts and a vexing problem to address---even in the simpler context of meta-analysis of independent effects (i.e., when each sample reports at most a single effect size estimate).
The challenge of detecting and adjusting for bias from selective reporting is all the more challenging in a corpus of primary study results that includes dependent effect sizes. 
Nonetheless, meta-analysts must critically evaluate the evidence summarized in a synthesis, and this includes weighing the potential for bias from selective reporting and selective publication.

Based on the simulation results we have presented, we recommend that researchers add step function selection models with clustered bootstrap confidence intervals to the toolkit ---currently, quite a limited one---of methods for investigating selective reporting bias in syntheses of dependent effect sizes.
As a parametric model that makes specific assumptions about the selection process, marginal step function models provide a useful complement to more agnostic techniques for identifying small-study effects, such as funnel plots and regression adjustment methods, the bias and accuracy of which are quite variable across data-generating processes.
Likewise, estimated step functions could inform the sensitivity analysis approach proposed by @mathur2020sensitivity, by using estimates of the strength of selection to inform assumptions about the maximal plausible degree of selection. 

Consistent with recommendations from past work in the context of independent effect sizes [@carter2019correcting; @mcshane2016adjusting], interpretation of any bias-corrected effect estimates needs to give consideration to the conditions under which the estimation method could be expected to perform well.
Interpretation of marginal step-function models should focus mostly on the bias-adjusted average effect size, and selection parameter estimates should be interpreted cautiously in light of the poor performance of their cluster bootstrapped confidence intervals.
More broadly, it is critical that meta-analysts applying and interpreting step function models do so while also considering the context of the evidence included in the synthesis, such as whether the effect size estimates are for focal results or merely incidental findings and whether studies were conducted under conditions where pressures to selectively reporting findings are present.
As with inferences from any statistical model, one's conclusions should be informed 
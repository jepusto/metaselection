---
title: "Selection Models for Meta-Analysis with Dependent Effects"
author:
  - James E. Pustejovsky
  - Megha Joshi
  - Martyna Citkowicz
output: 
  rmarkdown::html_vignette:
    number_sections: true
    toc: true
date: "`r Sys.Date()`"
bibliography: references.bib
link-citations: yes
csl: apa.csl
vignette: >
  %\VignetteIndexEntry{P-Value Selection Models for Meta-Analysis with Dependent Effects}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
library(ggplot2)
library(metaselection)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

A systematic review and meta-analysis project aims to provide a comprehensive synthesis of available evidence on a topic of interest. 
One major challenge to this aim is selective reporting of evidence from primary studies. 
Selective reporting occurs when the statistical significance level of a finding influences whether it is reported and therefore whether the finding is available for inclusion in a systematic review. 
Selective reporting can arise from biases in the publication process, on the part of journals, editors, and reviewers, as well as through strategic decisions on part of the authors [@Rothstein2005publication; @sutton2009publication].
If statistically significant, affirmative results are more likely to be reported compared to null, non-affirmative results, the evidence base available for meta-analysis will be distorted, leading to inflation of effect size estimates from meta-analysis [@carter2019correcting; @mcshane2016adjusting] and bias in estimates of heterogeneity [@augusteijn2019effect]. 

Because selective reporting can make it difficult to draw accurate inferences from a meta-analysis, many tools have been developed that try to detect selective reporting problems and correct for the biases they create in meta-analytic summaries. 
Widely used methods include graphical diagnostics like funnel plots [@sterne2001funnel; @Sterne2011recommendations]; tests and adjustments for funnel plot asymmetry such as trim-and-fill [@duval2000nonparametric], Egger's regression [@egger1997bias], and PET/PEESE [@stanley2008meta; @stanley2014meta]; p-value diagnostics such as p-curve and p-uniform [@simonsohn2014pcurve; @VanAssen2015meta; @vanaert2016conducting]; and selection models [@hedges1992modeling; @vevea1995general; @Hedges1996estimating]. 
However, very few methods for investigating selective reporting can also accommodate dependent effect sizes.
This limitation poses a problem for meta-analyses in education, psychology and other social sciences, where dependent effects are a very common feature of meta-analytic data.

Dependent effect sizes occur when primary studies report results for multiple measures of an outcome construct, collected repeated measures an outcome across multiple time-points, or involve comparisons between multiple intervention conditions. 
Generally, failing to account for dependency can result in misleading conclusions,  overly narrow confidence intervals, and hypothesis tests that have inflated type one error rates.
Meta-analysts now have access to an array of methods for summarizing and modeling dependent effect sizes, including multi-level meta-analyses [@konstantopoulos2011fixed; @vandennoortgate2013threelevel; @vandennoortgate2015metaanalysis], robust variance estimation [@Hedges2010robust; @tipton2015small; @tiptonpusto2015small], and combinations thereof [@pustejovsky2022preventionscience]. These methods can be combined with a few of the available techniques for investigating selective reporting, but this is currently limited to techniques based on regression adjustment [@fernandezcastilla2019detecting; @rodgers2020evaluating] or sensitivity analyses based on simple forms of selection models, which provide bounds on average effects given an _a priori_ level of selective reporting [@mathur2020sensitivity]. 
<!-- Another sentence on limitations of regression adjustment methods? -->

The `metaselection` package aims to expand the range of techniques available for investigating selective reporting bias while also accommodating meta-analytic datasets that include dependent effect sizes. In particular, the package provides methods for investigating and accounting for selective reporting based on selection models, where prior developments were limited to data with independent effect sizes.
The package implements two distinct methods to account for dependent effect sizes, using either cluster-robust variance estimation (CRVE, i.e., sandwich estimation) or clustered bootstrapping techniques. 
Simulation results show that applying selection models to dependent effect size estimates leads to low bias in the estimate of the overall effect size. 
Combining the selection models with cluster-bootstrapping leads to confidence intervals with close-to-nominal coverage rates. 

Several existing packages provide implementations of selection models assuming that effect size estimates are independent.
The `metafor` package [@metafor] includes the `selmodel()` function, which allows users to fit many different types of selection models.
The `weightr` package [@weightr] includes functions to estimate a class of p-value selection models described in @vevea1995.
However, the functions available in these packages can only be applied to meta-analytic data assuming that the effect sizes are independent.
In addition, the `PublicationBias` package [@PublicationBias] implements sensitivity analyses for selective reporting bias that incorporate cluster-robust variance estimation methods for handling dependent effect sizes.
However, the sensitivity analyses implemented in the package are based on a pre-specified degree of selective reporting, rather than allowing the degree of selection to be estimated from the data. The sensitivity analyses are also based on a specific and simple form of selection model, and do not allow consideration of more complex forms of selection functions.

# Selection Models

Selection models are a tool for investigating selective reporting by making explicit assumptions about the process by which the effect size estimates are reported [@Rothstein2005publication]. 
Such models have two components: a set of assumptions describing the evidence-generation process and a set of assumptions describing the selection process.
The `metaselection` package implements a flexible class of selection models, in which the evidence-generating process follows a random effects location-scale meta-regression model [@viechtbauer2022locationscale] and where the selection process is a function of one-sided p-values, either in the form of a step function [@vevea1995general] or a beta-density function [@Citkowicz2017parsimonious]. 
The step function model involves specifying steps which categorize the p-values into intervals that have different probabilities of selection [@vevea1995general]. 
The beta-density model uses a different selection function, based on a beta distribution, to capture distinctive, more smoothly varying patterns of selection [@Citkowicz2017parsimonious]. 

Consider a meta-analytic dataset with a total of $J$ samples, where study $j$ includes $k_j$ effect size estimates. 
Let $Y_{ij}$ denote an effect size estimate produced by a samples, prior to selective reporting. 
The effect size estimate has standard error $\sigma_{ij}$, which is treated as a fixed quantity.
Let $\mathbf{x}_{ij}$ be a $1 \times x$ row vector of predictors that encode characteristics of the effect sizes or the samples and may be related to average effect size magnitude. 
Let $\mathbf{u}_{ij}$ be a $1 \times u$ row vector of predictors that may be related to effect size heterogeneity. 
Let $\Phi()$ denote the standard normal cumulative distribution function and $\phi()$ the standard normal density. 
Finally, let $p_{ij}$ be the one-sided p-value corresponding to the effect size estimate, which is a function of the effect size estimate and its standard error: $p_{ij} = 1 - \Phi\left(Y_{ij} / \sigma_{ij}\right) = \Phi\left(-Y_{ij} / \sigma_{ij}\right)$.

## The evidence-generating process

The model for the evidence-generating process is a random effects location-scale meta-regression model, in which
$$
Y_{ij} = \mathbf{x}_{ij} \boldsymbol\beta + v_{ij} + e_{ij},
$$
where $\boldsymbol\beta$ is a $1 \times x$ vector of regression coefficients that relate the predictors to average effect size magnitude, $v_i$ is a normally distributed random effect with mean zero and variance $\tau^2_{ij}$, and $e_{ij}$ is a normally distributed sampling error with mean zero and known variance $\sigma^{2}_{ij}$. 
The variance of the random effects is modeled as
$$
\log\left(\tau_{ij}^{2}\right) = \mathbf{u}_{ij} \boldsymbol\gamma,
$$
where $\boldsymbol\gamma$ is a $u \times 1$ vector of coefficients that relate the predictors to the degree of marginal variation in the random effects.
If the model does not include predictors of heterogeneity, then $\mathbf{u}_{ij} = 1$ and the model reduces to a conventional random effects meta-regression in which $\gamma = \log(\tau^2)$.

Note that this random-effects location scale model treats each observed effect size as if it were independent, even though the data may include multiple, statistically dependent effect size estimates generated from the same sample. Thus, it is a model for the _marginal_ distribution of effect size estimates, which does not attempt to capture the dependence structure among effect size estimates drawn from the same sample. As a result, the regression coefficients $\boldsymbol\beta$ describe the overall average effects (given the predictors) and variance parameters $\boldsymbol\gamma$ describe the marginal or _total_ heterogeneity of the effect size distribution, rather than decomposing the heterogeneity into within-sample and between-sample components. 

## Selective reporting processes

At a general level, the selective reporting process is defined by a selection function, which specifies the probability that an effect size estimate is reported given its $p$-value. 
Let $O_{ij}$ be an indicator for whether effect size $i$ in study $j$ is observed. 
Then the selection model defines $\text{Pr}(O_{ij} = 1 \ | \ p_{ij}) = \text{Pr}(O_{ij} = 1 \ | \ Y_{ij}, \sigma_{ij})$. 
The package includes two different forms of selection functions: step functions and beta-density functions.

### Step functions

@hedges1992modeling and @vevea1995general proposed to model the selective reporting process using a step-function, with thresholds chosen to correspond to "psychologically salient" $p$-values. In the general formulation, suppose that there are $H$ steps, $\alpha_1,...,\alpha_H$, and that 
$$
\text{Pr}(O_{ij} = 1 | p_{ij}) = \begin{cases}
1 & \text{if} & p_{ij} < \alpha_1 \\ 
\lambda_1 & \text{if} & \alpha_1 \leq p_{ij} < \alpha_2 \\ \lambda_2 & \text{if} & \alpha_2 \leq p_{ij} < \alpha_3 \\
\vdots \\
\lambda_H & \text{if} & \alpha_H \leq p_{ij}.
\end{cases}
$$
The selection parameters $\lambda_1,...,\lambda_H$ control the probabilities of selection given a p-value, with parameter $\lambda_h$ defined as the relative probability that an effect size estimate is observed, given that its p-value is in the range $[\alpha_h, \alpha_{h+1})$, compared to the probability that an effect size estimate is observed, given that its p-value is in the range $[0, \alpha_1)$. 
The model is estimated in terms of log-transformed relative probabilities so that the parameter space is unrestricted, with $\zeta_h = \log(\lambda_h)$, where $-\infty < \zeta_h < +\infty$, for $h=1,...,H$. 
With this parameterization, $\zeta_1 = \cdots = \zeta_H = 0$ describes a process where effect sizes are reported with uniform probability regardless of their statistical significance levels.


In practice, meta-analysts will often use only a small number of steps in the selection model. 
One common choice is the three-parameter selection model, which has a single step at $\alpha_1 = .025$, as depicted in the first figure below. With this choice of threshold, positive effects that are statistically significant at the two-sided level of $p < .05$ have a different probability of selection than effects that are not statistically significant or not in the anticipated direction.

```{r, echo = FALSE, fig.retina = 2, fig.width = 5, fig.height = 3, out.width = "75%", fig.cap = "One-step selection model with $\\lambda_1 = 0.4$"}
lambda1 <- 0.4
lambda2 <- 0.2
pvals <- seq(0,1,.005)
PSM3 <- step_fun(cut_vals = .025, weights = lambda1)
PSM4 <- step_fun(cut_vals = c(.025, .500), weights = c(lambda1, lambda2))

dat <- data.frame(p = pvals, PSM3 = PSM3(pvals), PSM4 = PSM4(pvals))

ggplot(dat, aes(x = pvals)) + 
  scale_y_continuous(limits = c(0,1.1), expand = expansion(0,0)) + 
  scale_x_continuous(breaks = seq(0,1,0.2), expand = expansion(0,0)) + 
  geom_vline(xintercept = 0.025, linetype = "dashed") + 
  geom_hline(yintercept = 0) + 
  geom_area(aes(y = PSM3), fill = "green", alpha = 0.6) +   
  theme_minimal() + 
  labs(x = "p-value (one-sided)", y = "Selection probability")
```

Another possibility is to use two steps at $\alpha_1 = .025$ and $\alpha_2 = .500$, which allows for different probabilities of selection for effects that are positive but not statistically significant and effects that are negative (i.e., in the opposite the intended direction). 
We call the latter model a four-parameter selection model; it is depicted in the figure below.

```{r, echo = FALSE, fig.retina = 2, fig.width = 5, fig.height = 3, out.width = "75%", fig.cap = "Two-step selection model with $\\lambda_1 = 0.4, \\lambda_2 = 0.2$"}
ggplot(dat, aes(x = pvals)) + 
  scale_y_continuous(limits = c(0,1.1), expand = expansion(0,0)) + 
  scale_x_continuous(breaks = seq(0,1,0.2), expand = expansion(0,0)) + 
  geom_vline(xintercept = c(0.025, .500), linetype = "dashed") + 
  geom_hline(yintercept = 0) + 
  geom_area(aes(y = PSM4), fill = "purple", alpha = 0.6) +   
  theme_minimal() + 
  labs(x = "p-value (one-sided)", y = "Selection probability")
```


### Beta-density functions

@Citkowicz2017parsimonious proposed a selection model based on an alternative form of selection function, where the probability of reporting follows a beta density. Compared to a step function, the beta density selection function can capture a very different set of shapes, with selection probabilities that vary smoothly over the range of possible one-sided $p$-values. In the original formulation, the selection function is given by
$$
\text{Pr}(O_{ij} = 1 | p_{ij}) = p_{ij}^{(\lambda_1 - 1)} \left(1 - p_{ij}\right)^{(\lambda_2 - 1)},
$$
for $\lambda_1, \lambda_2 > 0$. 

```{r, echo = FALSE}
pvals <- c(.005, .500, .900)
sel_probs_un <- beta_fun(delta_1 = .4, delta_2 = 5, trunc_1 = 1e-6, trunc_2 = 1 - 1e-6)(pvals)
rel_sel_un <- sel_probs_un[-1] / sel_probs_un[1]
sel_probs_trunc <- beta_fun(delta_1 = .4, delta_2 = 5, trunc_1 = .025, trunc_2 = .975)(pvals)
rel_sel_trunc <- sel_probs_trunc[-1] / sel_probs_trunc[1]
```

In a commentary on @Citkowicz2017parsimonious, @hedges2017plausibility highlighted an important limitation of using the beta-density as a selection function. For some parameter values, the beta density implies selection probabilities that differ by many orders of magnitude. Following the example discussed in @hedges2017plausibility, the beta density with selection parameters $\lambda_1 = 0.4, \lambda_2 = 5$ implies that the probability of reporting an effect with $p_{ij} = .5$ (corresponding to $T_{ij} = 0$) is only `r round(rel_sel_un[1], 3)` times the probability of reporting a significant effect with $p_{ij} = .005$, and the probability of reporting a non-significant, negative effect with $p_{ij} = .9$ is `r formatC(rel_sel_un[2], digits = 6, format = "f")` times the probability of reporting a significant effect with $p_{ij} = .005$. These extreme differences in selection probability can imply implausible selection processes, in which hundreds of non-significant effect size estimates would need to go unreported to observe a sample of a few dozen findings. Furthermore, they lead to estimates of average effects that can be highly sensitive to the inclusion or exclusion of some effect size estimates because the influence of each estimate is driven by the inverse of its selection probability [@hedges2017plausibility]. 

To mitigate these issues, the `metaselection` package implements a modification of the original selection function in which the selection probabilities are truncated at user-specified steps, $\alpha_1$ and $\alpha_2$. The truncated density is given by
$$
\text{Pr}(O_{ij} = 1 | p_{ij}) = \begin{cases}
\alpha_1^{(\lambda_1 - 1)} \left(1 - \alpha_1\right)^{(\lambda_2 - 1)} & \text{if} & p_{ij} < \alpha_1 \\ 
p_{ij}^{(\lambda_1 - 1)} \left(1 - p_{ij}\right)^{(\lambda_2 - 1)} & \text{if} & \alpha_1 \leq p_{ij} < \alpha_2 \\ 
\alpha_2^{(\lambda_1 - 1)} \left(1 - \alpha_2\right)^{(\lambda_2 - 1)} & \text{if} & \alpha_2 \leq p_{ij}.
\end{cases}
$$
Equivalently, the truncated beta function can be written as
$$
\text{Pr}(O_{ij} = 1 | p_{ij}) = \tilde{p}_{ij}^{(\lambda_1 - 1)} \left(1 - \tilde{p}\right)^{(\lambda_2 - 1)},
$$
where $\tilde{p}_{ij} = \min\{\max\{\alpha_1, p_{ij}\}, \alpha_2\}$. 
Using this modification, one might set truncation points at $\alpha_1 = .025$ and $\alpha_2 = .975$ (these are the default values in `metaselection`) so that all statistically significant positive effect size estimates have the same selection probability and, likewise, all statistically significant, negative effect size estimates have the same selection probability, with the selection probabilities of non-significant effect sizes varying according to a beta density. The first figure below depicts this truncated beta density using the default truncation points and with $\lambda_1 = 0.4, \lambda_2 = 5$, which represents very strong selection. The second figure depicts a truncated beta density with more moderate values of $\lambda_1 = 0.7, \lambda_2 = 1$ and where the second truncation point is set at $\alpha_2 = 0.5$, so that all negative effect size estimates have equal selection probability.

```{r, echo = FALSE, fig.retina = 2, fig.width = 5, fig.height = 3, out.width = "75%", fig.cap = c("Beta-density selection model with $\\lambda_1 = 0.4, \\lambda_2 = 5.0$, using truncation points $\\alpha_1 = .025, \\alpha_2 = .975$", "Beta-density selection model with $\\lambda_1 = 0.7, \\lambda_2 = 1$,  using truncation points $\\alpha_1 = .025, \\alpha_2 = .500$")}

pvals <- seq(0,1,.005)
beta_strong <- beta_fun(delta_1 = .4, delta_2 = 5, trunc_1 = .025, trunc_2 = .975)
beta_mild <- beta_fun(delta_1 = 0.7, delta_2 = 1, trunc_1 = .025, trunc_2 = .500)

dat <- data.frame(p = pvals, strong = beta_strong(pvals), mild = beta_mild(pvals))

ggplot(dat, aes(x = pvals)) + 
  scale_y_continuous(limits = c(0,1.1), expand = expansion(0,0)) + 
  scale_x_continuous(breaks = seq(0,1,0.2), expand = expansion(0,0)) + 
  geom_vline(xintercept = c(0.025, .975), linetype = "dashed") + 
  geom_hline(yintercept = 0) + 
  geom_area(aes(y = strong), fill = "red", alpha = 0.6) +   
  theme_minimal() + 
  labs(x = "p-value (one-sided)", y = "Selection probability")

ggplot(dat, aes(x = pvals)) + 
  scale_y_continuous(limits = c(0,1.1), expand = expansion(0,0)) + 
  scale_x_continuous(breaks = seq(0,1,0.2), expand = expansion(0,0)) + 
  geom_vline(xintercept = c(0.025, .500), linetype = "dashed") + 
  geom_hline(yintercept = 0) + 
  geom_area(aes(y = mild), fill = "yellow", alpha = 0.6) +   
  theme_minimal() + 
  labs(x = "p-value (one-sided)", y = "Selection probability")
```

In the package, the beta-density function is parameterized in terms of $\zeta_h = \log(\lambda_h)$, where $-\infty < \zeta_h < +\infty$, for $h=1,2$. 
With this parameterization, $\zeta_1 = \zeta_2 = 0$ describes a process where effect sizes are reported with uniform probability regardless of their statistical significance levels.

## Estimation

The combination of assumptions about the evidence-generating process and assumptions about the selection process implies a marginal distribution for the observed effect sizes. 
The evidence-generating process describes the distribution $\text{Pr}(Y_{ij} = y_{ij} \ | \ \sigma_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij})$ and the selection process specifies $\text{Pr}(O_{ij} = 1 \ | \ Y_{ij} = y, \sigma_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij})$.
The distribution of observed effect size estimates then corresponds to 
$$
\text{Pr}(Y_{ij} = y_{ij} \ | \ O_{ij} = 1, \sigma_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij}) = \frac{\text{Pr}(O_{ij} = 1 \ | \ Y_{ij} = y_{ij},\sigma_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij}) \times \text{Pr}(Y_{ij} = y_{ij} \ | \ \sigma_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij})}{\text{Pr}(O_{ij} = 1 \ | \ \sigma_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij})}.
$$
The distribution of observed effect sizes depends on the meta-regression coefficients $\boldsymbol\beta$, the scale regression coefficients $\boldsymbol\gamma$, and the selection parameters $\boldsymbol\zeta$. 

The `metaselection` package implements two different estimation strategies: composite maximum likelihood and hybrid estimating equations. In the first, model parameter estimates are obtained by taking the values that maximize the log-likelihood of the observations, treating each effect size estimate as if it were independent. Denoting the log likelihood of observation $i$ from sample $j$ as $\mathcal{l}_C(\boldsymbol\beta, \boldsymbol\gamma, \boldsymbol\zeta | y_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij}) = \log \text{Pr}(Y_{ij} = y_{ij} \ | \ O_{ij} = 1, \sigma_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij})$, the composite log-likelihood is
$$
\mathcal{l}_C\left(\boldsymbol\beta, \boldsymbol\gamma, \boldsymbol\zeta\right) = \sum_{j=1}^J \sum_{i=1}^{k_j} \mathcal{l}_C(\boldsymbol\beta, \boldsymbol\gamma, \boldsymbol\zeta | y_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij}).
$$
The composite maximum likelihood estimator is then taken to be the parameter values that maximize $\mathcal{l}_C\left(\boldsymbol\beta, \boldsymbol\gamma, \boldsymbol\zeta\right)$. 

An alternative estimation strategy, hybrid estimating equations, defines the parameter estimator as the solution to a set of mean-zero estimating equations. In this approach, the selection parameters are estimated using their score equations, just as in the composite maximum likelihood estimator. Unlike the composite maximum likelihood strategy, the meta-regression coefficients and scale regression coefficients are estimated using the re-weighted log likelihood of the data-generating process, with weights equal to the inverse probability of selection. This strategy was initially suggested by @mathur2020sensitivity, who focused on using inverse probability of selection weighting for purposes of sensitivity analysis with selection parameters specified a priori. In constrast, the `metaselection` package implementation applies the strategy to jointly estimate the meta-regression, scale regression, and selection parameters. 

## Cluster-robust variance estimation

If each sample included in a meta-analysis provides just a single effect size estimate (i.e., if $k_j = 1$ for $j = 1,...,J$), then standard errors and confidence intervals for the selection model parameters can be constructed using standard techniques based on the inverse of the Fisher information under the model. 
Such an approach is used in other implementations of selection models, including the `selmodel()` function in the `metafor` package [@Viechtbauer2010conducting] and the `weightfunct()` function in the `weightr` package [@weightr].[^model-based]
However, this approach is predicated on the assumption that the effect size estimates are mutually independent. Thus, it is inappropriate if the data include samples that provide multiple, statistically dependent effect size estimates.

[^model-based]: The `metaselection` package also provides standard errors based on the inverse of the observed Fisher information matrix, which are valid when effect size estimates are mutually independent. These can be obtained by setting `vcov_type = 'model-based' in  the call to `selection_model()`.

For meta-regression models that do not account for selective reporting problems, @Hedges2010robust proposed to use CRVE methods (also known as sandwich estimators) that accommodate dependent effect sizes, even if the exact dependence structure is not correctly specified. The form of CRVE originally described in @Hedges2010robust is based on large-sample approximations and requires a relatively large number of independent samples (each of which might have multiple effect size estimates) to function well. 
Subsequent work developed small-sample refinements to CRVE, including adjustments to hypothesis tests and confidence intervals based on CRVE, which are accurately calibrated even for datasets that include a small number of independent clusters [@tipton2015small; @tiptonpusto2015small].

CRVE methods can also be applied to selection models in order to quantify uncertainty in parameter estimates obtained by composite maximum likehood or hybrid estimating equations.
The `metaselection` package provides standard errors and confidence intervals based on CRVE methods by default. 
The implementation is similar to the original, large-sample CRVE methods described by @Hedges2010robust; the subsequently developed small-sample refinements are not currently available for selection models. 
It is important to bear in mind that CRVE methods are the default not because they are the best available method, but rather because they are computationally convenient and quicker to compute compared to bootstrap-based methods.

For composite maximum likelihood estimators, CRVE matrices can be computed using the score (first derivative) and Hessian (second derivative) of the likelihood. Denoting the full parameter vector by $\boldsymbol\theta = (\boldsymbol\beta', \boldsymbol\gamma', \boldsymbol\zeta')'$ and the corresponding maximum likelihood estimator as $\boldsymbol{\hat\theta}$, the score contribution of sample $j$ is 
$$
\mathbf{S}_j(\boldsymbol\theta) = \sum_{i=1}^{k_j} \frac{\partial\ \mathcal{l}_C \left(\boldsymbol\theta | \ y_{ij}, \mathbf{x}_{ij}, \mathbf{u}_{ij} \right)}{\partial \boldsymbol\theta}
$$
and the Hessian contribution of sample $j$ is 
$$
\mathbf{H}_j(\boldsymbol\theta) = \frac{\partial\ \mathbf{S}_j(\boldsymbol\theta)}{\partial \boldsymbol\theta'}.
$$
The CRVE matrix is computed as
$$
\mathbf{V}^{CR} = \left(\sum_{j=1}^J \mathbf{H}_j(\boldsymbol{\hat\theta})\right)^{-1} \left(\sum_{j=1}^J \mathbf{S}_j(\boldsymbol{\hat\theta}) \mathbf{S}_j'(\boldsymbol{\hat\theta}) \right) \left(\sum_{j=1}^J \mathbf{H}_j(\boldsymbol{\hat\theta})\right)^{-1},
$$
where the score and Hessian contributions are evaluated at the composite maximum likelihood estimates. 
With a sufficiently large number of independent samples, confidence intervals for model parameters can be constructed using Wald test statistics. For example, a large-sample, $1 - \alpha$ confidence interval for $\beta_f$ is given by
$$
\hat\beta_f \pm \Phi^{-1}(1 - \alpha / 2) \times \sqrt{V^{CR}_{\beta f}},
$$
where $V^{CR}_{\beta f}$ is the diagonal entry of $\mathbf{V}^{CR}$ corresponding to $\hat\beta_f$.
Very similar methods are implemented to obtain standard errors with the hybrid estimating equations approach.

## Bootstrapped confidence intervals

An alternative to CRVE is to use bootstrap re-sampling methods to quantify uncertainty in parameter estimates. 
Bootstrapping involves re-sampling many times from the original data to create an empirical distribution that can be used as a proxy for the actual sampling distribution of parameter estimates [@boos2003introduction]. 
With dependent data structures, entire clusters of observations are re-sampled so that each bootstrapped dataset includes dependent observations, emulating the structure of the original dataset. 
In the implementation in the `metaselection` package, this approach is called the multinomial bootstrap. 
The package also implements a different variation of bootstrapping, known as the fractional random weight bootstrap [@xu2020applications] or Bayesian bootstrap [@rubin1981bayesian; @newton1994approximate].
This variation involves assigning a random weight to each cluster of dependent observations, where the weights are simulated from an exponential distribution with mean 1. 

Several different methods can be used to construct confidence intervals from a bootstrap distribution [@davison1997bootstrap], which use various approximations and therefore vary in the accuracy of their coverage levels.
The `metaselection` package implements three different techniques, following the same methods and terminology as in the `boot` package [@boot]. For a meta-regression coefficient $\beta_f$, let $\hat\beta_f$ denote the point estimator, $SE_f$ denote its robust standard error, $\hat\beta_f^{(b)}$ denote a bootstrap replicate of the estimator, and $q(\alpha; \hat\beta_f^{(b)})$ denote the $\alpha$ quantile of $\hat\beta_f^{(1)},...,\hat\beta_f^{(B)}$. The methods used to construct a $1 - \alpha$ confidence interval are as follows:

* The percentile  confidence interval (`CI_method = "percentile"`) simply takes quantiles of the bootstrap distribution, $q\left(\frac{\alpha}{2}; \hat\beta_f^{(b)}\right)$ and $q\left(1 - \frac{\alpha}{2}; \hat\beta_f^{(b)}\right)$. 
* The "basic" confidence interval (`CI_method = "basic"`) pivots the bootstrap distribution around the point estimate, so that the interval has end-points corresponding to $2 \times \hat\beta_f - q\left(1 - \frac{\alpha}{2}; \hat\beta_f^{(b)}\right)$ and $2 \times \hat\beta_f - q\left(\frac{\alpha}{2}; \hat\beta_f^{(b)}\right)$. 
* The studentized confidence interval (`CI_method = "student"`) is based on the bootstrap distribution of the $t$ statistic rather than the point estimator of a parameter. It has endpoints $\hat\beta_f - SE_f \times q\left(1  - \frac{\alpha}{2}; t^{(b)}\right)$ and $\hat\beta_f - SE_f \times q\left(\frac{\alpha}{2}; t^{(b)}\right)$, where $t^{(b)} = \frac{\hat\beta_f^{(b)} - \hat\beta_f}{SE_f^{(b)}}$ is the t statistic from a bootstrap replicate, taking the original point estimator as the null. 


Our simulation results indicate that bootstrap confidence intervals, particularly using the multinomial bootstrap approach with percentile confidence intervals, leads to coverage rates that are close to nominal levels whereas the coverage rates provided by the CRVE method are below nominal. We highly recommend using the multinomial bootstrap with the percentile confidence intervals. 

Bootstrap confidence intervals require re-estimating the selection model and re-calculating parameter estimates on each re-sampled dataset, which is a computationally demanding process.
Furthermore, obtaining accurate confidence intervals requires using a relatively large number of bootstrap replications [@davidson2000bootstrap]; using an insufficient number will produce confidence intervals that are too narrow and have below-nominal coverage. 
We recommend using 1999 replications, and this is the default used in the `selection_model()` function when the `bootstrap` argument is set to `"multinomial"` or `"exponential"`. 
The computational demands of bootstrapping can be mitigated by using parallel processing, as we demonstrate below.
 
# Using the `metaselection` package

```{r, echo = FALSE}
data("dat.lehmann2018", package = "metadat")
n_ES <- nrow(dat.lehmann2018)
n_studies <- length(table(dat.lehmann2018$Full_Citation))
```

We now demonstrate the key functions from the `metaselection` package.
As a running example, we use data from a meta-analysis by @lehmann2018meta, who examined the effects of exposure to the color red on judgements of attractiveness.
The dataset is available in the `metadat` package [@metadat] as `dat.lehmann2018`. 
It consists of `r n_ES` effect sizes from `r n_studies` studies. The following code loads the dataset and creates variables that will be needed for the subsequent analysis.

```{r lehmann}
data("dat.lehmann2018", package = "metadat")
dat.lehmann2018$study <- dat.lehmann2018$Full_Citation
dat.lehmann2018$sei <- sqrt(dat.lehmann2018$vi)
dat.lehmann2018$esid <- 1:nrow(dat.lehmann2018) 
```

## Prelminary Analysis 

As a point of comparison, we first run an analysis that ignores the possibility of selective reporting bias but accounts for the dependence structure of the effect sizes using a correlated-and-hierarchical effects (CHE) working model and CRVE [@pustejovsky2022preventionscience]. 
The following code first creates a sampling variance-covariance matrix assuming that effect size estimates from the same study have sampling errors that are correlated at 0.8. It then fits a CHE working model and applies robust variance estimation, clustering by study.

```{r CHE}
library(metafor)
library(clubSandwich)

# Create sampling variance-covariance matrix
V_mat <- vcalc(
  vi = vi, 
  cluster = study,
  obs = esid, 
  data = dat.lehmann2018,
  rho = .8,
  sparse = TRUE
)

# First CHE working model
CHE_mod <- rma.mv(
  yi = yi, V = V_mat,
  random = ~ 1 | study / esid,
  data = dat.lehmann2018,
  sparse = TRUE
) |>
  
# Apply RVE with small-sample corrections, clustering by study
  robust(cluster = study, clubSandwich = TRUE)

CHE_mod
```

The overall estimate of the average effect is `r round(as.numeric(CHE_mod$beta), 3)`, 95% CI [`r round(as.numeric(CHE_mod$ci.lb), 3)`, `r round(as.numeric(CHE_mod$ci.ub), 3)`], which is significantly different from zero (p = `r round(CHE_mod$pval, 3)`). 
The estimated total variance (including both between- and within-study heterogeneity) is `r round(sum(CHE_mod$sigma2), 3)`, corresponding to a total standard deviation of `r round(sqrt(sum(CHE_mod$sigma2)), 3)`.
Next, we examine how this average effect size estimate differs from the estimates based on a step-function or beta-function selection model, fitted using the `metaselection` package.

## Three-Parameter Step Function with RVE

The primary function for fitting p-value selection models is `selection_model()`. 
In the code below, we fit a step-function selection model to the `dat.lehmann2018` data using the `selection_model()` function. 
We specify which variable is the effect size, `yi`, and which is the standard error for the effect size, `sei`. 
We indicate that we want to estimate a `"step"` selection model and specify a single step at .025 by setting  `step = 0.025`. By default, the function fits the model using composite maximum likelihood estimation and calculates standard errors and confidence intervals using CRVE.

```{r 3PSM}
library(metaselection)

mod_3PSM <- selection_model(
  data = dat.lehmann2018, 
  yi = yi,
  sei = sei,
  cluster = study,
  selection_type = "step",
  steps = .025
)

mod_3PSM
```

The estimate of the overall average effect is now `r round(mod_3PSM$est$Est[mod_3PSM$est$param=="beta"], 3)`, which is over a third smaller than the estimate that does not account for selection bias (`r round(CHE_mod$beta, 3)`). This estimate is no longer statistically distinct from zero, as indicated by the 95% confidence interval [`r round(mod_3PSM$est$CI_lo[mod_3PSM$est$param=="beta"], 3)`, `r round(mod_3PSM$est$CI_hi[mod_3PSM$est$param=="beta"], 3)`].

The estimates of the total heterogeneity parameter and the selection parameter are on the log-scale. We can obtain an estimate of the total variance by setting the argument `transf_gamma` to `TRUE` in the `print()` method. Similarly, we  can transform the selection parameter estimates to the probability scale by setting `transf_zeta = TRUE`:
```{r}
print(mod_3PSM, transf_gamma = TRUE, transf_zeta = TRUE)
```
The estimated total heterogeneity of `r round( exp(mod_3PSM$est$Est[mod_3PSM$est$param=="gamma"]), 3)` is somewhat smaller than the total heterogeneity estimate from the CHE model, but is also very imprecisely estimated. 
The selection parameter is now called `lambda_1`. The estimate of `r round(exp(mod_3PSM$est$Est[mod_3PSM$est$param=="zeta1"]), 3)` indicates that effect size estimates with one-sided p-values greater than .025 are only about half as likely to be reported as estimates that are positive and statistically significant (i.e., estimates with $p < .025$). 
The selection parameter estimate is quite imprecise, with a 95% confidence interval of `r round(exp(mod_3PSM$est$CI_lo[mod_3PSM$est$param=="zeta1"]), 3)` to `r round(exp(mod_3PSM$est$CI_hi[mod_3PSM$est$param=="zeta1"]), 3)`, which includes the value $\lambda_1 = 1$ corresponding to no selective reporting.

For more detailed information about the results, the `metaselection` package also provides a `summary()` function which also includes the `transf_gamma` and `transf_zeta` arguments:
```{r}
summary(mod_3PSM, transf_gamma = TRUE, transf_zeta = TRUE)
```


Furthermore, the `metaselection` package provides a function `selection_plot()` to visualize the estimated selection weights:

```{r 3PSM-plot, fig.retina = 2, fig.width = 5, fig.height = 3, out.width = "75%"}
selection_plot(mod_3PSM)
```

The plot illustrates how the likelihood of selection differs as a function of the one-sided p-value of an effect size estimate. In this example, the plot shows that studies with p-values > 0.025 are about half as likely ($\lambda_1 = `r round(exp(mod_3PSM$est$Est[mod_3PSM$est$param=="zeta1"]), 3)`$) to be published than studies with smaller p-values.

## Four-Parameter Step Model with RVE

Rather than using a single threshold at $\alpha_1 = .025$, we could fit a model that also allows the selection probability for negative effect size estimates to differ from the selection probability for positive but non-significant estimates. The following code fits such a model, setting `steps = c(.025, .500)`:
```{r}
mod_4PSM <- selection_model(
  data = dat.lehmann2018, 
  yi = yi,
  sei = sei,
  cluster = study,
  selection_type = "step",
  steps = c(.025, .500)
)

print(mod_4PSM, transf_gamma = TRUE, transf_zeta = TRUE)
```
The estimate of the overall average effect is `r round(mod_4PSM$est$Est[mod_4PSM$est$param=="beta"], 3)`, even smaller than the estimated effect from the three-parameter step model and only one third the magnitude of the estimate that does not account for selection bias.

We can visualize the estimated selection function with `selection_plot()`:
```{r 4PSM-plot, fig.retina = 2, fig.width = 5, fig.height = 3, out.width = "75%"}
selection_plot(mod_4PSM)
```

As is apparent from the plot, this estimated model indicates that negatively signed effects (i.e., those with a one-sided p-value greater than .50) are even less likely to be observed than effects that are positive but not statistically significant.
However, as can be seen from the robust confidence intervals in the model output, the selection parameters are very imprecisely estimated. 

## Beta Function with RVE

The `selection_model()` function also allows fitting models based on beta density selection functions by specifying `selection_type = "beta"`. The default estimator for the beta function model is maximum likelihood; the hybrid estimator is not yet available.  

```{r beta}
mod_beta <- selection_model(
  data = dat.lehmann2018, 
  yi = yi,
  sei = sei,
  cluster = study,
  selection_type = "beta",
  steps = c(.025, .975)
)

print(mod_beta, transf_gamma = TRUE, transf_zeta = TRUE)
```

The estimate of the overall average effect is `r round(mod_beta$est$Est[mod_beta$est$param=="beta"], 3)`, which is smaller than both the estimate that does not account for selection bias (`r round(CHE_mod$beta, 3)`) and the estimates from the three- and four-parameter step function models. The average effect estimate based on the beta function is not statistically distinct from zero, as indicated by the 95% confidence interval [`r round(mod_beta$est$CI_lo[mod_beta$est$param=="beta"], 3)`, `r round(mod_beta$est$CI_hi[mod_beta$est$param=="beta"], 3)`].

To see how the probability of selection differs across studies with different p-values, we can again visualize the selection function:

```{r beta-plot, fig.retina = 2, fig.width = 5, fig.height = 3, out.width = "75%"}
selection_plot(mod_beta)
```

The plot shows that effect sizes with smaller p-values are more likely to be published than effects with larger p-values. For example, an effect size estimate of zero (with one-sided p-value of $p = .500$) is about half as likely to be published as an effect with a statistically significant, positive effect.

## Bootstrap Confidence Intervals

Rather than relying on robust variance estimation to construct standard errors and confidence intervals, it is advisable to instead use confidence intervals based on clustered bootstrap re-sampling. The code below re-fits the three-parameter step function model to obtain cluster-bootstrap confidence intervals. We specify `bootstrap = "multinomial"` to run cluster bootstrapping and we specify that we want `"percentile"` bootstrap confidence intervals as the recommended approach. We specify that number of bootstraps by setting R to `199`. We set the value to 199 here solely to limit the amount of computation. In practice, we recommend using a much higher number of bootstrap replications, such as 1999, to obtain confidence intervals with more accurate coverage rates [@davidson2000bootstrap]. We highly recommend running the selection models with cluster bootstrapping, particularly the regular multinomial bootstrap with percentile confidence intervals, as this has been shown to improve confidence interval coverage rates relative to using other forms of bootstrap confidence intervals. 

```{r 3PSM-bootstrap}
set.seed(20240916)

system.time(
  mod_3PSM_boot <- selection_model(
    data = dat.lehmann2018, 
    yi = yi,
    sei = sei,
    cluster = study,
    selection_type = "step",
    steps = .025,
    CI_type = "percentile",
    bootstrap = "multinomial",
    R = 199
  )
)

print(mod_3PSM_boot, transf_gamma = TRUE, transf_zeta = TRUE)
```

The overall estimate of the average effect does not change when bootstrapping is applied (`r round(mod_3PSM_boot$est$Est[mod_3PSM_boot$est$param=="beta"], 3)`). However, the confidence internal is narrower, [`r round(mod_3PSM_boot$est$percentile_lower[mod_3PSM_boot$est$param=="beta"], 3)`, `r round(mod_3PSM_boot$est$percentile_upper[mod_3PSM_boot$est$param=="beta"], 3)`] (due partially to the use of a smaller-than-desirable number of bootstrap replications).

The bootstrapping routine is implemented to work with the `future` and `future.apply` packages for parallel processing. For example, the following code specifies to use a `multisession` future processing plan with 4 worker nodes, then fits the same model as above:
```{r parallel-boot}
library(future)
library(future.apply)
plan(multisession, workers = 4L)

system.time(
  selection_model(
    data = dat.lehmann2018, 
    yi = yi,
    sei = sei,
    cluster = study,
    selection_type = "step",
    steps = .025,
    CI_type = "percentile",
    bootstrap = "multinomial",
    R = 199
  )  
)

```
Parallel processing substantially reduces the overall computing time. If available, using a larger number of workers would further reduce computing time.

Setting a `sequential` plan will discontinue use of parallel processing:
```{r sequential-boot}
plan(sequential)
```

# References


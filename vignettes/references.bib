@article{kossmeier2020PowerEnhanced,
  title = {Power-Enhanced Funnel Plots for Meta-Analysis},
  author = {Kossmeier, Michael and Tran, Ulrich S. and Voracek, Martin},
  date = {2020-03-31},
  journaltitle = {Zeitschrift für Psychologie},
  publisher = {{Hogrefe Publishing}},
  issn = {2151-2604},
  url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000392},
}

@incollection{Sterne2005funnel,
  title = {The Funnel Plot},
  booktitle = {Publication {{Bias}} in {{Meta-Analysis}}: {{Prevention}}, {{Assessment}}, and {{Adjustments}}},
  author = {Sterne, Jonathan A. C. and Becker, Betsy Jane and Egger, Matthias},
  editor = {Rothstein, Hannah R and Sutton, Alex J and Borenstein, Michael},
  date = {2005},
  pages = {73--98},
  publisher = {{John Wiley \& Sons}},
  location = {{West Sussex, England}},
  doi = {10.1002/0470870168},
  chapter = {5}
}

@article{sterne2011Recommendations,
  title = {Recommendations for Examining and Interpreting Funnel Plot Asymmetry in Meta-Analyses of Randomised Controlled Trials},
  author = {Sterne, Jonathan A. C. and Sutton, Alex J. and Ioannidis, John P. A. and Terrin, Norma and Jones, David R. and Lau, Joseph and Carpenter, James and Rücker, Gerta and Harbord, Roger M. and Schmid, Christopher H. and Tetzlaff, Jennifer and Deeks, Jonathan J. and Peters, Jaime and Macaskill, Petra and Schwarzer, Guido and Duval, Sue and Altman, Douglas G. and Moher, David and Higgins, Julian P. T.},
  date = {2011-07-22},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  volume = {343},
  pages = {d4002},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.d4002},
}

@book{light1984Summing,
  title = {Summing {{Up}}},
  author = {Light, Richard J. and Pillemer, David B.},
  date = {1984-10-15},
  eprint = {qel3lAm4K6gC},
  eprinttype = {googlebooks},
  publisher = {{Harvard University Press}},
  abstract = {How can a scientist or policy analyst summarize and evaluate what is already known about a particular topic? This book offers practical guidance.The amount and diversity of information generated by academic and policy researchers in the contemporary world is staggering. How is an investigator to cope with the tens or even hundreds of studies on a particular problem? How can conflicting findings be reconciled? Richard Light and David Pillemer have developed both general guidelines and step-by-step procedures that can be used to synthesize existing data. They show how to apply quantitative methods, including the newest statistical procedures and simple graphical displays, to evaluate a mass of studies and combine separate data sets. At the same time, they insist on the value of qualitative information, of asking the right questions, and of considering the context in which research is conducted. The authors use exemplary reviews in education, psychology, health, and the policy sciences to illustrate their suggestions.Written in nontechnical language and addressed to the beginning researcher as well as to the practicing professional, Summing Up will set a new standard for valid research reviews and is likely to become a methodological classic.},
  isbn = {978-0-674-85431-4},
  langid = {english},
  pagetotal = {212},
  keywords = {Education / General,Social Science / Reference,Social Science / Research}
}

@article{marksanglin2020historical,
  title = {A Historical Review of Publication Bias},
  author = {Marks‐Anglin, Arielle and Chen, Yong},
  date = {2020-11},
  journaltitle = {Research Synthesis Methods},
  shortjournal = {Res Syn Meth},
  volume = {11},
  number = {6},
  pages = {725--742},
  issn = {1759-2879, 1759-2887},
  doi = {10.1002/jrsm.1452},
  url = {https://doi.org/10.1002/jrsm.1452},
  urldate = {2022-03-03},
  langid = {english},
  file = {C:\Users\James\Zotero\storage\23RQ7TX5\Marks‐Anglin and Chen - 2020 - A historical review of publication bias.pdf}
}

@article{zheng2022How,
  title = {How Consistently Do 13 Clearinghouses Identify Social and Behavioral Development Programs as “{{Evidence-Based}}”?},
  author = {Zheng, Jingwen and Wadhwa, Mansi and Cook, Thomas D.},
  date = {2022-11},
  journaltitle = {Prevention Science},
  shortjournal = {Prev Sci},
  volume = {23},
  number = {8},
  pages = {1343--1358},
  issn = {1389-4986, 1573-6695},
  doi = {10.1007/s11121-022-01407-y},
  url = {https://doi.org/10.1007/s11121-022-01407-y},
  urldate = {2024-03-06},
  langid = {english}
}


@incollection{Rothstein2005publication,
  title = {Publication Bias in Meta-Analysis},
  booktitle = {Publication {{Bias}} in {{Meta-Analysis}}: {{Prevention}}, {{Assessment}}, and {{Adjustments}}},
  author = {Rothstein, Hannah R and Sutton, Alexander J and Borenstein, Michael},
  editor = {Rothstein, Hannah R and Sutton, Alex J and Borenstein, Michael},
  date = {2005},
  pages = {1--7},
  publisher = {{John Wiley \& Sons}},
  location = {{West Sussex, England}},
  doi = {10.1002/0470870168},
  chapter = {1}
}


@article{ioannidis_appropriateness_2007,
	title = {The appropriateness of asymmetry tests for publication bias in meta-analyses: a large survey},
	volume = {176},
	issn = {0820-3946, 1488-2329},
	shorttitle = {The appropriateness of asymmetry tests for publication bias in meta-analyses},
	url = {http://www.cmaj.ca/cgi/doi/10.1503/cmaj.060410},
	doi = {10.1503/cmaj.060410},
	abstract = {Background: Statistical tests for funnel-plot asymmetry are common in meta-analyses. Inappropriate application can generate misleading inferences about publication bias. We aimed to measure, in a survey of meta-analyses, how frequently the application of these tests would be not meaningful or inappropriate.},
	language = {en},
	number = {8},
	urldate = {2019-05-28},
	journal = {Canadian Medical Association Journal},
	author = {Ioannidis, J. P.A. and Trikalinos, T. A.},
	month = apr,
	year = {2007},
	pages = {1091--1096},
	file = {Ioannidis & Trikalinos (2007) asymmetry tests for publication bias.pdf:/Users/mjoshi/Zotero/storage/6PTZP8IQ/Ioannidis & Trikalinos (2007) asymmetry tests for publication bias.pdf:application/pdf},
}

@article{ioannidis_authors_2007,
	title = {Authors' response to {V} {Johnson} and {Y} {Yuan}},
	volume = {4},
	issn = {1740-7745, 1740-7753},
	url = {http://journals.sagepub.com/doi/10.1177/1740774507079433},
	doi = {10.1177/1740774507079433},
	language = {en},
	number = {3},
	urldate = {2019-05-28},
	journal = {Clinical Trials: Journal of the Society for Clinical Trials},
	author = {Ioannidis, John PA and Trikalinos, Thomas A},
	month = jun,
	year = {2007},
	pages = {256--257},
	file = {Ioannidis & Trikalinos (2007) response.pdf:/Users/mjoshi/Zotero/storage/PA5LSAFM/Ioannidis & Trikalinos (2007) response.pdf:application/pdf},
}

@article{ioannidis_exploratory_2007,
	title = {An exploratory test for an excess of significant findings},
	volume = {4},
	issn = {1740-7745, 1740-7753},
	url = {http://journals.sagepub.com/doi/10.1177/1740774507079441},
	doi = {10.1177/1740774507079441},
	abstract = {Background The published clinical research literature may be distorted by the pursuit of statistically significant results.
Purpose We aimed to develop a test to explore biases stemming from the pursuit of nominal statistical significance.
Methods The exploratory test evaluates whether there is a relative excess of formally significant findings in the published literature due to any reason (e.g., publication bias, selective analyses and outcome reporting, or fabricated data). The number of expected studies with statistically significant results is estimated and compared against the number of observed significant studies. The main application uses ␣ ϭ 0.05, but a range of ␣ thresholds is also examined. Different values or prior distributions of the effect size are assumed. Given the typically low power (few studies per research question), the test may be best applied across domains of many meta-analyses that share common characteristics (interventions, outcomes, study populations, research environment).
Results We evaluated illustratively eight meta-analyses of clinical trials with Ͼ50 studies each and 10 meta-analyses of clinical efficacy for neuroleptic agents in schizophrenia; the 10 meta-analyses were also examined as a composite domain. Different results were obtained against commonly used tests of publication bias. We demonstrated a clear or possible excess of significant studies in 6 of 8 large metaanalyses and in the wide domain of neuroleptic treatments. Limitations The proposed test is exploratory, may depend on prior assumptions, and should be applied cautiously.
Conclusions An excess of significant findings may be documented in some clinical research fields. Clinical Trials 2007; 4: 245–253; http://ctj.sagepub.com},
	language = {en},
	number = {3},
	urldate = {2019-05-28},
	journal = {Clinical Trials: Journal of the Society for Clinical Trials},
	author = {Ioannidis, John PA and Trikalinos, Thomas A},
	month = jun,
	year = {2007},
	pages = {245--253},
	file = {Ioannidis & Trikalinos (2007).pdf:/Users/mjoshi/Zotero/storage/89VBTCBQ/Ioannidis & Trikalinos (2007).pdf:application/pdf},
}

@article{ioannidis_clarifications_2013,
	title = {Clarifications on the application and interpretation of the test for excess significance and its extensions},
	volume = {57},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249613000278},
	doi = {10.1016/j.jmp.2013.03.002},
	abstract = {This commentary discusses challenges in the application of the test for excess significance (Ioannidis \& Trikalinos, 2007) including the definition of the body of evidence, the plausible effect size for power calculations and the threshold of statistical significance. Interpretation should be cautious, given that it is not possible to separate different mechanisms of bias (classic publication bias, selective analysis, and fabrication) that lead to an excess of significance and in some fields significance-related biases may follow a complex pattern (e.g. Proteus phenomenon and occasional preference for ‘‘negative’’ results). Likelihood ratio estimates can be used to generate the post-test probability of bias, and correcting effect estimates for bias is possible in theory, but may not necessarily be reliable.},
	language = {en},
	number = {5},
	urldate = {2019-05-28},
	journal = {Journal of Mathematical Psychology},
	author = {Ioannidis, John P.A.},
	month = oct,
	year = {2013},
	pages = {184--187},
	file = {Ioannidis (2013).pdf:/Users/mjoshi/Zotero/storage/ZBHTNC4Y/Ioannidis (2013).pdf:application/pdf},
}

@article{johnson_comments_2007,
	title = {Comments on `{An} exploratory test for an excess of significant findings' by {JPA} loannidis and {TA} {Trikalinos}},
	volume = {4},
	issn = {1740-7745, 1740-7753},
	url = {http://journals.sagepub.com/doi/10.1177/1740774507079437},
	doi = {10.1177/1740774507079437},
	language = {en},
	number = {3},
	urldate = {2019-05-28},
	journal = {Clinical Trials: Journal of the Society for Clinical Trials},
	author = {Johnson, Valen and Yuan, Ying},
	month = jun,
	year = {2007},
	pages = {254--255},
	file = {Johnson & Yuan (2007).pdf:/Users/mjoshi/Zotero/storage/NCAMBCXI/Johnson & Yuan (2007).pdf:application/pdf},
}

@article{mavridis_selection_2014,
	title = {A selection model for accounting for publication bias in a full network meta-analysis},
	volume = {33},
	issn = {02776715},
	url = {http://doi.wiley.com/10.1002/sim.6321},
	doi = {10.1002/sim.6321},
	language = {en},
	number = {30},
	urldate = {2019-05-28},
	journal = {Statistics in Medicine},
	author = {Mavridis, Dimitris and Welton, Nicky J. and Sutton, Alex and Salanti, Georgia},
	month = dec,
	year = {2014},
	pages = {5399--5412},
	file = {Mavridis_et_al-2014-Statistics_in_Medicine.pdf:/Users/mjoshi/Zotero/storage/7AV6MXPU/Mavridis_et_al-2014-Statistics_in_Medicine.pdf:application/pdf},
}

@article{morey_consistency_2013,
	title = {The consistency test does not–and cannot–deliver what is advertised: {A} comment on {Francis} (2013)},
	volume = {57},
	issn = {00222496},
	shorttitle = {The consistency test does not–and cannot–deliver what is advertised},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249613000291},
	doi = {10.1016/j.jmp.2013.03.004},
	abstract = {The statistical consistency test of Ioannidis and Trikalinos (2007) has been used recently by Francis (2012a,c,d,e,2013,in press), to argue that specific sets of experiments show evidence of publication bias. I argue that the test is unnecessary because publication bias exists almost everywhere as property of the research process, not individual studies. Furthermore, for several reasons, the test does not support the claims made on its behalf. Instead of focusing on testing sets of experiments for publication bias, we should focus on changes to scientific culture to reduce the bias.},
	language = {en},
	number = {5},
	urldate = {2019-05-28},
	journal = {Journal of Mathematical Psychology},
	author = {Morey, Richard D.},
	month = oct,
	year = {2013},
	pages = {180--183},
	file = {Morey (2013).pdf:/Users/mjoshi/Zotero/storage/3ZHH8YE7/Morey (2013).pdf:application/pdf},
}

@article{mueller_methods_2016,
	title = {Methods for detecting, quantifying, and adjusting for dissemination bias in meta-analysis are described},
	volume = {80},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435616303018},
	doi = {10.1016/j.jclinepi.2016.04.015},
	abstract = {Objective: To systematically review methodological articles which focus on nonpublication of studies and to describe methods of detecting and/or quantifying and/or adjusting for dissemination in meta-analyses. To evaluate whether the methods have been applied to an empirical data set for which one can be reasonably conﬁdent that all studies conducted have been included. Study Design and Setting: We systematically searched Medline, the Cochrane Library, and Web of Science, for methodological articles that describe at least one method of detecting and/or quantifying and/or adjusting for dissemination bias in meta-analyses.
Results: The literature search retrieved 2,224 records, of which we ﬁnally included 150 full-text articles. A great variety of methods to detect, quantify, or adjust for dissemination bias were described. Methods included graphical methods mainly based on funnel plot approaches, statistical methods, such as regression tests, selection models, sensitivity analyses, and a great number of more recent statistical approaches. Only few methods have been validated in empirical evaluations using unpublished studies obtained from regulators (Food and Drug Administration, European Medicines Agency).
Conclusion: We present an overview of existing methods to detect, quantify, or adjust for dissemination bias. It remains difﬁcult to advise which method should be used as they are all limited and their validity has rarely been assessed. Therefore, a thorough literature search remains crucial in systematic reviews, and further steps to increase the availability of all research results need to be taken. Ó 2016 Elsevier Inc. All rights reserved.},
	language = {en},
	urldate = {2019-05-28},
	journal = {Journal of Clinical Epidemiology},
	author = {Mueller, Katharina Felicitas and Meerpohl, Joerg J. and Briel, Matthias and Antes, Gerd and von Elm, Erik and Lang, Britta and Motschall, Edith and Schwarzer, Guido and Bassler, Dirk},
	month = dec,
	year = {2016},
	pages = {25--33},
	file = {Mueller et al. (2019).pdf:/Users/mjoshi/Zotero/storage/23EFM7BM/Mueller et al. (2019).pdf:application/pdf},
}

@article{pfeiffer_quantifying_2011,
	title = {Quantifying {Selective} {Reporting} and the {Proteus} {Phenomenon} for {Multiple} {Datasets} with {Similar} {Bias}},
	volume = {6},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0018362},
	doi = {10.1371/journal.pone.0018362},
	abstract = {Meta-analyses play an important role in synthesizing evidence from diverse studies and datasets that address similar questions. A major obstacle for meta-analyses arises from biases in reporting. In particular, it is speculated that findings which do not achieve formal statistical significance are less likely reported than statistically significant findings. Moreover, the patterns of bias can be complex and may also depend on the timing of the research results and their relationship with previously published work. In this paper, we present an approach that is specifically designed to analyze large-scale datasets on published results. Such datasets are currently emerging in diverse research fields, particularly in molecular medicine. We use our approach to investigate a dataset on Alzheimer’s disease (AD) that covers 1167 results from case-control studies on 102 genetic markers. We observe that initial studies on a genetic marker tend to be substantially more biased than subsequent replications. The chances for initial, statistically non-significant results to be published are estimated to be about 44\% (95\% CI, 32\% to 63\%) relative to statistically significant results, while statistically non-significant replications have almost the same chance to be published as statistically significant replications (84\%; 95\% CI, 66\% to 107\%). Early replications tend to be biased against initial findings, an observation previously termed Proteus phenomenon: The chances for nonsignificant studies going in the same direction as the initial result are estimated to be lower than the chances for nonsignificant studies opposing the initial result (73\%; 95\% CI, 55\% to 96\%). Such dynamic patters in bias are difficult to capture by conventional methods, where typically simple publication bias is assumed to operate. Our approach captures and corrects for complex dynamic patterns of bias, and thereby helps generating conclusions from published results that are more robust against the presence of different coexisting types of selective reporting.},
	language = {en},
	number = {3},
	urldate = {2019-05-28},
	journal = {PLoS ONE},
	author = {Pfeiffer, Thomas and Bertram, Lars and Ioannidis, John P. A.},
	editor = {Biondi-Zoccai, Giuseppe},
	month = mar,
	year = {2011},
	pages = {e18362},
	file = {Pfeiffer, Bertram, & Ioannidis (2011).PDF:/Users/mjoshi/Zotero/storage/9N832EBZ/Pfeiffer, Bertram, & Ioannidis (2011).PDF:application/pdf},
}

@article{trinquart_test_2014,
	title = {A test for reporting bias in trial networks: simulation and case studies},
	volume = {14},
	issn = {1471-2288},
	shorttitle = {A test for reporting bias in trial networks},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-112},
	doi = {10.1186/1471-2288-14-112},
	abstract = {Background: Networks of trials assessing several treatment options available for the same condition are increasingly considered. Randomized trial evidence may be missing because of reporting bias. We propose a test for reporting bias in trial networks.
Methods: We test whether there is an excess of trials with statistically significant results across a network of trials. The observed number of trials with nominally statistically significant p-values across the network is compared with the expected number. The performance of the test (type I error rate and power) was assessed using simulation studies under different scenarios of selective reporting bias. Examples are provided for networks of antidepressant and antipsychotic trials, where reporting biases have been previously demonstrated by comparing published to Food and Drug Administration (FDA) data.
Results: In simulations, the test maintained the type I error rate and was moderately powerful after adjustment for type I error rate, except when the between-trial variance was substantial. In all, a positive test result increased moderately or markedly the probability of reporting bias being present, while a negative test result was not very informative. In the two examples, the test gave a signal for an excess of statistically significant results in the network of published data but not in the network of FDA data.
Conclusion: The test could be useful to document an excess of significant findings in trial networks, providing a signal for potential publication bias or other selective analysis and outcome reporting biases.},
	language = {en},
	number = {1},
	urldate = {2019-05-28},
	journal = {BMC Medical Research Methodology},
	author = {Trinquart, Ludovic and Ioannidis, John PA and Chatellier, Gilles and Ravaud, Philippe},
	month = dec,
	year = {2014},
	file = {Trinquart et al. (2014).pdf:/Users/mjoshi/Zotero/storage/C9FJED2S/Trinquart et al. (2014).pdf:application/pdf},
}

@article{van_aert_conducting_2016,
	title = {Conducting {Meta}-{Analyses} {Based} on \textit{p} {Values}: {Reservations} and {Recommendations} for {Applying} \textit{p} -{Uniform} and \textit{p} -{Curve}},
	volume = {11},
	issn = {1745-6916, 1745-6924},
	shorttitle = {Conducting {Meta}-{Analyses} {Based} on \textit{p} {Values}},
	url = {http://journals.sagepub.com/doi/10.1177/1745691616650874},
	doi = {10.1177/1745691616650874},
	abstract = {Because of overwhelming evidence of publication bias in psychology, techniques to correct meta-analytic estimates for such bias are greatly needed. The methodology on which the p-uniform and p-curve methods are based has great promise for providing accurate meta-analytic estimates in the presence of publication bias. However, in this article, we show that in some situations, p-curve behaves erratically, whereas p-uniform may yield implausible estimates of negative effect size. Moreover, we show that (and explain why) p-curve and p-uniform result in overestimation of effect size under moderate-to-large heterogeneity and may yield unpredictable bias when researchers employ­ p-hacking. We offer hands-on recommendations on applying and interpreting results of meta-analyses in general and p-uniform and p-curve in particular. Both methods as well as traditional methods are applied to a meta-analysis on the effect of weight on judgments of importance. We offer guidance for applying p-uniform or p-curve using R and a user-friendly web application for applying p-uniform.},
	language = {en},
	number = {5},
	urldate = {2019-05-28},
	journal = {Perspectives on Psychological Science},
	author = {van Aert, Robbie C. M. and Wicherts, Jelte M. and van Assen, Marcel A. L. M.},
	month = sep,
	year = {2016},
	pages = {713--729},
	file = {van Aert, Wicherts, & van Assen (2016).pdf:/Users/mjoshi/Zotero/storage/6CSJDAHR/van Aert, Wicherts, & van Assen (2016).pdf:application/pdf},
}

@article{brunner_estimating_2018,
	title = {Estimating {Population} {Mean} {Power} {Under} {Conditions} of {Heterogeneity} and {Selection} for {Signiﬁcance}},
	abstract = {In scientiﬁc ﬁelds that depend on signiﬁcance tests to document their ﬁndings, statistical power is a necessary condition for replicability. For any population of published results, there is a population of power values of the statistical tests on which conclusions are based. We give exact theoretical results showing how suppression of non-signiﬁcant results (publication bias) aﬀects the distribution of statistical power in a heterogeneous population of signiﬁcance tests. In a set of large-scale simulation studies, we compare four methods for estimating population mean power, based only on signiﬁcant results. The methods are maximum likelihood, extensions of of p-curve and p-uniform, and a new method we call z-curve. The versions of p-uniform and pcurve we consider perform well when eﬀect size is a single ﬁxed value, and under heterogeneity in sample size. When there is substantial variability in eﬀect size as well as sample size, both methods fail. If the assumptions of maximum likelihood are satisﬁed, it is the most accurate method of estimation under most conditions. When the assumptions of maximum likelihood are incorrect, z-curve is better. We describe and validate a conservative bootstrap conﬁdence interval that makes it possible to use z-curve with smaller samples of studies.},
	language = {en},
	author = {Brunner, Jerry and Schimmack, Ulrich},
	year = {2018},
	pages = {25},
	file = {Brunner & Schimmack (2018).pdf:/Users/mjoshi/Zotero/storage/RTLISDUL/Brunner & Schimmack (2018).pdf:application/pdf},
}

@article{francis_replication_2013,
	title = {Replication, statistical consistency, and publication bias},
	volume = {57},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002224961300014X},
	doi = {10.1016/j.jmp.2013.02.003},
	abstract = {Scientific methods of investigation offer systematic ways to gather information about the world; and in the field of psychology application of such methods should lead to a better understanding of human behavior. Instead, recent reports in psychological science have used apparently scientific methods to report strong evidence for unbelievable claims such as precognition. To try to resolve the apparent conflict between unbelievable claims and the scientific method many researchers turn to empirical replication to reveal the truth. Such an approach relies on the belief that true phenomena can be successfully demonstrated in well-designed experiments, and the ability to reliably reproduce an experimental outcome is widely considered the gold standard of scientific investigations. Unfortunately, this view is incorrect; and misunderstandings about replication contribute to the conflicts in psychological science. Because experimental effects in psychology are measured by statistics, there should almost always be some variability in the reported outcomes. An absence of such variability actually indicates that experimental replications are invalid, perhaps because of a bias to suppress contrary findings or because the experiments were run improperly. Recent investigations have demonstrated how to identify evidence of such invalid experiment sets and noted its appearance for prominent findings in experimental psychology. The present manuscript explores those investigative methods by using computer simulations to demonstrate their properties and limitations. The methods are shown to be a check on the statistical consistency of a set of experiments by comparing the reported power of the experiments with the reported frequency of statistical significance. Overall, the methods are extremely conservative about reporting inconsistency when experiments are run properly and reported fully. The manuscript also considers how to improve scientific practice to avoid inconsistency, and discusses criticisms of the investigative method.},
	language = {en},
	number = {5},
	urldate = {2019-05-28},
	journal = {Journal of Mathematical Psychology},
	author = {Francis, Gregory},
	month = oct,
	year = {2013},
	pages = {153--169},
	file = {Francis (2013).pdf:/Users/mjoshi/Zotero/storage/J4GYGKM9/Francis (2013).pdf:application/pdf},
}

@article{van_aert_publication_2019,
	title = {Publication bias examined in meta-analyses from psychology and medicine: {A} meta-meta-analysis},
	volume = {14},
	issn = {1932-6203},
	shorttitle = {Publication bias examined in meta-analyses from psychology and medicine},
	url = {http://dx.plos.org/10.1371/journal.pone.0215052},
	doi = {10.1371/journal.pone.0215052},
	abstract = {Publication bias is a substantial problem for the credibility of research in general and of meta-analyses in particular, as it yields overestimated effects and may suggest the existence of non-existing effects. Although there is consensus that publication bias exists, how strongly it affects different scientific literatures is currently less well-known. We examined evidence of publication bias in a large-scale data set of primary studies that were included in 83 meta-analyses published in Psychological Bulletin (representing meta-analyses from psychology) and 499 systematic reviews from the Cochrane Database of Systematic Reviews (CDSR; representing meta-analyses from medicine). Publication bias was assessed on all homogeneous subsets (3.8\% of all subsets of meta-analyses published in Psychological Bulletin) of primary studies included in meta-analyses, because publication bias methods do not have good statistical properties if the true effect size is heterogeneous. Publication bias tests did not reveal evidence for bias in the homogeneous subsets. Overestimation was minimal but statistically significant, providing evidence of publication bias that appeared to be similar in both fields. However, a Monte-Carlo simulation study revealed that the creation of homogeneous subsets resulted in challenging conditions for publication bias methods since the number of effect sizes in a subset was rather small (median number of effect sizes equaled 6). Our findings are in line with, in its most extreme case, publication bias ranging from no bias until only 5\% statistically nonsignificant effect sizes being published. These and other findings, in combination with the small percentages of statistically significant primary effect sizes (28.9\% and 18.9\% for subsets published in Psychological Bulletin and CDSR), led to the conclusion that evidence for publication bias in the studied homogeneous subsets is weak, but suggestive of mild publication bias in both psychology and medicine.},
	language = {en},
	number = {4},
	urldate = {2019-05-28},
	journal = {PLOS ONE},
	author = {van Aert, Robbie C. M. and Wicherts, Jelte M. and van Assen, Marcel A. L. M.},
	editor = {Macleod, Malcolm R.},
	month = apr,
	year = {2019},
	pages = {e0215052},
	file = {van Aert et al. (2019).pdf:/Users/mjoshi/Zotero/storage/7NURHK2I/van Aert et al. (2019).pdf:application/pdf},
}

@article{van_assen_meta-analysis_2015,
	title = {Meta-analysis using effect size distributions of only statistically significant studies.},
	volume = {20},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000025},
	doi = {10.1037/met0000025},
	language = {en},
	number = {3},
	urldate = {2019-05-28},
	journal = {Psychological Methods},
	author = {van Assen, Marcel A. L. M. and van Aert, Robbie C. M. and Wicherts, Jelte M.},
	year = {2015},
	pages = {293--309},
	file = {van Assen et al. (2015).pdf:/Users/mjoshi/Zotero/storage/QQUGKW5I/van Assen et al. (2015).pdf:application/pdf},
}

@misc{brunner_how_2016,
	title = {How replicable is psychology? {A} comparison of four methods of estimating replicability on the basis of test statistics in original studies},
	author = {Brunner, Jerry and Schimmack, Ulrich},
	year = {2016},
	file = {Brunner & Schimmack (2016):/Users/mjoshi/Zotero/storage/NUDDZI4X/Brunner & Schimmack (2016).pdf:application/pdf},
}

@article{hedges_estimating_1996,
	title = {Estimating {Effect} {Size} under {Publication} {Bias}: {Small} {Sample} {Properties} and {Robustness} of a {Random} {Effects} {Selection} {Model}},
	volume = {21},
	issn = {10769986},
	shorttitle = {Estimating {Effect} {Size} under {Publication} {Bias}},
	url = {http://links.jstor.org/sici?sici=1076-9986%28199624%2921%3A4%3C299%3AEESUPB%3E2.0.CO%3B2-V&origin=crossref},
	doi = {10.2307/1165338},
	language = {en},
	number = {4},
	urldate = {2019-05-28},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Hedges, Larry V. and Vevea, Jack L.},
	year = {1996},
	pages = {299},
	file = {Hedges & Vevea (1996).pdf:/Users/mjoshi/Zotero/storage/UYMTEG8Y/Hedges & Vevea (1996).pdf:application/pdf},
}

@article{hedges_modeling_1992,
	title = {Modeling {Publication} {Selection} {Effects} in {Meta}-{Analysis}},
	volume = {7},
	issn = {0883-4237},
	url = {http://projecteuclid.org/euclid.ss/1177011364},
	doi = {10.1214/ss/1177011364},
	abstract = {Publication selection effects arise in meta-analysis when the effect magnitude estimates are observed in (available from) only a subset of the studies that were actually conducted and the probability that an estimate is observed is related to the size of that estimate. Such selection effects can lead to substantial bias in estimates of effect magnitude. Research on the selection process suggests that much of the selection occurs because researchers, reviewers and editors view the results of studies as more conclusive when they are more highly statistically significant. This suggests a model of the selection process that depends on effect magnitude via the p-value or significance level. A model of the selection process involving a step function relating the p-value to the probability of selection is introduced in the context of a random effects model for meta-analysis. The model permits estimation of a weight function representing selection along the mean and variance of effects. Some ideas for graphical procedures and a test for publication selection are also introduced. The method is then applied to a meta-analysis of test validity studies.},
	language = {en},
	number = {2},
	urldate = {2019-05-28},
	journal = {Statistical Science},
	author = {Hedges, Larry V.},
	month = may,
	year = {1992},
	pages = {246--255},
	file = {Hedges (1992).pdf:/Users/mjoshi/Zotero/storage/HG6FQ5NC/Hedges (1992).pdf:application/pdf},
}

@article{hedges_plausibility_2017,
	title = {Plausibility and influence in selection models: {A} comment on {Citkowicz} and {Vevea} (2017).},
	volume = {22},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Plausibility and influence in selection models},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000108},
	doi = {10.1037/met0000108},
	language = {en},
	number = {1},
	urldate = {2019-05-28},
	journal = {Psychological Methods},
	author = {Hedges, Larry V.},
	year = {2017},
	pages = {42--46},
	file = {Hedges (2017).pdf:/Users/mjoshi/Zotero/storage/WCMJJDU9/Hedges (2017).pdf:application/pdf},
}

@article{vevea_general_1995,
	title = {A general linear model for estimating effect size in the presence of publication bias},
	volume = {60},
	issn = {0033-3123, 1860-0980},
	url = {http://link.springer.com/10.1007/BF02294384},
	doi = {10.1007/BF02294384},
	language = {en},
	number = {3},
	urldate = {2019-05-28},
	journal = {Psychometrika},
	author = {Vevea, Jack L. and Hedges, Larry V.},
	month = sep,
	year = {1995},
	pages = {419--435},
	file = {Vevea, Hedges - 1995 - A general linear model for estimating effect size in the presence of publication bias.pdf:/Users/mjoshi/Zotero/storage/RXHE8MJA/Vevea, Hedges - 1995 - A general linear model for estimating effect size in the presence of publication bias.pdf:application/pdf},
}

@article{dear_approach_1992,
	title = {An {Approach} for {Assessing} {Publication} {Bias} {Prior} to {Performing} a {Meta}-{Analysis}},
	volume = {7},
	url = {http://www.jstor.org/stable/2246310},
	abstract = {A semi-parametrmicethodis developedforassessingpublicationbias priorto performinag meta-analysisS.ummaryestimatesfor theindividualstudiesin themeta-analysiasre assumedto have known distributionaflormS. electivepublicationis modeledusinga nonparametricweightfunctiond,efinedon thetwo-sidedp-valuescale.The shapeof theestimatedweightfunctionprovidesvisual evidenceofthepresence of bias, if it exists, and observedtrendsmay be tested using rank orderstatisticsor likelihoodratiotests. The methodis intendedas an exploratorytechniquepriorto embarkingon a standardmeta-analysis.},
	language = {en},
	number = {2},
	journal = {Statistical Science},
	author = {Dear, Keith B. G. and Begg, Colin B.},
	year = {1992},
	pages = {237--245},
	file = {Dear, Begg - 1992 - An approach for assessing publication bias prior to performing a meta-analysis.pdf:/Users/mjoshi/Zotero/storage/X2XFTVFW/Dear, Begg - 1992 - An approach for assessing publication bias prior to performing a meta-analysis.pdf:application/pdf},
}

@article{iyengar_selection_1988,
	title = {Selection {Models} and the {File} {Drawer} {Problem}},
	volume = {3},
	issn = {0883-4237},
	url = {http://projecteuclid.org/euclid.ss/1177013012},
	doi = {10.1214/ss/1177013012},
	abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
	language = {en},
	number = {1},
	urldate = {2019-05-28},
	journal = {Statistical Science},
	author = {Iyengar, Satish and Greenhouse, Joel B.},
	month = feb,
	year = {1988},
	pages = {109--117},
	file = {Iyengar & Greenhouse (1988).pdf:/Users/mjoshi/Zotero/storage/53EGCNLT/Iyengar & Greenhouse (1988).pdf:application/pdf},
}

@article{carter_correcting_2019,
	title = {Correcting for bias in psychology: {A} comparison of meta-analytic methods},
	volume = {2},
	shorttitle = {Correcting for bias in psychology},
	url = {https://journals.sagepub.com/doi/full/10.1177/2515245919847196},
	doi = {10.1177/2515245919847196},
	abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated eﬀects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, much of this work has not been tailored speciﬁcally to psychology, so it is not clear which methods work best for data typically seen in our ﬁeld. Here, we present a comprehensive simulation study to examine how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We created such scenarios by simulating several levels of questionable research practices, publication bias, heterogeneity, and using study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change based on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their eﬀorts on improving the primary literature and conducting large-scale, pre-registered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive ﬁgures at http://www.shinyapps.org/apps/metaExplorer/.},
	language = {en},
	number = {2},
	urldate = {2019-05-28},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Carter, Evan C and Schönbrodt, Felix D. and Gervais, Will M and Hilgard, Joseph},
	month = jun,
	year = {2019},
	file = {Carter et al. (2018) Correcting-bias-in-psychology_preprint.pdf:/Users/mjoshi/Zotero/storage/BX7V6IMJ/Carter et al. (2018) Correcting-bias-in-psychology_preprint.pdf:application/pdf;Carter et al. (2019) correcting for bias in psychology.pdf:/Users/mjoshi/Zotero/storage/DJE39XEL/Carter et al. (2019) correcting for bias in psychology.pdf:application/pdf},
}

@article{mcshane_adjusting_2016,
	title = {Adjusting for {Publication} {Bias} in {Meta}-{Analysis}: {An} {Evaluation} of {Selection} {Methods} and {Some} {Cautionary} {Notes}},
	volume = {11},
	issn = {1745-6916, 1745-6924},
	shorttitle = {Adjusting for {Publication} {Bias} in {Meta}-{Analysis}},
	url = {http://journals.sagepub.com/doi/10.1177/1745691616662243},
	doi = {10.1177/1745691616662243},
	language = {en},
	number = {5},
	urldate = {2019-05-28},
	journal = {Perspectives on Psychological Science},
	author = {McShane, Blakeley B. and Böckenholt, Ulf and Hansen, Karsten T.},
	month = sep,
	year = {2016},
	pages = {730--749},
	file = {McShane, Bockenholt, & Hansen (2016).pdf:/Users/mjoshi/Zotero/storage/HSMP44S5/McShane, Bockenholt, & Hansen (2016).pdf:application/pdf},
}

@incollection{rao_score_2005,
	address = {Boston, MA},
	series = {Statistics for {Industry} and {Technology}},
	title = {Score {Test}: {Historical} {Review} and {Recent} {Developments}},
	isbn = {978-0-8176-4422-2},
	shorttitle = {Score {Test}},
	url = {https://doi.org/10.1007/0-8176-4422-9_1},
	abstract = {The three asymptotic tests, Neyman and Pearson Likelihood Ratio (LR), Wald’s statistic (W) and Rao’s score (RS)are referred to in statistical literature on testing of hypotheses as the Holy Trinity. All these tests are equivalent to the first-order of asymptotics, but differ to some extent in the second-order properties. Some of the merits and defects of these tests are presented.Some applications of the score test, recent developments on refining the score test and problems for further investigation are presented.},
	language = {en},
	urldate = {2019-05-31},
	booktitle = {Advances in {Ranking} and {Selection}, {Multiple} {Comparisons}, and {Reliability}: {Methodology} and {Applications}},
	publisher = {Birkhäuser Boston},
	author = {Rao, C. R.},
	editor = {Balakrishnan, N. and Nagaraja, H. N. and Kannan, N.},
	year = {2005},
	doi = {10.1007/0-8176-4422-9_1},
	keywords = {Composite hypothesis, Lagrangian multiplier (LM) test, Likelihood ratio (LR), Neyman-Rao test, Neyman’s C(α), Rao’s score (RS), Wald’s statistic (W)},
	pages = {3--20},
	file = {Springer Full Text PDF:/Users/mjoshi/Zotero/storage/3ES22WII/Rao - 2005 - Score Test Historical Review and Recent Developme.pdf:application/pdf},
}

@article{henmi_confidence_2010,
	title = {Confidence intervals for random effects meta-analysis and robustness to publication bias},
	volume = {29},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4029},
	doi = {10.1002/sim.4029},
	abstract = {The DerSimonian–Laird confidence interval for the average treatment effect in meta-analysis is widely used in practice when there is heterogeneity between studies. However, it is well known that its coverage probability (the probability that the interval actually includes the true value) can be substantially below the target level of 95 per cent. It can also be very sensitive to publication bias. In this paper, we propose a new confidence interval that has better coverage than the DerSimonian–Laird method, and that is less sensitive to publication bias. The key idea is to note that fixed effects estimates are less sensitive to such biases than random effects estimates, since they put relatively more weight on the larger studies and relatively less weight on the smaller studies. Whereas the DerSimonian–Laird interval is centred on a random effects estimate, we centre our confidence interval on a fixed effects estimate, but allow for heterogeneity by including an assessment of the extra uncertainty induced by the random effects setting. Properties of the resulting confidence interval are studied by simulation and compared with other random effects confidence intervals that have been proposed in the literature. An example is briefly discussed. Copyright © 2010 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {29},
	urldate = {2019-05-31},
	journal = {Statistics in Medicine},
	author = {Henmi, Masayuki and Copas, John B.},
	year = {2010},
	keywords = {DerSimonian–Laird confidence interval, meta-analysis, publication bias, random effects models},
	pages = {2969--2983},
	file = {Full Text PDF:/Users/mjoshi/Zotero/storage/IGYZIQRP/Henmi and Copas - 2010 - Confidence intervals for random effects meta-analy.pdf:application/pdf;Snapshot:/Users/mjoshi/Zotero/storage/EX6UL4TV/sim.html:text/html},
}

@article{boos_generalized_1992,
	title = {On {Generalized} {Score} {Tests}},
	volume = {46},
	issn = {00031305},
	url = {https://www.jstor.org/stable/2685328?origin=crossref},
	doi = {10.2307/2685328},
	language = {en},
	number = {4},
	urldate = {2019-05-29},
	journal = {The American Statistician},
	author = {Boos, Dennis D.},
	month = nov,
	year = {1992},
	pages = {327},
	file = {Boos (1992).pdf:/Users/mjoshi/Zotero/storage/RM8T4ML7/Boos (1992).pdf:application/pdf},
}

@article{rao_large_1948,
	title = {Large sample tests of statistical hypotheses concerning several parameters with applications to problems of estimation},
	volume = {44},
	issn = {0305-0041, 1469-8064},
	url = {http://www.journals.cambridge.org/abstract_S0305004100023987},
	doi = {10.1017/S0305004100023987},
	language = {en},
	number = {01},
	urldate = {2019-05-29},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Rao, C. Radhakrishna},
	month = jan,
	year = {1948},
	pages = {50},
}

@article{de_bruin_cognitive_2015,
	title = {Cognitive {Advantage} in {Bilingualism}: {An} {Example} of {Publication} {Bias}?},
	volume = {26},
	issn = {0956-7976},
	shorttitle = {Cognitive {Advantage} in {Bilingualism}},
	url = {https://doi.org/10.1177/0956797614557866},
	doi = {10.1177/0956797614557866},
	abstract = {It is a widely held belief that bilinguals have an advantage over monolinguals in executive-control tasks, but is this what all studies actually demonstrate? The idea of a bilingual advantage may result from a publication bias favoring studies with positive results over studies with null or negative effects. To test this hypothesis, we looked at conference abstracts from 1999 to 2012 on the topic of bilingualism and executive control. We then determined which of the studies they reported were subsequently published. Studies with results fully supporting the bilingual-advantage theory were most likely to be published, followed by studies with mixed results. Studies challenging the bilingual advantage were published the least. This discrepancy was not due to differences in sample size, tests used, or statistical power. A test for funnel-plot asymmetry provided further evidence for the existence of a publication bias.},
	language = {en},
	number = {1},
	urldate = {2019-06-03},
	journal = {Psychological Science},
	author = {de Bruin, Angela and Treccani, Barbara and Della Sala, Sergio},
	month = jan,
	year = {2015},
	pages = {99--107},
	file = {SAGE PDF Full Text:/Users/mjoshi/Zotero/storage/YC2AMK53/de Bruin et al. - 2015 - Cognitive Advantage in Bilingualism An Example of.pdf:application/pdf},
}

@article{rotnitzky_hypothesis_1990,
	title = {Hypothesis {Testing} of {Regression} {Parameters} in {Semiparametric} {Generalized} {Linear} {Models} for {Cluster} {Correlated} {Data}},
	volume = {77},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2336986?origin=crossref},
	doi = {10.2307/2336986},
	abstract = {Generalized and 'working' Wald and score tests for regression coefficients in the class of semiparametric marginal generalized linear models for cluster correlated data (Liang \& Zeger, 1986) are proposed, and their asymptotic distribution examined. In addition, the asymptotic distribution of the naive likelihood ratio test, or deviance difference, is presented. Following Rao \& Scott (1984), we propose simple adjustments to such 'working' tests. The asymptotic distributions of the 'working' tests allow us to explore theoretical bounds on the ratios of the robust variance of the regression parameter estimators and their naive variance counterparts computed assuming independent observations. In addition, the adequacy of a particular choice of working correlation structure is considered. We illustrate our results with a numerical example.},
	language = {en},
	number = {3},
	urldate = {2019-07-05},
	journal = {Biometrika},
	author = {Rotnitzky, Andrea and Jewell, Nicholas P.},
	month = sep,
	year = {1990},
	pages = {485},
	file = {Rotnitzky, Jewell - 1990 - Hypothesis testing of regression parameters in semiparametric generalized linear models for cluster correlate.pdf:/Users/mjoshi/Zotero/storage/UKS8C2L3/Rotnitzky, Jewell - 1990 - Hypothesis testing of regression parameters in semiparametric generalized linear models for cluster correlate.pdf:application/pdf},
}

@article{fernandez-castilla_detecting_2019,
	title = {Detecting {Selection} {Bias} in {Meta}-{Analyses} with {Multiple} {Outcomes}: {A} {Simulation} {Study}},
	issn = {0022-0973, 1940-0683},
	shorttitle = {Detecting {Selection} {Bias} in {Meta}-{Analyses} with {Multiple} {Outcomes}},
	url = {https://www.tandfonline.com/doi/full/10.1080/00220973.2019.1582470},
	doi = {10.1080/00220973.2019.1582470},
	abstract = {This study explores the performance of classical methods for detecting publication bias—namely, Egger’s regression test, Funnel Plot test, Begg’s Rank Correlation and Trim and Fill method—in meta-analysis of studies that report multiple effects. Publication bias, outcome reporting bias, and a combination of these were generated. Egger’s regression test and the Funnel Plot test were extended to three-level models, and possible cutoffs for the estimator of the Trim and Fill method were explored. Furthermore, we checked whether the combination of results of several methods yielded a better control of Type I error rates. Results show that no method works well across all conditions and that performance depends mainly on the population effect size value and the total variance.},
	language = {en},
	urldate = {2019-08-14},
	journal = {The Journal of Experimental Education},
	author = {Fernández-Castilla, Belén and Declercq, Lies and Jamshidi, Laleh and Beretvas, S. Natasha and Onghena, Patrick and Van den Noortgate, Wim},
	month = apr,
	year = {2019},
	pages = {1--20},
	file = {Fernández-Castilla et al. - 2019 - Detecting Selection Bias in Meta-Analyses with Mul.pdf:/Users/mjoshi/Zotero/storage/9WIBUDGS/Fernández-Castilla et al. - 2019 - Detecting Selection Bias in Meta-Analyses with Mul.pdf:application/pdf},
}

@article{leijten_parenting_2018,
	title = {Parenting behaviors that shape child compliance: {A} multilevel meta-analysis},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {Parenting behaviors that shape child compliance},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0204929},
	doi = {10.1371/journal.pone.0204929},
	abstract = {Background What are the parenting behaviors that shape child compliance? Most research on parent-child interactions relies on correlational research or evaluations of “package deal” interventions that manipulate many aspects of parenting at the same time. Neither approach allows for identifying the specific parenting behaviors that shape child compliance. To overcome this, we systematically reviewed and meta-analyzed available evidence on the effects of experimentally manipulated, discrete parenting behaviors—a niche in parent-child interaction research that contributes unique information on the specific parenting behaviors that shape child behavior. Methods We identified studies by systematically searching databases and through contacting experts. Nineteen studies (75 effect sizes) on four discrete parenting behaviors were included: praise, verbal reprimands, time-out, and ignore. In multilevel models, we tested for each parenting behavior whether it increased child compliance, including both observed and parent-reported measures of child compliance. Results Providing “time-out” for noncompliance robustly increased both observed and parent-reported child compliance (ds = 0.84–1.72; 95\% CI 0.30 to 2.54). The same holds for briefly ignoring the child after non-compliance (ds = 0.36–1.77; 95\% CI 0.04 to 2.90). When observed and parent-reported outcomes were combined, but not when they were examined separately, verbal reprimands also increased child compliance (d = 0.72; 95\% CI 0.26 to 1.19). Praise did not increase child compliance (ds = –0.27–1.19; 95\% CI –2.04 to 1.59). Conclusion Our findings suggest that of the discrete parenting behaviors that are experimentally studied in multiple trials, especially time-out and ignore, and to some extent verbal reprimands, shape child compliance.},
	language = {en},
	number = {10},
	urldate = {2019-09-10},
	journal = {PLOS ONE},
	author = {Leijten, Patty and Gardner, Frances and Melendez-Torres, G. J. and Knerr, Wendy and Overbeek, Geertjan},
	month = oct,
	year = {2018},
	keywords = {Behavior, Child health, Children, Database searching, Metaanalysis, Motivation, Parenting behavior, Verbal behavior},
	pages = {e0204929},
	file = {Full Text PDF:/Users/mjoshi/Zotero/storage/5M8ZY5RP/Leijten et al. - 2018 - Parenting behaviors that shape child compliance A.pdf:application/pdf;Snapshot:/Users/mjoshi/Zotero/storage/A9IX39EV/article.html:text/html},
}

@article{john_measuring_2012,
	title = {Measuring the {Prevalence} of {Questionable} {Research} {Practices} {With} {Incentives} for {Truth} {Telling}},
	volume = {23},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611430953},
	doi = {10.1177/0956797611430953},
	abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
	language = {en},
	number = {5},
	urldate = {2019-08-13},
	journal = {Psychological Science},
	author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
	month = may,
	year = {2012},
	pages = {524--532},
	file = {John et al. - 2012 - Measuring the Prevalence of Questionable Research .pdf:/Users/mjoshi/Zotero/storage/NTR7A9HT/John et al. - 2012 - Measuring the Prevalence of Questionable Research .pdf:application/pdf},
}

@article{chow_published_2018,
	title = {Do published studies yield larger effect sizes than unpublished studies in education and special education? {A} meta-review},
	volume = {30},
	issn = {1040-726X, 1573-336X},
	shorttitle = {Do {Published} {Studies} {Yield} {Larger} {Effect} {Sizes} than {Unpublished} {Studies} in {Education} and {Special} {Education}?},
	url = {http://link.springer.com/10.1007/s10648-018-9437-7},
	doi = {10.1007/s10648-018-9437-7},
	abstract = {Meta-analyses are used to make educational decisions in policy and practice. Publication bias refers to the extent to which published literature is more likely to have statistically significant results and larger sample sizes than studies that do not make it through the publication process. The purpose of the present study is to estimate the extent to which publication bias is present in a broad set of education and special education journals. We reviewed 222 meta-analyses to describe the prevalence of publication bias tests, and further identified 29 that met inclusion criteria for effect size extraction. Descriptive data reveal that 58\% of meta-analyses (n = 128) documented no effort to test for possible publication bias, and analyses of 72 difference statistics revealed that published studies were associated with significantly larger effect sizes than unpublished studies (d = 0.64). Exploratory moderator analyses revealed that effect size metric was a significant predictor of the difference between published and unpublished studies.},
	language = {en},
	number = {3},
	urldate = {2019-09-10},
	journal = {Educational Psychology Review},
	author = {Chow, Jason C. and Ekholm, Eric},
	month = sep,
	year = {2018},
	pages = {727--744},
	file = {Chow & Ekholm (2018).pdf:/Users/mjoshi/Zotero/storage/CH8BBAMR/Chow & Ekholm (2018).pdf:application/pdf},
}

@article{jackson_modelling_2005,
	title = {Modelling reporting bias: the operative mortality rate for ruptured abdominal aortic aneurysm repair},
	volume = {168},
	issn = {1467-985X},
	shorttitle = {Modelling reporting bias},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-985X.2005.00375.x},
	doi = {10.1111/j.1467-985X.2005.00375.x},
	abstract = {Summary. It is perhaps underappreciated that ruptured abdominal aortic aneurysm is a significant cause of mortality in the UK. The only curative treatment is an emergency operation and quantifying the success of this presents many difficulties. In particular, there is empirical evidence of reporting bias, suggesting that studies failing to report operating theatre mortality may be those where death in theatre is more common. We suggest a procedure for correcting for this bias and re-examine a recent meta-analysis of the available data. This casts considerable doubt on some conclusions from naïve analyses that do not take into account the potential bias. Perhaps most importantly, our procedure indicates a modest improvement in operating theatre mortality over the last 50 years, which is a trend that is not evident from the usual naïve analyses.},
	language = {en},
	number = {4},
	urldate = {2019-09-30},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Jackson, Dan and Copas, John and Sutton, Alex J.},
	year = {2005},
	keywords = {Meta-analysis, Meta-regression, Non-ignorable selection, Reporting bias},
	pages = {737--752},
	file = {Snapshot:/Users/mjoshi/Zotero/storage/JC7QWZKK/j.1467-985X.2005.00375.html:text/html},
}

@article{schimmack_ironic_2012,
	title = {The ironic effect of significant results on the credibility of multiple-study articles.},
	volume = {17},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029487},
	doi = {10.1037/a0029487},
	abstract = {Cohen (1962) pointed out the importance of statistical power for psychology as a science, but statistical power of studies has not increased, while the number of studies in a single article has increased. It has been overlooked that multiple studies with modest power have a high probability of producing nonsignificant results because power decreases as a function of the number of statistical tests that are being conducted (Maxwell, 2004). The discrepancy between the expected number of significant results and the actual number of significant results in multiple-study articles undermines the credibility of the reported results, and it is likely that questionable research practices have contributed to the reporting of too many significant results (Sterling, 1959). The problem of low power in multiple-study articles is illustrated using Bem’s (2011) article on extrasensory perception and Gailliot et al.’s (2007) article on glucose and self-regulation. I conclude with several recommendations that can increase the credibility of scientific evidence in psychological journals. One major recommendation is to pay more attention to the power of studies to produce positive results without the help of questionable research practices and to request that authors justify sample sizes with a priori predictions of effect sizes. It is also important to publish replication studies with nonsignificant results if these studies have high power to replicate a published finding.},
	language = {en},
	number = {4},
	urldate = {2019-12-05},
	journal = {Psychological Methods},
	author = {Schimmack, Ulrich},
	year = {2012},
	pages = {551--566},
}

@article{ferguson_publication_2012,
	title = {Publication bias in psychological science: {Prevalence}, methods for identifying and controlling, and implications for the use of meta-analyses.},
	volume = {17},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Publication bias in psychological science},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0024445},
	doi = {10.1037/a0024445},
	language = {en},
	number = {1},
	urldate = {2020-03-10},
	journal = {Psychological Methods},
	author = {Ferguson, Christopher J. and Brannick, Michael T.},
	year = {2012},
	pages = {120--128},
	file = {download.pdf:/Users/mjoshi/Zotero/storage/SLVBGWBT/download.pdf:application/pdf},
}

@misc{noauthor_not_nodate,
	title = {Not {Published} {Is} {Not} {Perished}: {Addressing} {Publication} {Bias} in {Meta}-{Analytic} {Studies} in {Communication} {\textbar} {Human} {Communication} {Research} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/hcr/advance-article/doi/10.1093/hcr/hqz015/5771510},
	urldate = {2020-03-10},
	file = {Not Published Is Not Perished\: Addressing Publication Bias in Meta-Analytic Studies in Communication | Human Communication Research | Oxford Academic:/Users/mjoshi/Zotero/storage/MAVENEAA/5771510.html:text/html},
}

@article{konstantopoulos_fixed_2011,
	title = {Fixed effects and variance components estimation in three-level meta-analysis: {Three}-level meta-analysis},
	volume = {2},
	issn = {17592879},
	shorttitle = {Fixed effects and variance components estimation in three-level meta-analysis},
	url = {http://doi.wiley.com/10.1002/jrsm.35},
	doi = {10.1002/jrsm.35},
	language = {en},
	number = {1},
	urldate = {2019-09-30},
	journal = {Research Synthesis Methods},
	author = {Konstantopoulos, Spyros},
	month = mar,
	year = {2011},
	pages = {61--76},
	file = {Konstantopoulos (2011) 3-level MA.pdf:/Users/mjoshi/Zotero/storage/U8DD23XN/Konstantopoulos (2011) 3-level MA.pdf:application/pdf},
}

@article{citkowicz_parsimonious_2017,
	title = {A parsimonious weight function for modeling publication bias.},
	volume = {22},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000119},
	doi = {10.1037/met0000119},
	abstract = {Quantitative research literature is often biased because studies that fail to find a significant effect (or that demonstrate effects in an undesired or unexpected direction) are less likely to be published. This phenomenon, termed publication bias, can cause problems when researchers attempt to synthesize results using metaanalytic methods. Various techniques exist that attempt to estimate and correct meta-analyses for publication bias. However, there is no single method that can (a) account for continuous moderators by including them within the model, (b) allow for substantial data heterogeneity, (c) produce an adjusted mean effect size, (d) include a formal test for publication bias, and (e) allow for correction when only a small number of effects is included in the analysis. This article describes a method that we believe helps fill that gap. The model uses the beta density as a weight function that represents the selection process and provides adjusted parameter estimates that account for publication bias. Use of the beta density allows us to represent selection using fewer parameters than similar models so that the proposed model is suitable for meta-analyses that include relatively few studies. We explain the model and its rationale, illustrate its use with a real data set, and describe the results of a simulation study that shows the model’s utility.},
	language = {en},
	number = {1},
	urldate = {2022-10-11},
	journal = {Psychological Methods},
	author = {Citkowicz, Martyna and Vevea, Jack L.},
	year = {2017},
	pages = {28--41},
	file = {Citkowicz and Vevea - 2017 - A parsimonious weight function for modeling public.pdf:/Users/mjoshi/Zotero/storage/P5GCNLVM/Citkowicz and Vevea - 2017 - A parsimonious weight function for modeling public.pdf:application/pdf},
}

@article{viechtbauer_locationscale_2022,
	title = {Location‐scale models for meta‐analysis},
	issn = {1759-2879, 1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1562},
	doi = {10.1002/jrsm.1562},
	language = {en},
	urldate = {2022-10-11},
	journal = {Research Synthesis Methods},
	author = {Viechtbauer, Wolfgang and López‐López, José Antonio},
	month = apr,
	year = {2022},
	pages = {jrsm.1562},
	file = {Full Text:/Users/mjoshi/Zotero/storage/V8UGNBDG/Viechtbauer and López‐López - 2022 - Location‐scale models for meta‐analysis.pdf:application/pdf},
}

@article{mathur_sensitivity_2020,
	title = {Sensitivity analysis for publication bias in meta‐analyses},
	volume = {69},
	issn = {0035-9254, 1467-9876},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssc.12440},
	doi = {10.1111/rssc.12440},
	language = {en},
	number = {5},
	urldate = {2022-10-13},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Mathur, Maya B. and VanderWeele, Tyler J.},
	month = nov,
	year = {2020},
	pages = {1091--1119},
	file = {Full Text:/Users/mjoshi/Zotero/storage/HT2R2TI4/Mathur and VanderWeele - 2020 - Sensitivity analysis for publication bias in meta‐.pdf:application/pdf},
}

@article{coburn_publication_2015,
	title = {Publication bias as a function of study characteristics.},
	volume = {20},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000046},
	doi = {10.1037/met0000046},
	abstract = {Researchers frequently conceptualize publication bias as a bias against publishing nonsignificant results. However, other factors beyond significance levels can contribute to publication bias. Some of these factors include study characteristics, such as the source of funding for the research project, whether the project was single center or multicenter, and prevailing theories at the time of publication. This article examines the relationship between publication bias and 2 study characteristics by breaking down 2 meta-analytic data sets into levels of the relevant study characteristic and assessing publication bias in each level with funnel plots, trim and fill (Duval \& Tweedie, 2000a, 2000b), Egger’s linear regression (Egger, Smith, Schneider, \& Minder, 1997), cumulative meta-analysis (Borenstein, Hedges, Higgins, \& Rothstein, 2009), and the Vevea and Hedges (1995) weight-function model. Using the Vevea and Hedges model, we conducted likelihood ratio tests to determine whether information was lost if only 1 pattern of selection was estimated. Results indicate that publication bias can differ over levels of study characteristics, and that developing a model to accommodate this relationship could be advantageous.},
	language = {en},
	number = {3},
	urldate = {2022-10-11},
	journal = {Psychological Methods},
	author = {Coburn, Kathleen M. and Vevea, Jack L.},
	year = {2015},
	pages = {310--330},
	file = {Coburn and Vevea - 2015 - Publication bias as a function of study characteri.pdf:/Users/mjoshi/Zotero/storage/CTE79RT4/Coburn and Vevea - 2015 - Publication bias as a function of study characteri.pdf:application/pdf},
}

@incollection{rothstein_selection_2006,
	address = {Chichester, UK},
	title = {Selection {Method} {Approaches}},
	isbn = {978-0-470-87016-7 978-0-470-87014-3},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0470870168.ch9},
	urldate = {2022-10-11},
	booktitle = {Publication {Bias} in {Meta}-{Analysis}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Hedges, Larry V. and Vevea, Jack},
	editor = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
	month = mar,
	year = {2006},
	doi = {10.1002/0470870168.ch9},
	pages = {145--174},
}

@article{vevea_publication_2005,
	title = {Publication {Bias} in {Research} {Synthesis}: {Sensitivity} {Analysis} {Using} {A} {Priori} {Weight} {Functions}.},
	volume = {10},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Publication {Bias} in {Research} {Synthesis}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.10.4.428},
	doi = {10.1037/1082-989X.10.4.428},
	language = {en},
	number = {4},
	urldate = {2022-10-13},
	journal = {Psychological Methods},
	author = {Vevea, Jack L. and Woods, Carol M.},
	month = dec,
	year = {2005},
	pages = {428--443},
}

@article{sterne_recommendations_2011,
	title = {Recommendations for examining and interpreting funnel plot asymmetry in meta-analyses of randomised controlled trials},
	volume = {343},
	copyright = {© BMJ Publishing Group Ltd 2011},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/343/bmj.d4002},
	doi = {10.1136/bmj.d4002},
	abstract = {{\textless}p{\textgreater}Funnel plots, and tests for funnel plot asymmetry, have been widely used to examine bias in the results of meta-analyses. Funnel plot asymmetry should not be equated with publication bias, because it has a number of other possible causes. This article describes how to interpret funnel plot asymmetry, recommends appropriate tests, and explains the implications for choice of meta-analysis model{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2023-11-02},
	journal = {BMJ},
	author = {Sterne, Jonathan A. C. and Sutton, Alex J. and Ioannidis, John P. A. and Terrin, Norma and Jones, David R. and Lau, Joseph and Carpenter, James and Rücker, Gerta and Harbord, Roger M. and Schmid, Christopher H. and Tetzlaff, Jennifer and Deeks, Jonathan J. and Peters, Jaime and Macaskill, Petra and Schwarzer, Guido and Duval, Sue and Altman, Douglas G. and Moher, David and Higgins, Julian P. T.},
	month = jul,
	year = {2011},
	pmid = {21784880},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	pages = {d4002},
	file = {Full Text PDF:/Users/mjoshi/Zotero/storage/WC7RQG56/Sterne et al. - 2011 - Recommendations for examining and interpreting fun.pdf:application/pdf},
}

@article{egger_bias_1997,
	title = {Bias in meta-analysis detected by a simple, graphical test},
	volume = {315},
	copyright = {© 1997 BMJ Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/315/7109/629},
	doi = {10.1136/bmj.315.7109.629},
	abstract = {Objective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses.
Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30\% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews.
Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision.
Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38\%) journal meta-analyses and 5 (13\%) Cochrane reviews, funnel plot asymmetry indicated that there was bias.
Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution.

Key messages Systematic reviews of randomised trials are the best strategy for appraising evidence; however, the findings of some meta-analyses were later contradicted by large trialsFunnel plots, plots of the trials' effect estimates against sample size, are skewed and asymmetrical in the presence of publication bias and other biasesFunnel plot asymmetry, measured by regression analysis, predicts discordance of results when meta-analyses are compared with single large trialsFunnel plot asymmetry was found in 38\% of meta-analyses published in leading general medicine journals and in 13\% of reviews from the Cochrane Database of Systematic ReviewsCritical examination of systematic reviews for publication and related biases should be considered a routine procedure},
	language = {en},
	number = {7109},
	urldate = {2023-11-02},
	journal = {BMJ},
	author = {Egger, Matthias and Smith, George Davey and Schneider, Martin and Minder, Christoph},
	month = sep,
	year = {1997},
	pmid = {9310563},
	note = {Publisher: British Medical Journal Publishing Group
Section: Paper},
	pages = {629--634},
	file = {Full Text:/Users/mjoshi/Zotero/storage/DWL3FB8N/Egger et al. - 1997 - Bias in meta-analysis detected by a simple, graphi.pdf:application/pdf;Snapshot:/Users/mjoshi/Zotero/storage/LA47AMUU/629.html:text/html},
}

@misc{noauthor_estimating_nodate,
	title = {Estimating publication bias in meta‐analyses of peer‐reviewed studies: {A} meta‐meta‐analysis across disciplines and journal tiers - {Mathur} - 2021 - {Research} {Synthesis} {Methods} - {Wiley} {Online} {Library}},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1464},
	urldate = {2023-11-02},
	file = {Estimating publication bias in meta‐analyses of peer‐reviewed studies\: A meta‐meta‐analysis across disciplines and journal tiers - Mathur - 2021 - Research Synthesis Methods - Wiley Online Library:/Users/mjoshi/Zotero/storage/QTVIWAS2/jrsm.html:text/html},
}

@article{mathur_estimating_2021,
	title = {Estimating publication bias in meta‐analyses of peer‐reviewed studies: {A} meta‐meta‐analysis across disciplines and journal tiers},
	volume = {12},
	issn = {1759-2879, 1759-2887},
	shorttitle = {Estimating publication bias in {\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1464},
	doi = {10.1002/jrsm.1464},
	abstract = {Selective publication and reporting in individual papers compromise the scientific record, but are meta‐analyses as compromised as their constituent studies? We systematically sampled 63 meta‐analyses (each comprising at least 40 studies) in
              PLoS One
              , top medical journals, top psychology journals, and Metalab, an online, open‐data database of developmental psychology meta‐analyses. We empirically estimated publication bias in each, including only the peer‐reviewed studies. Across all meta‐analyses, we estimated that “statistically significant” results in the expected direction were only 1.17 times more likely to be published than “nonsignificant” results or those in the unexpected direction (95\% CI: [0.93, 1.47]), with a confidence interval substantially overlapping the null. Comparable estimates were 0.83 for meta‐analyses in
              PLoS One
              , 1.02 for top medical journals, 1.54 for top psychology journals, and 4.70 for Metalab. The severity of publication bias did differ across individual meta‐analyses; in a small minority (10\%; 95\% CI: [2\%, 21\%]), publication bias appeared to favor “significant” results in the expected direction by more than threefold. We estimated that for 89\% of meta‐analyses, the amount of publication bias that would be required to attenuate the point estimate to the null exceeded the amount of publication bias estimated to be actually present in the vast majority of meta‐analyses from the relevant scientific discipline (exceeding the 95th percentile of publication bias). Study‐level measures (“statistical significance” with a point estimate in the expected direction and point estimate size) did not indicate more publication bias in higher‐tier versus lower‐tier journals, nor in the earliest studies published on a topic versus later studies. Overall, we conclude that the mere act of performing a meta‐analysis with a large number of studies (at least 40) and that includes non‐headline results may largely mitigate publication bias in meta‐analyses, suggesting optimism about the validity of meta‐analytic results.},
	language = {en},
	number = {2},
	urldate = {2023-11-02},
	journal = {Research Synthesis Methods},
	author = {Mathur, Maya B. and VanderWeele, Tyler J.},
	month = mar,
	year = {2021},
	pages = {176--191},
	file = {Accepted Version:/Users/mjoshi/Zotero/storage/IBFV4RDG/Mathur and VanderWeele - 2021 - Estimating publication bias in span style=font-v.pdf:application/pdf},
}

@article{vevea1995,
	title = {A general linear model for estimating effect size in the presence of publication bias},
	author = {Vevea, Jack L. and Hedges, Larry V.},
	year = {1995},
	month = {09},
	date = {1995-09},
	journal = {Psychometrika},
	pages = {419{\textendash}435},
	volume = {60},
	number = {3},
	doi = {10.1007/BF02294384},
	url = {http://link.springer.com/10.1007/BF02294384},
	langid = {en}
}

@article{carter2019correcting,
  title={Correcting for bias in psychology: A comparison of meta-analytic methods},
  author={Carter, Evan C and Sch{\"o}nbrodt, Felix D and Gervais, Will M and Hilgard, Joseph},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={2},
  number={2},
  pages={115--144},
  year={2019},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@article{rodgers2021evaluating,
  title={Evaluating meta-analytic methods to detect selective reporting in the presence of dependent effect sizes.},
  author={Rodgers, Melissa A and Pustejovsky, James E},
  journal={Psychological methods},
  volume={26},
  number={2},
  pages={141},
  year={2021},
  publisher={American Psychological Association}
}


@incollection{cooper2009handbook,
  title={The handbook of research synthesis and meta-analysis 2nd edition},
  author={Cooper, Harris and Hedges, Larry Vernon and Valentine, Jeffrey C},
  booktitle={The Hand. of Res. Synthesis and Meta-Analysis, 2nd Ed.},
  pages={1--615},
  year={2009},
  publisher={Russell Sage Foundation}
}


@Manual{rcoreteam,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2023},
    url = {https://www.R-project.org/},
  }


@article{kraft2020interpreting,
  title={Interpreting effect sizes of education interventions},
  author={Kraft, Matthew A},
  journal={Educational researcher},
  volume={49},
  number={4},
  pages={241--253},
  year={2020},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@book{pigott2012advances,
  title={Advances in meta-analysis},
  author={Pigott, Terri},
  year={2012},
  publisher={Springer Science \& Business Media}
}


@article{polanin2016estimating,
  title={Estimating the difference between published and unpublished effect sizes: A meta-review},
  author={Polanin, Joshua R and Tanner-Smith, Emily E and Hennessy, Emily A},
  journal={Review of educational research},
  volume={86},
  number={1},
  pages={207--236},
  year={2016},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@article{chan2004empirical,
  title={Empirical evidence for selective reporting of outcomes in randomized trials: comparison of protocols to published articles},
  author={Chan, An-Wen and Hr{\'o}bjartsson, Asbj{\o}rn and Haahr, Mette T and G{\o}tzsche, Peter C and Altman, Douglas G},
  journal={Jama},
  volume={291},
  number={20},
  pages={2457--2465},
  year={2004},
  publisher={American Medical Association}
}


@article{lancee2017outcome,
  title={Outcome reporting bias in randomized-controlled trials investigating antipsychotic drugs},
  author={Lancee, M and Lemmens, CMC and Kahn, RS and Vinkers, CH and Luykx, JJ},
  journal={Translational psychiatry},
  volume={7},
  number={9},
  pages={e1232--e1232},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{benjamin2018redefine,
  title={Redefine statistical significance},
  author={Benjamin, Daniel J and Berger, James O and Johannesson, Magnus and Nosek, Brian A and Wagenmakers, E-J and Berk, Richard and Bollen, Kenneth A and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and others},
  journal={Nature human behaviour},
  volume={2},
  number={1},
  pages={6--10},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{roest2015reporting,
  title={Reporting bias in clinical trials investigating the efficacy of second-generation antidepressants in the treatment of anxiety disorders: a report of 2 meta-analyses},
  author={Roest, Annelieke M and De Jonge, Peter and Williams, Craig D and De Vries, Ymkje Anna and Schoevers, Robert A and Turner, Erick H},
  journal={JAMA psychiatry},
  volume={72},
  number={5},
  pages={500--510},
  year={2015},
  publisher={American Medical Association}
}

@article{flint2015there,
  title={Is there an excess of significant findings in published studies of psychotherapy for depression?},
  author={Flint, J and Cuijpers, P and Horder, J and Koole, SL and Munaf{\`o}, MR},
  journal={Psychological medicine},
  volume={45},
  number={2},
  pages={439--446},
  year={2015},
  publisher={Cambridge University Press}
}

@article{matthes2015questionable,
  title={Questionable research practices in experimental communication research: A systematic analysis from 1980 to 2013},
  author={Matthes, J{\"o}rg and Marquart, Franziska and Naderer, Brigitte and Arendt, Florian and Schmuck, Desir{\'e}e and Adam, Karoline},
  journal={Communication methods and measures},
  volume={9},
  number={4},
  pages={193--207},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{john2012measuring,
  title={Measuring the prevalence of questionable research practices with incentives for truth telling},
  author={John, Leslie K and Loewenstein, George and Prelec, Drazen},
  journal={Psychological science},
  volume={23},
  number={5},
  pages={524--532},
  year={2012},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{pigott2013outcome,
  title={Outcome-reporting bias in education research},
  author={Pigott, Therese D and Valentine, Jeffrey C and Polanin, Joshua R and Williams, Ryan T and Canada, Dericka D},
  journal={Educational Researcher},
  volume={42},
  number={8},
  pages={424--432},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{williams2014consequences,
  title={Consequences of Outcome Reporting Bias in Education Research.},
  author={Williams, RT and Polanin, JR},
  journal={Society for Research on Educational Effectiveness},
  year={2014},
  publisher={ERIC}
}

@article{o2017chrysalis,
  title={The chrysalis effect: How ugly initial results metamorphosize into beautiful articles},
  author={O’Boyle Jr, Ernest Hugh and Banks, George Christopher and Gonzalez-Mul{\'e}, Erik},
  journal={Journal of Management},
  volume={43},
  number={2},
  pages={376--399},
  year={2017},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{cairo2020gray,
  title={Gray (literature) matters: Evidence of selective hypothesis reporting in social psychological research},
  author={Cairo, Athena H and Green, Jeffrey D and Forsyth, Donelson R and Behler, Anna Maria C and Raldiris, Tarah L},
  journal={Personality and Social Psychology Bulletin},
  volume={46},
  number={9},
  pages={1344--1362},
  year={2020},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{hedges2010robust,
  title={Robust variance estimation in meta-regression with dependent effect size estimates},
  author={Hedges, Larry V and Tipton, Elizabeth and Johnson, Matthew C},
  journal={Research synthesis methods},
  volume={1},
  number={1},
  pages={39--65},
  year={2010},
  publisher={Wiley Online Library}
}

@article{tipton2015small,
  title={Small sample adjustments for robust variance estimation with meta-regression.},
  author={Tipton, Elizabeth},
  journal={Psychological methods},
  volume={20},
  number={3},
  pages={375},
  year={2015},
  publisher={American Psychological Association}
}

@article{pustejovsky2022preventionscience,
  author = {Pustejovsky, James E. and Tipton, Elizabeth},
  title = {Meta-Analysis with Robust Variance Estimation: {Expanding}
    the Range of Working Models},
  journal = {Prevention Science},
  volume = {23},
  pages = {425-438},
  date = {2022-04-01},
  url = {https://doi.org/10.1007/s11121-021-01246-3},
  doi = {10.1016/j.jsp.2018.02.003},
  langid = {en}
}
@article{tiptonpusto2015small,
  title={Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression},
  author={Tipton, Elizabeth and Pustejovsky, James E},
  journal={Journal of Educational and Behavioral Statistics},
  volume={40},
  number={6},
  pages={604--634},
  year={2015},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@article{becker2000multivariate,
  title={Multivariate meta-analysis},
  author={Becker, Betsy J},
  journal={Handbook of applied multivariate statistics and mathematical modeling},
  pages={499--525},
  year={2000},
  publisher={Elsevier}
}


@article{Terrin2003heterogeneity,
  title={Adjusting for publication bias in the presence of heterogeneity},
  author={Terrin, N and Schmid, C H and Lau, J and Olkin, I},
  journal={Statistics in Medicine},
  volume={22},
  number={13},
  pages={2113--2126},
  year={2003},
  doi = {10.1002/sim.1461}
}

@article{Greenwald1975prejudice,
  title={Consequences of prejudice against the null hypothesis},
  author={Greenwald, A G},
  journal={Psychological Bulletin},
  volume={82},
  number={1},
  pages={1--20},
  year={1975},
  doi = {10.1037/h0076157}
}

@article{Nelson1986significance,
  title={Interpretation of significance levels and effect sizes by psychological researchers},
  author={Nelson, N and Rosenthal, R and Rosnow, R L},
  journal={American Psychologist},
  volume={41},
  number={11},
  pages={1299--1301},
  year={1986},
  doi = {10.1037/0003-066X.41.11.1299}
}

@article{Rosenthal1963significance,
  title={The interpretation of levels of significance by psychological researchers},
  author={Rosenthal, R and Gaito, J},
  journal={The Journal of Psychology: Interdisciplinary and Applied},
  volume={55},
  number={1},
  pages={33--38},
  year={1963},
  doi = {10.1080/00223980.1963.9916596}
}

@article{Rosenthal1964significance,
  title={Further evidence for the cliff effect in the interpretation of levels of significance},
  author={Rosenthal, R and Gaito, J},
  journal={Psychological Reports},
  volume={15},
  number={2},
  pages={570},
  year={1964},
  doi = {10.2466/pr0.1964.15.2.570}
}

@article{tipton2019current,
  title={Current practices in meta-regression in psychology, education, and medicine},
  author={Tipton, Elizabeth and Pustejovsky, James E and Ahmadi, Hedyeh},
  journal={Research synthesis methods},
  volume={10},
  number={2},
  pages={180--194},
  year={2019},
  publisher={Wiley Online Library}
}

@article{schild2013less,
  title={Less is less: a systematic review of graph use in meta-analyses},
  author={Schild, Anne HE and Voracek, Martin},
  journal={Research synthesis methods},
  volume={4},
  number={3},
  pages={209--219},
  year={2013},
  publisher={Wiley Online Library}
}

@article{begg1994operating,
  title={Operating characteristics of a rank correlation test for publication bias},
  author={Begg, Colin B and Mazumdar, Madhuchhanda},
  journal={Biometrics},
  pages={1088--1101},
  year={1994},
  publisher={JSTOR}
}


@article{harbord2006modified,
  title={A modified test for small-study effects in meta-analyses of controlled trials with binary endpoints},
  author={Harbord, Roger M and Egger, Matthias and Sterne, Jonathan AC},
  journal={Statistics in medicine},
  volume={25},
  number={20},
  pages={3443--3457},
  year={2006},
  publisher={Wiley Online Library}
}

@article{macaskill2001comparison,
  title={A comparison of methods to detect publication bias in meta-analysis},
  author={Macaskill, Petra and Walter, Stephen D and Irwig, Les},
  journal={Statistics in medicine},
  volume={20},
  number={4},
  pages={641--654},
  year={2001},
  publisher={Wiley Online Library}
}

@article{moreno2009assessment,
  title={Assessment of regression-based methods to adjust for publication bias through a comprehensive simulation study},
  author={Moreno, Santiago G and Sutton, Alex J and Ades, AE and Stanley, Tom D and Abrams, Keith R and Peters, Jaime L and Cooper, Nicola J},
  journal={BMC medical research methodology},
  volume={9},
  pages={1--17},
  year={2009},
  publisher={Springer}
}


@article{pustejovsky2019testing,
  title={Testing for funnel plot asymmetry of standardized mean differences},
  author={Pustejovsky, James E and Rodgers, Melissa A},
  journal={Research Synthesis Methods},
  volume={10},
  number={1},
  pages={57--71},
  year={2019},
  publisher={Wiley Online Library}
}


@article{thompson1999explaining,
  title={Explaining heterogeneity in meta-analysis: a comparison of methods},
  author={Thompson, Simon G and Sharp, Stephen J},
  journal={Statistics in medicine},
  volume={18},
  number={20},
  pages={2693--2708},
  year={1999},
  publisher={Wiley Online Library}
}

@article{copas1997inference,
  title={Inference for non-random samples},
  author={Copas, John B and Li, HG},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={59},
  number={1},
  pages={55--95},
  year={1997},
  publisher={Oxford University Press}
}


@article{hedges1984estimation,
  title={Estimation of effect size under nonrandom sampling: The effects of censoring studies yielding statistically insignificant mean differences},
  author={Hedges, Larry V},
  journal={Journal of Educational Statistics},
  volume={9},
  number={1},
  pages={61--85},
  year={1984},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{vevea2005publication,
  title={Publication bias in research synthesis: sensitivity analysis using a priori weight functions.},
  author={Vevea, Jack L and Woods, Carol M},
  journal={Psychological methods},
  volume={10},
  number={4},
  pages={428},
  year={2005},
  publisher={American Psychological Association}
}


@article{stanley2014meta,
  title={Meta-regression approximations to reduce publication selection bias},
  author={Stanley, Tom D and Doucouliagos, Hristos},
  journal={Research Synthesis Methods},
  volume={5},
  number={1},
  pages={60--78},
  year={2014},
  publisher={Wiley Online Library}
}

@article{stanley2008meta,
  title={Meta-regression methods for detecting and estimating empirical effects in the presence of publication selection},
  author={Stanley, Tom D},
  journal={Oxford Bulletin of Economics and statistics},
  volume={70},
  number={1},
  pages={103--127},
  year={2008},
  publisher={Wiley Online Library}
}

@article{duval2000nonparametric,
  title={A nonparametric “trim and fill” method of accounting for publication bias in meta-analysis},
  author={Duval, Sue and Tweedie, Richard},
  journal={Journal of the american statistical association},
  volume={95},
  number={449},
  pages={89--98},
  year={2000},
  publisher={Taylor \& Francis}
}

@article{duval2000trim,
  title={Trim and fill: a simple funnel-plot--based method of testing and adjusting for publication bias in meta-analysis},
  author={Duval, Sue and Tweedie, Richard},
  journal={Biometrics},
  volume={56},
  number={2},
  pages={455--463},
  year={2000},
  publisher={Oxford University Press}
}

@article{bom2019kinked,
  title={A kinked meta-regression model for publication bias correction},
  author={Bom, Pedro RD and Rachinger, Heiko},
  journal={Research synthesis methods},
  volume={10},
  number={4},
  pages={497--514},
  year={2019},
  publisher={Wiley Online Library}
}


@article{peters2006comparison,
  title={Comparison of two methods to detect publication bias in meta-analysis},
  author={Peters, Jaime L and Sutton, Alex J and Jones, David R and Abrams, Keith R and Rushton, Lesley},
  journal={Jama},
  volume={295},
  number={6},
  pages={676--680},
  year={2006},
  publisher={American Medical Association}
}


@article{moreno2012generalized,
  title={A generalized weighting regression-derived meta-analysis estimator robust to small-study effects and heterogeneity},
  author={Moreno, Santiago G and Sutton, Alex J and Thompson, John R and Ades, AE and Abrams, Keith R and Cooper, Nicola J},
  journal={Statistics in medicine},
  volume={31},
  number={14},
  pages={1407--1417},
  year={2012},
  publisher={Wiley Online Library}
}


@incollection{hedges2005selection,
  title = {Selection method approaches},
  booktitle = {Publication Bias in Meta-Analysis},
  author = {Hedges, Larry V. and Vevea, Jack},
  editor = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
  year = {2005},
  pages = {145--174},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/0470870168.ch9},
}


@article{borenstein2019effect,
  title={Effect sizes for meta-analysis},
  author={Borenstein, MICHAEL and Hedges, LARRY V},
  journal={The handbook of research synthesis and meta-analysis},
  volume={3},
  pages={207--243},
  year={2019},
  publisher={Russell Sage Foundation West Sussex}
}


@Manual{metadat,
    title = {metadat: Meta-Analysis Datasets},
    author = {Thomas White and Daniel Noble and Alistair Senior and W. Kyle Hamilton and Wolfgang Viechtbauer},
    year = {2022},
    note = {R package version 1.2-0},
    url = {https://CRAN.R-project.org/package=metadat},
  }


@article{lehmann2018meta,
  title={Meta-analysis of the effect of red on perceived attractiveness},
  author={Lehmann, Gabrielle K and Elliot, Andrew J and Calin-Jageman, Robert J},
  journal={Evolutionary Psychology},
  volume={16},
  number={4},
  pages={1474704918802412},
  year={2018},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@article{davidson2000bootstrap,
  title={Bootstrap tests: How many bootstraps?},
  author={Davidson, Russell and MacKinnon, James G},
  journal={Econometric Reviews},
  volume={19},
  number={1},
  pages={55--68},
  year={2000},
  publisher={Taylor \& Francis}
}


@article{boos2003introduction,
  title={Introduction to the bootstrap world},
  author={Boos, Dennis D},
  journal={Statistical science},
  volume={18},
  number={2},
  pages={168--174},
  year={2003},
  publisher={Institute of Mathematical Statistics}
}


@article{cameron2008bootstrap,
  title={Bootstrap-based improvements for inference with clustered errors},
  author={Cameron, A Colin and Gelbach, Jonah B and Miller, Douglas L},
  journal={The review of economics and statistics},
  volume={90},
  number={3},
  pages={414--427},
  year={2008},
  publisher={The MIT Press}
}


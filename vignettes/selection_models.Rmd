---
title: "Selection Models for Selective Reporting in Meta-Analysis with Dependent Effects"
output: html_document
date: "2024-09-10"
bibliography: references.bib
link-citations: yes
csl: apa.csl
vignette: >
  %\VignetteIndexEntry{Using the metaselection package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Selective reporting occurs when statistically significant, affirmative results are more likely to be reported (and therefore more likely to be available for meta-analysis) compared to null, non-affirmative results. 
Selective reporting is a major concern for research synthesis because it distorts the evidence base available for meta-analysis. 
Failure to account for selective reporting can inflate effect size estimates from meta-analysis and bias estimates of heterogeneity, making it difficult to draw accurate conclusions from a synthesis.

There are many tools available already to investigate and correct for selective outcome reporting. 
Widely used methods include: graphical diagnostics like funnel plots; tests and adjustments for funnel plot asymmetry like trim-and-fill, Egger's regression, PET/PEESE, selection models, and p-value diagnostics. 
However, very few methods available for investigating selective reporting can accommodate dependent effect sizes. 
Such limitation poses a problem for meta-analyses in education, psychology and other social sciences, where dependent effects are a very common feature of meta-analytic data.

Dependent effect sizes occur when primary studies report multiple measures of the outcomes or repeated measures of the outcome. 
Failing to account for dependency can result in misleading conclusions too narrow confidence intervals and hypothesis tests that have inflated type one error rates.

X (2024) developed and examined methods for investigating and accounting for selective reporting in meta-analysis that account for dependent effect sizes. The results showed that selection models combined with robust variance estimation led to lower bias in the estimate of the overall effect size. Combining the selection models with cluster bootstrapping led to close to nominal coverage rates. 

## Modified Selection Models

Selection models are a general set of methods that correct for selective reporting by modeling the process by which the effect size estimates are reported [@rothstein_selection_2006]. Selection models have two components: a model describing the evidence generation process and model describing the selection process.
X (2025) particularly examined selection processes that are a function of one-sided p-values, with weights corresponding to the likelihood that effects with their respective p-values are reported. 
The study examined two types of selection models: step-function [@vevea_general_1995] and beta-density [@citkowicz_parsimonious_2017]. 
Step function model involves specifying steps which categorize the p-values into intervals that have different probabilities of selection [@vevea_general_1995]. 
The beta-density model, also a weight function model, uses the shape parameters from the beta distribution to identify complex patterns of selection [@citkowicz_parsimonious_2017]. 
X (2025) modified the step and beta-density models to accommodate effect size dependencies.

### Bootstrapped Confidence Intervals

To improve confidence interval coverage, X (2025) also examined bootstrapping the selection models to obtain the confidence intervals. 
Bootstrapping involves re-sampling many times from the original data to create an empirical distribution that can be use in place of the sampling distribution to estimate measures of uncertainty [@boos2003introduction]. 
X (2025) examined variations of bootstrapping approaches combined with different confidence interval calculations. 
The results showed that regular cluster bootstrapping with percentile confidence intervals performed the best. 
The modified selection models combined with this bootstrapping led to close to nominal coverage rates.  



# Example from `metaselection` 

In this section, we will walk through examples on how to implement functions from the `metaselection` package. 
We use `metadat::dat.lehmann` data as our working example [@lehmann2018meta; @metadat]. 
The data is from a meta-analysis by Lehmann, Elliot, & Calin-Jageman (2018) which examined the effects of color red on attractiveness judgments. 
The dataset includes 81 effect sizes from 41 studies. 


```{r, warning = FALSE, message = FALSE}
library(metaselection)
library(metadat)
library(metafor)
library(clubSandwich)
library(kableExtra)
library(tidyverse)

lehmann_dat <- 
  dat.lehmann2018 %>%
  mutate(study = str_split_fixed(Short_Title, pattern = "-", n = 2)[, 1]) %>%
  arrange(study) %>%
  mutate(sei = sqrt(vi)) %>%
  select(study, yi, vi, sei, everything()) %>%
  mutate(esid = 1:nrow(.))

glimpse(lehmann_dat %>% select(study, yi, vi, sei))
```


## Prelminary Analysis 

First, we run analysis pretending there is no selection bias but accounting for the dependent data structure. 
Below is the code to run correlated and hierarchical effects model. 

```{r}
V_mat <- vcalc(vi = vi, 
               cluster = study,
               obs = esid, 
               data = lehmann_dat,
               rho = .8)

mod <- rma.mv(yi = yi,
              V = V_mat,
              data = lehmann_dat,
              random = ~ 1 | study/esid,
              sparse = TRUE)

che_res <- conf_int(mod, cluster = lehmann_dat$study, vcov = "CR2", p_values = TRUE, tidy = TRUE)

kbl(che_res, digits = 3) |>
  kable_paper()
```


The overall estimate of the average effect is `r round(che_res$beta, 3)`, which is significantly different from zero (p = `r round(che_res$p, 3)`). 
Next, we examine how this estimate differs from the estimates calculated using the `metaselection` package.

## Step Function with RVE

In the code below, we input the `lehmann_dat` to the `selection_model()` function for our package. 
We specify which variable is the effect size, `yi`, and which is the standard error for the effect size, `sei`. 
We indicate that we want to estimate `"step"` selection model and input steps as `0.025`. The function, by default, uses maximum likelihood estimation. Another option is to use a hybrid estimator. More details on these estimators can be found in X (2024).

```{r}
step_results <- selection_model(data = lehmann_dat, 
                                yi = yi,
                                sei = sei,
                                selection_type = "step",
                                steps = .025)
#step_results



kbl(step_results$est, digits = 3) |>
  kable_paper()
```

The overall estimate of the average effect is now `r round(step_results$est$Est[step_results$est$param=="beta"], 3)`, which is over a third smaller than the estimate that does not account for selection bias (`r round(che_res$beta, 3)`). This estimate is no longer significant, as depicted by the confidence intervals [`r round(step_results$est$CI_lo[step_results$est$param=="beta"], 3)`, `r round(step_results$est$CI_hi[step_results$est$param=="beta"], 3)`].

We can plot the selection weights to see how the likelihood of selection differs at the pre-specified p-value cutpoint of `0.025`. The selection weight is in the log metric; thus, we must first exponentiate the estimate (exp(`r round(step_results$est$Est[step_results$est$param=="zeta1"], 3)`) = `r round(exp(step_results$est$Est[step_results$est$param=="zeta1"]), 3)`).

```{r}
pvalue <- seq(0,1,.01)
zeta1 <- exp(step_results$est$Est[step_results$est$param=="zeta1"])
stepdat <- data.frame(pvalue = rep(pvalue, 5), weights = zeta1)
stepdat$weights <- ifelse(stepdat$pvalue < .025, 1, stepdat$weights)

ggplot(stepdat, aes(x = pvalue, y = weights)) +
  geom_line() + 
  scale_x_continuous(limits = c(0,1), expand = expansion(0,0), breaks = c(0, .025, .25, .5, .75, .975, 1)) + 
  scale_y_continuous(limits = c(0,1), expand = expansion(0,0)) + 
  geom_hline(yintercept = 0) +  
  geom_vline(xintercept = c(.025, .5, .975), linetype = "dashed") +  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  labs(x = "p-value", y = "Relative Likelihood of Being Published")
```

The plot shows that studies with p-values > 0.025 are about half as likely (`r round(exp(step_results$est$Est[step_results$est$param=="zeta1"]), 3)`) to be published than studies with smaller p-values.

### Bootstrap Confidence Intervals

The code below is similar to above except now we specify that we want to run cluster bootstrapping to estimate the standard error and confidence intervals. We specify `bootstrap = "multinomial"` to run cluster bootstrapping and we specify that we want `"percentile"` bootstrap confidence intervals. We specify that number of bootstraps by setting R to `19`. We are setting the value to 19 here just for an example to speed up the computation. In practice, we recommend a much higher number of bootstrap replications (i.e., 1999). Please see @davidson2000bootstrap for further information on how to select the number of bootstraps. We highly recommend running the selection models with cluster bootstrapping, particularly the regular multinomial bootstrap with percentile confidence intervals, as this has been shown to improve confidence interval coverage rate. 

```{r}
step_results_boot <- selection_model(data = lehmann_dat, 
                                     yi = yi,
                                     sei = sei,
                                     steps = .025,
                                     bootstrap = "multinomial",
                                     boot_CI = "percentile",
                                     R = 19)

kbl(step_results_boot$est, digits = 3) |>
  kable_paper()
```

The overall estimate of the average effect does not change when bootstrapping is applied (`r round(step_results_boot$est$Est[step_results_boot$est$param=="beta"], 3)`). However, the confidence internal is narrower, [`r round(step_results_boot$est$percentile_lower[step_results_boot$est$param=="beta"], 3)`, `r round(step_results_boot$est$percentile_upper[step_results_boot$est$param=="beta"], 3)`], which can change the significance of the effect. [NOTE: I say "can" because the CI changes each time I run it.]

## Beta Function with RVE

In the code below, we specify `selection_type = "beta"` to run beta-density model and specify `steps = c(.025, .975)`. The default estimator for the beta function model is maximum likelihood; the hybrid estimator is not yet available. In additional, although not presented below, we can request bootstrap confidence intervals similarly to how we did for step function model. 

```{r}
beta_results <- selection_model(data = lehmann_dat, 
                                yi = yi,
                                sei = sei,
                                cluster = study,
                                selection_type = "beta",
                                steps = c(.025, .975))


kbl(beta_results$est, digits = 3) %>%
  kable_paper()
```

The overall estimate of the average effect is `r round(beta_results$est$Est[beta_results$est$param=="beta"], 3)`, which is smaller than both the estimate that does not account for selection bias (`r round(che_res$beta, 3)`) and the estimate from the step function model with RVE (`r round(step_results$est$Est[step_results$est$param=="beta"], 3)`). This estimate is not significant, as depicted by the confidence intervals [`r round(beta_results$est$CI_lo[beta_results$est$param=="beta"], 3)`, `r round(beta_results$est$CI_hi[beta_results$est$param=="beta"], 3)`].

To see how the likelihood of selection differs across studies with different p-values, we use the beta density function. We use a truncated version of the beta density as the beta function with RVE truncates the pre-specified cutpoints to psychologically salient thresholds for one-sided p-values, specifically `.025` and `.975`. Again, we must first exponentiate the selection weights as they are in the log metric (exp(`r round(beta_results$est$Est[beta_results$est$param=="zeta1"], 3)`) = `r round(exp(beta_results$est$Est[beta_results$est$param=="zeta1"]), 3)`, exp(`r round(beta_results$est$Est[beta_results$est$param=="zeta2"], 3)`) = `r round(exp(beta_results$est$Est[beta_results$est$param=="zeta2"]), 3)`).

```{r}
trunc_beta <- function(pval, a, b) {
  pval_trunc <- pmin(pmax(pval, .025), .975)
  wt <- pval_trunc^(a - 1) * (1 - pval_trunc)^(b - 1)
  scale <- .025^(a - 1) * (1 - .025)^(b - 1)
  wt / scale
}

pvalue <- seq(0,1,.01)
zeta1 <- exp(beta_results$est$Est[beta_results$est$param=="zeta1"])
zeta2 <- exp(beta_results$est$Est[beta_results$est$param=="zeta2"])
betadat <- data.frame(pvalue = rep(pvalue, 5), weights = c(trunc_beta(pvalue, zeta1, zeta2)))

ggplot(betadat, aes(x = pvalue, y = weights)) +
  geom_line() + 
  scale_x_continuous(limits = c(0,1), expand = expansion(0,0), breaks = c(0, .025, .25, .5, .75, .975, 1)) + 
  scale_y_continuous(limits = c(0,1), expand = expansion(0,0)) + 
  geom_hline(yintercept = 0) +  
  geom_vline(xintercept = c(.025, .5, .975), linetype = "dashed") +  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  labs(x = "p-value", y = "Relative Likelihood of Being Published")
```

The plot shows that studies with smaller p-values are more likely to be published than studies with larger p-values. For example, studies with p-values = 0.050 are about half as likely to be published than studies with p-value = 0.025.

# References


---
title: "P-Value Selection Models for Meta-Analysis with Dependent Effects"
authors:
  - James E. Pustejovsky
  - Megha Joshi
  - Martyna Citkowicz
output: 
  html_document:
    number_sections: true
    toc: true
    toc_float: true
date: "`r Sys.Date()`"
bibliography: references.bib
link-citations: yes
csl: apa.csl
vignette: >
  %\VignetteIndexEntry{Using the metaselection package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

A meta-analysis aims to provide a comprehensive synthesis of available evidence on a topic of interest. 
One major challenge to this aim is selective reporting of evidence from primary studies. Selective reporting occurs when the statistical significance level of a finding affects whether it is reported and therefore whether the finding is available for inclusion in a meta-analysis. 
If statistically significant, affirmative results are more likely to be reported compared to null, non-affirmative results, the evidence base available for meta-analysis will be distorted, leading to inflation of effect size estimates from meta-analysis and bias in estimates of heterogeneity. 

Because selective reporting makes it difficult to draw accurate conclusions from a synthesis, many tools have been developed that try to detect selective reporting problems and to correct for the biases it creates in meta-analytic summaries. 
Widely used methods include: graphical diagnostics like funnel plots; tests and adjustments for funnel plot asymmetry such as trim-and-fill, Egger's regression, and PET/PEESE; selection models; and p-value diagnostics such as p-curve and p-uniform. 
However, very few methods available for investigating selective reporting can accommodate dependent effect sizes. 
This limitation poses a problem for meta-analyses in education, psychology and other social sciences, where dependent effects are a very common feature of meta-analytic data.

Dependent effect sizes occur when primary studies report multiple measures of the outcomes or repeated measures of the outcome. 
Generally, failing to account for dependency can result in misleading conclusions,  overly narrow confidence intervals, and hypothesis tests that have inflated type one error rates.

In this package, we provide methods for investigating and accounting for selective reporting based on p-value selection models. 
The methods account for dependent effect sizes by using cluster-robust variance estimation (i.e., sandwich estimation) or clustered bootstrapping techniques. 
Simulation results show that selection models combined with robust variance estimation led to low bias in the estimate of the overall effect size. 
Combining the selection models with cluster-bootstrapping leads to confidence intervals with close-to-nominal coverage rates. 

# P-Value Selection Models

P-value selection models are a general set of methods that correct for selective reporting by modeling the process by which the effect size estimates are reported [@rothstein_selection_2006]. 
Selection models have two components: a model describing the evidence generation process and model describing the selection process.
The `metaselection` package implements a very flexible class of selection models, in which the evidence-generating process follows a random effects location-scale model [@viechtbauer_locationscale_2022] and where the selection process is a function of one-sided p-values, either in the form of a step function [@vevea_general_1995] or a beta-density function [@citkowicz_parsimonious_2017]. 
Step function model involves specifying steps which categorize the p-values into intervals that have different probabilities of selection [@vevea_general_1995]. 
The beta-density model uses a different selection function, based on a beta distribution, to capture complex patterns of selection [@citkowicz_parsimonious_2017]. 

Consider a meta-analytic dataset with a total of $J$ studies, where study $j$ includes $k_j$ effect size estimates. 
Let $Y_{ij}$ denote an effect size estimate produced by a study, prior to selective reporting. 
The effect size estimate has standard error $\sigma_{ij}$, which is treated as a fixed quantity.
Let $\mathbf{x}_{ij}$ be a $1 \times x$ row vector of predictors that encode characteristics of the effect size and/or the study and may be related to average effect size magnitude. 
Let $\mathbf{u}_{ij}$ be a $1 \times u$ row vector of predictors that may be related to effect size heterogeneity. 
Let $\Phi()$ denote the standard normal cumulative distribution function and $\phi()$ the standard normal density. 
Finally, let $p_{ij}$ be the one-sided p-value corresponding to the effect size estimate, which is a function of the effect size estimate and its standard error: $p_{ij} = 1 - \Phi\left(Y_{ij} / \sigma_{ij}\right) = \Phi\left(-Y_{ij} / \sigma_{ij}\right)$.

## The evidence-generating process

The model for the evidence-generating process is a random effects location-scale model, in which
$$
Y_{ij} = \mathbf{x}_{ij} \boldsymbol\beta + v_{ij} + e_{ij},
$$
where $\boldsymbol\beta$ is a $1 \times x$ vector of regression coefficients that relate the predictors to average effect size magnitude, $v_i$ is a normally distributed random effect with mean zero and variance $\tau^2_{ij}$, and $e_i$ is a normally distributed sampling error with mean zero and known variance $\sigma^{*2}_{ij}$. 
The variance of the random effects is modeled as
$$
\log\left(\tau_{ij}^{2}\right) = \mathbf{u}_{ij} \boldsymbol\gamma,
$$
where $\boldsymbol\gamma$ is a $u \times 1$ vector of coefficients that relate the predictors to the degree of marginal variation in the random effects.
If the model does not include predictors of heterogeneity, then $\mathbf{u}_{ij} = 1$ and the model reduces to a conventional random effects meta-regression in which $\gamma = \log(\tau^2)$.

## The selective reporting process

The selective reporting process is defined by a selection function, which specifies how the probability that a given effect size estimate is reported varies depending on its $p$-value. 
Let $O_{ij}$ be an indicator for whether effect size $i$ in study $j$ is observed. 
Then the selection model defines $\text{Pr}(O_{ij} = 1 \ | \ p_{ij}) = \text{Pr}(O_{ij} = 1 \ | \ Y_{ij}, \sigma_{ij})$. The distribution of observed effect sizes then corresponds to 
$$
\text{Pr}(Y_{ij} = y \ | \ O_{ij} = 1) = \frac{\text{Pr}(O_{ij} = 1 \ | \ Y_{ij} = y, \sigma_{ij}) \times \text{Pr}(Y_{ij} = y \ | \ \sigma_{ij})}{\text{Pr}(O_{ij} = 1 \ | \ \sigma_{ij})}.
$$
In the above, the first term in the numerator is defined by the selective reporting process and the second term is defined by the evidence-generating process. 

@hedges_modeling_1992 and @vevea_general_1995 proposed to model the selective reporting process using a step-function, with thresholds chosen to correspond to "psychologically salient" $p$-values. In the general formulation, we have steps $\alpha_1,...,\alpha_H$ and 
$$
\text{Pr}(O_{ij} = 1 | p_{ij}) = \begin{cases}
1 & \text{if} & p_{ij} < \alpha_1 \\ 
\exp(\zeta_1) & \text{if} & \alpha_1 \leq p_{ij} < \alpha_2 \\ \exp(\zeta_2) & \text{if} & \alpha_2 \leq p_{ij} < \alpha_3 \\
\vdots \\
\exp(\zeta_H) & \text{if} & \alpha_H \leq p_{ij}.
\end{cases}
$$
The selection parameters $\zeta_1,...,\zeta_H$ control the probabilities of selection given a p-value, with parameter $\zeta_h$ defined as the log of the probability that an effect size estimate is observed, given that its p-value is in the range $[\alpha_h, \alpha_{h+1})$, relative to the probability than effect size estimate is observe, given that its p-value is in the range $[0, \alpha_1)$. 
We use log-transformed probabilities so that the parameter space is unrestricted, $-\infty < \zeta_h < +\infty$ for $h=1,...,H$; $\zeta_1 = \cdots = \zeta_H = 0$ therefore describes a process where effect sizes are reported with uniform probability, regardless of their statistical significance level.

In practice, meta-analysts will often use only a small number of steps in the selection model. 
One common choice is the three-parameter selection model, which has a single step at $\alpha = .025$. With this choice of threshold, positive effects that are statistically significant at the two-sided level of $p < .05$ have a different probability of selection than effects that are not statistically significant or not in the anticipated direction.
Another possibility is to use two steps at $\alpha_1 = .025$ and $\alpha_2 = .500$, which allows for different probabilities of selection for effects that are positive but not statistically significant and effects that are negative (i.e., in the opposite the intended direction). 
We call the latter model a four-parameter selection model.

<!-- Martyna, can you add a paragraph on the beta function model? -->

## Estimation

## Cluster-robust variance estimation

## Bootstrapped Confidence Intervals

To improve confidence interval coverage, X (2025) also examined bootstrapping the selection models to obtain the confidence intervals. 
Bootstrapping involves re-sampling many times from the original data to create an empirical distribution that can be use in place of the sampling distribution to estimate measures of uncertainty [@boos2003introduction]. 
X (2025) examined variations of bootstrapping approaches combined with different confidence interval calculations. 
The results showed that regular cluster bootstrapping with percentile confidence intervals performed the best. 
The modified selection models combined with this bootstrapping led to close to nominal coverage rates.  

# Using the `metaselection` package

```{r, echo = FALSE}
data("dat.lehmann2018", package = "metadat")
n_ES <- nrow(dat.lehmann2018)
n_studies <- length(table(dat.lehmann2018$Full_Citation))
```

We now demonstrate the key functions from the `metaselection` package.
As a running example, We data from a meta-analysis by @lehmann2018meta, who examined the effects of the color red on attractiveness judgments.
The dataset is available in the `metadat` package [@metadat] as `dat.lehmann2018`. 
It consists of `r n_ES` effect sizes from `r n_studies` studies. The following code loads the dataset and creates variables that will be needed for the subsequent analysis.

```{r lehmann}
data("dat.lehmann2018", package = "metadat")
dat.lehmann2018$study <- dat.lehmann2018$Full_Citation
dat.lehmann2018$sei <- sqrt(dat.lehmann2018$vi)
dat.lehmann2018$esid <- 1:nrow(dat.lehmann2018) 
```

## Prelminary Analysis 

As a point of comparison, we first run an analysis that ignores the possibility of selective reporting bias but accounts for the dependence structure of the effect sizes using a correlated-and-heirarchical effects working model and cluster-robust variance estimation [@pustejovsky2022preventionscience]. 
The following code first creates a sampling variance-covariance matrix assuming that effect size estimates from the same study have sampling errors that are correlated at 0.8; it then fits a correlated-and-heirarchical effects working model and applies robust variance estimation, clustering by study.

```{r CHE}
library(metafor)
library(clubSandwich)

# Create sampling variance-covariance matrix
V_mat <- vcalc(
  vi = vi, 
  cluster = study,
  obs = esid, 
  data = dat.lehmann2018,
  rho = .8,
  sparse = TRUE
)

# First CHE working model
CHE_mod <- rma.mv(
  yi = yi, V = V_mat,
  random = ~ 1 | study / esid,
  data = dat.lehmann2018,
  sparse = TRUE
) |>
  
# Apply RVE with small-sample corrections, clustering by study
  robust(cluster = study, clubSandwich = TRUE)

CHE_mod
```

The overall estimate of the average effect is `r round(as.numeric(CHE_mod$beta), 3)`, 95% CI [`r round(as.numeric(CHE_mod$ci.lb), 3)`, `r round(as.numeric(CHE_mod$ci.ub), 3)`], which is significantly different from zero (p = `r round(CHE_mod$pval, 3)`). 
The estimated total heterogeneity (including both between- and within-study heterogeneity) is `r round(sum(CHE_mod$sigma2), 3)`, corresponding to a total standard deviation of `r round(sqrt(sum(CHE_mod$sigma2)), 3)`.
Next, we examine how this average effect size estimate differs from the estimates based on a step-function or beta-function selection model, fitted using the `metaselection` package.

## Three-Parameter Step Function with RVE

The primary function for fitting p-value selection models is `selection_model()`. 
In the code below, we fit a step-function selection model to the `dat.lehmann2018` data using the `selection_model()` function. 
We specify which variable is the effect size, `yi`, and which is the standard error for the effect size, `sei`. 
We indicate that we want to estimate `"step"` selection model and specify a single step at .025 by setting  `step = 0.025`. By default, the function fits the modeling using composite maximum likelihood estimation and calculates standard errors and confidence intervals using cluster-robust variance estimation.

```{r 3PSM}
library(metaselection)

mod_3PSM <- selection_model(
  data = dat.lehmann2018, 
  yi = yi,
  sei = sei,
  cluster = study,
  selection_type = "step",
  steps = .025
)

mod_3PSM
```

The estimate of the overall average effect is now `r round(mod_3PSM$est$Est[mod_3PSM$est$param=="beta"], 3)`, which is over a third smaller than the estimate that does not account for selection bias (`r round(CHE_mod$beta, 3)`). This estimate is no longer statistically distinct from zero, as indicated by the 95% confidence interval [`r round(mod_3PSM$est$CI_lo[mod_3PSM$est$param=="beta"], 3)`, `r round(mod_3PSM$est$CI_hi[mod_3PSM$est$param=="beta"], 3)`].

The estimates of the total heterogeneity parameter and the selection parameter are on the log-scale. We can obtain an estimate of the total variance by setting the argument `transf_gamma` to `TRUE` in the `print()` method. Similarly, we  can transform the selection parameter estimates to the probability scale by setting `transf_zeta = TRUE`:
```{r}
print(mod_3PSM, transf_gamma = TRUE, transf_zeta = TRUE)
```
The estimated total heterogeneity of `r round( exp(mod_3PSM$est$Est[mod_3PSM$est$param=="gamma"]), 3)` is slightly smaller than the total heterogeneity estimate from the CHE model, but is also very imprecisely estimated. 
The selection parameter is now called `lambda_1`. The estimate of `r round(exp(mod_3PSM$est$Est[mod_3PSM$est$param=="zeta1"]), 3)` indicates that effect size estimates with one-sided p-values greater than .025 are only about half as likely to be reported as estimates that are positive and statistically significant (i.e., estimates with $p < .025$). 

The `metaselection` package provides a function `selection_plot()` to visualize the estimated selection weights:

```{r 3PSM-plot}
selection_plot(mod_3PSM)
```

The plot illustrates how the likelihood of selection differs as a function of the one-sided p-value of an effect size estimate. In this example, the plot shows that studies with p-values > 0.025 are about half as likely ($\lambda_1 = `r round(exp(mod_3PSM$est$Est[mod_3PSM$est$param=="zeta1"]), 3)`$) to be published than studies with smaller p-values.

## Four-Parameter Step Model with RVE

Rather than using a single threshold at $\alpha_1 = .025$, we could fit a model that also allows the selection probability for negative effect size estimates to differ from the selection probability for positive but non-significant estimates. The following code fits such a model, setting `steps = c(.025, .500)`:
```{r}
mod_4PSM <- selection_model(
  data = dat.lehmann2018, 
  yi = yi,
  sei = sei,
  cluster = study,
  selection_type = "step",
  steps = c(.025, .500)
)

print(mod_4PSM, transf_gamma = TRUE, transf_zeta = TRUE)
```
The estimate of the overall average effect is `r round(mod_4PSM$est$Est[mod_4PSM$est$param=="beta"], 3)`, even smaller than the estimated effect from the three-parameter step model and only one third the magnitude of the estimate that does not account for selection bias (`r round(CHE_mod$beta, 3)`).

We can visualize the estimated selection function with `selection_plot()`:
```{r}
selection_plot(mod_4PSM)
```

As is apparent from the plot, this estimated model indicates that negatively signed effects (i.e., those with a one-sided p-value greater than .50) are even less likely to be observed than effects that are positive but not statistically significant.
However, as can be seen from the robust confidence intervals in the model output, the selection parameters are very imprecisely estimated. 

## Beta Function with RVE

The `selection_model()` function also allows fitting models based on beta density selection functions by specifying `selection_type = "beta"`. The default estimator for the beta function model is maximum likelihood; the hybrid estimator is not yet available.  

```{r beta}
mod_beta <- selection_model(
  data = dat.lehmann2018, 
  yi = yi,
  sei = sei,
  cluster = study,
  selection_type = "beta"
)

print(mod_beta, transf_gamma = TRUE, transf_zeta = TRUE)
```

The estimate of the overall average effect is `r round(mod_beta$est$Est[mod_beta$est$param=="beta"], 3)`, which is smaller than both the estimate that does not account for selection bias (`r round(CHE_mod$beta, 3)`) and the estimates from the three- and four-parameter step function models. The average effect estimate based on the beta function is not statistically distinct from zero, as indicated by the 95% confidence intervals [`r round(mod_beta$est$CI_lo[mod_beta$est$param=="beta"], 3)`, `r round(mod_beta$est$CI_hi[mod_beta$est$param=="beta"], 3)`].

To see how the probability of selection differs across studies with different p-values, we can again visualize the selection function:

```{r beta-plot}
selection_plot(mod_beta)
```

The plot shows that effect sizes with smaller p-values are more likely to be published than effects with larger p-values. For example, an effect size estimate of zero (with one-sided p-value of $p = .500$) is about half as likely to be published as an effect with a statistically significant, positive effect.

## Bootstrap Confidence Intervals

Rather than relying on robust variance estimation to construct standard errors and confidence intervals, we can use cluster-wise bootstrap resampling. The code below re-fits the three-parameter step function model to obtain cluster-bootstrap confidence intervals. We specify `bootstrap = "multinomial"` to run cluster bootstrapping and we specify that we want `"percentile"` bootstrap confidence intervals. We specify that number of bootstraps by setting R to `99`. We set the value to 99 here solely to limit the amount of computation. In practice, we recommend using a much higher number of bootstrap replications, such as 1999, to obtain confidence intervals with more accurate coverage rates [@davidson2000bootstrap]. We highly recommend running the selection models with cluster bootstrapping, particularly the regular multinomial bootstrap with percentile confidence intervals, as this has been shown to improve confidence interval coverage rates relative to using other forms of bootstrap confidence intervals. 

```{r 3PSM-bootstrap}
set.seed(20240916)

system.time(
  mod_3PSM_boot <- selection_model(
    data = dat.lehmann2018, 
    yi = yi,
    sei = sei,
    cluster = study,
    selection_type = "step",
    steps = .025,
    CI_type = "percentile",
    bootstrap = "multinomial",
    R = 99
  )
)

print(mod_3PSM_boot, transf_gamma = TRUE, transf_zeta = TRUE)
```

The overall estimate of the average effect does not change when bootstrapping is applied (`r round(mod_3PSM_boot$est$Est[mod_3PSM_boot$est$param=="beta"], 3)`). However, the confidence internal is narrower, [`r round(mod_3PSM_boot$est$percentile_lower[mod_3PSM_boot$est$param=="beta"], 3)`, `r round(mod_3PSM_boot$est$percentile_upper[mod_3PSM_boot$est$param=="beta"], 3)`] (due to the use of a smaller-than-desirable number of bootstrap replications).

The bootstrapping routine is implemented to work with the `future` and `future.apply` packages for parallel processing. For example, the following code specifies to use a `multisession` future processing plan with 4 worker nodes, then fits the same model as above:
```{r parallel-boot}
library(future)
library(future.apply)
plan(multisession, workers = 4L)

system.time(
  selection_model(
    data = dat.lehmann2018, 
    yi = yi,
    sei = sei,
    cluster = study,
    selection_type = "step",
    steps = .025,
    CI_type = "percentile",
    bootstrap = "multinomial",
    R = 99
  )  
)

```

Setting a `sequential` plan will stop the use of parallel processing:
```{r sequential-boot}
plan(sequential)
```


# References

